{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Collaborative Filtering\n* 유저가 좋아하는 것을 다른 유저와 유사성에 기반해 예측하는 것.\n* Advantages: 아이템 자체에 대한 이해가 없이, 영화와 같은 복잡한 항목을 정확하게 추천 할 수 있음. \n* 보통 추천 시스템에는 user similarity 와 item similarity가 있음.\n* Task 1: finding similar animes\n* Task 2: finding similar users\n* Task 3: Recommending Animes for a random user","metadata":{}},{"cell_type":"code","source":"INPUT_DIR = \"../input/anime-recommendation-database-2020\"\n!ls {INPUT_DIR}","metadata":{"execution":{"iopub.status.busy":"2022-03-13T04:38:16.812382Z","iopub.execute_input":"2022-03-13T04:38:16.812766Z","iopub.status.idle":"2022-03-13T04:38:17.61937Z","shell.execute_reply.started":"2022-03-13T04:38:16.812728Z","shell.execute_reply":"2022-03-13T04:38:17.617972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nrating_df = pd.read_csv(INPUT_DIR + \"/animelist.csv\",\n                       low_memory = False,\n                        usecols=[\"user_id\",\"anime_id\",\"rating\"]\n                       )\nrating_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-13T04:38:19.159318Z","iopub.execute_input":"2022-03-13T04:38:19.159826Z","iopub.status.idle":"2022-03-13T04:39:21.469424Z","shell.execute_reply.started":"2022-03-13T04:38:19.159775Z","shell.execute_reply":"2022-03-13T04:39:21.468418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# User should rate at least 400\nn_ratings = rating_df['user_id'].value_counts() # 칼럼의 unique값을 카운트. 왼쪽이 user_id, rating 개수.\nrating_df = rating_df[rating_df['user_id'].isin(n_ratings[n_ratings>=400])].copy() #isin 은 해당 리스트를 포함하고 있는 행을 골라줌.\nlen(rating_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T04:40:02.318428Z","iopub.execute_input":"2022-03-13T04:40:02.318853Z","iopub.status.idle":"2022-03-13T04:40:04.243418Z","shell.execute_reply.started":"2022-03-13T04:40:02.318819Z","shell.execute_reply":"2022-03-13T04:40:04.242739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0, 1.0 사이로 스케일링\nmin_rating = min(rating_df['rating'])\nmax_rating = max(rating_df['rating'])\nrating_df['rating'] = rating_df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values.astype(np.float64)\n\nAvgRating = np.mean(rating_df['rating'])\nprint(\"Avg\", AvgRating)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T04:40:04.245226Z","iopub.execute_input":"2022-03-13T04:40:04.24547Z","iopub.status.idle":"2022-03-13T04:40:04.946526Z","shell.execute_reply.started":"2022-03-13T04:40:04.245441Z","shell.execute_reply":"2022-03-13T04:40:04.945611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing Duplicated Rows\nduplicates = rating_df.duplicated()\n\nif duplicates.sum() > 0:\n    print(\"> {} duplicates\".format(duplicates.sum()))\n    rating_df = rating_df[~duplicates] # ~은 not의 의미. 즉, 중복된 것이 없는 것만 뽑아낸 것이라고 보면 될 듯.\n\nprint(\"> {} duplicates\".format(rating_df.duplicated().sum()))","metadata":{"execution":{"iopub.status.busy":"2022-03-13T04:40:04.94821Z","iopub.execute_input":"2022-03-13T04:40:04.948561Z","iopub.status.idle":"2022-03-13T04:40:05.163559Z","shell.execute_reply.started":"2022-03-13T04:40:04.948514Z","shell.execute_reply":"2022-03-13T04:40:05.162927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = rating_df.groupby(\"user_id\")['rating'].count() # count() : Count the number of (not NULL) values in each row 유저가 평점을 준 개수 구하기\ntop_users = g.dropna().sort_values(ascending=False)[:20] # dropna(): Remove missing values. 결측치 제거 후 역순으로 20개 정렬(별점이 많은 순으로)\ntop_r = rating_df.join(top_users, rsuffix=\"_r\", how=\"inner\", on=\"user_id\") # user_id(on)가 겹치는 항목에 대해 두 프레임 합치기\ntop_r\n\ng = rating_df.groupby('anime_id')['rating'].count()\ntop_animes = g.dropna().sort_values(ascending=False)[:20]\ntop_r = top_r.join(top_animes, rsuffix=\"_r\", how = \"inner\", on = \"anime_id\")\n\npd.crosstab(top_r.user_id, top_r.anime_id, top_r.rating, aggfunc = np.sum)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T04:40:05.165105Z","iopub.execute_input":"2022-03-13T04:40:05.165452Z","iopub.status.idle":"2022-03-13T04:40:05.373007Z","shell.execute_reply.started":"2022-03-13T04:40:05.165424Z","shell.execute_reply":"2022-03-13T04:40:05.372273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Encoding categorical data\n# 유니크한 user_id 도출 -> ex. 400:1, 402:4로 user 테이블에 매핑시켜 둠 -> categorical한 데이터를 인코딩?\nuser_ids = rating_df['user_id'].unique().tolist()\nuser2user_encoded = {x: i for i, x in enumerate(user_ids)}\nuser_encoded2user = {i: x for i, x in enumerate(user_ids)}\nrating_df['user'] = rating_df['user_id'].map(user2user_encoded) # user2user_encoded에 있는 값으로 매핑시킴\nn_users = len(user2user_encoded)\n\nanime_ids = rating_df['anime_id'].unique().tolist()\nanime2anime_encoded = {x: i for i, x in enumerate(anime_ids)}\nanime_encoded2anime = {i: x for i, x in enumerate(anime_ids)}\nrating_df['anime'] = rating_df['anime_id'].map(anime2anime_encoded)\nn_animes = len(anime2anime_encoded)\n\nprint(\"Num of users: {}, Num of animes: {}\".format(n_users, n_animes))\nprint(\"Min rating: {}, Max rating: {}\".format(min(rating_df['rating']), max(rating_df['rating'])))","metadata":{"execution":{"iopub.status.busy":"2022-03-13T04:40:05.374128Z","iopub.execute_input":"2022-03-13T04:40:05.374474Z","iopub.status.idle":"2022-03-13T04:40:05.691929Z","shell.execute_reply.started":"2022-03-13T04:40:05.374448Z","shell.execute_reply":"2022-03-13T04:40:05.690846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shuffle\n# frac: 추출할 데이터 비율. 비복원의 경우 frac = 0 ~ 1 사이의 값\n# n: 추출할 데이터의 개수.\nrating_df = rating_df.sample(frac=1, random_state=73)\n\nX = rating_df[['user', 'anime']].values\ny = rating_df['rating']","metadata":{"execution":{"iopub.status.busy":"2022-03-13T04:40:05.693858Z","iopub.execute_input":"2022-03-13T04:40:05.694199Z","iopub.status.idle":"2022-03-13T04:40:05.83313Z","shell.execute_reply.started":"2022-03-13T04:40:05.694156Z","shell.execute_reply":"2022-03-13T04:40:05.832072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split\ntest_set_size = 10000\ntrain_indices = rating_df.shape[0] - test_set_size\n\nX_train, X_test, y_train, y_test = (\n    X[:train_indices],\n    X[train_indices:],\n    y[:train_indices],\n    y[train_indices:],\n)\n\nprint(\"> Train set ratings: {}\".format(len(y_train)))\nprint(\"> Test set ratings: {}\".format(len(y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-03-13T04:40:08.008854Z","iopub.execute_input":"2022-03-13T04:40:08.0093Z","iopub.status.idle":"2022-03-13T04:40:08.017207Z","shell.execute_reply.started":"2022-03-13T04:40:08.00927Z","shell.execute_reply":"2022-03-13T04:40:08.016033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_array = [X_train[:,0], X_train[:,1]]\nX_test_array = [X_test[:,0], X_test[:,1]]","metadata":{"execution":{"iopub.status.busy":"2022-03-13T04:40:08.342223Z","iopub.execute_input":"2022-03-13T04:40:08.342993Z","iopub.status.idle":"2022-03-13T04:40:08.349061Z","shell.execute_reply.started":"2022-03-13T04:40:08.342937Z","shell.execute_reply":"2022-03-13T04:40:08.348193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accelerator check\nimport tensorflow as tf\n\nTPU_INIT = True\n\nif TPU_INIT:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    !nvidia-smi\n\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T04:40:11.088522Z","iopub.execute_input":"2022-03-13T04:40:11.088856Z","iopub.status.idle":"2022-03-13T04:40:22.172756Z","shell.execute_reply.started":"2022-03-13T04:40:11.088824Z","shell.execute_reply":"2022-03-13T04:40:22.171784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Building","metadata":{}},{"cell_type":"code","source":"!pip install keras","metadata":{"execution":{"iopub.status.busy":"2022-03-13T04:42:14.001314Z","iopub.execute_input":"2022-03-13T04:42:14.001684Z","iopub.status.idle":"2022-03-13T04:42:25.477418Z","shell.execute_reply.started":"2022-03-13T04:42:14.001647Z","shell.execute_reply":"2022-03-13T04:42:25.476424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade tensorflow\n!pip install --upgrade tensorflow-gpu","metadata":{"execution":{"iopub.status.busy":"2022-03-13T04:44:31.082015Z","iopub.execute_input":"2022-03-13T04:44:31.08285Z","iopub.status.idle":"2022-03-13T04:46:24.600964Z","shell.execute_reply.started":"2022-03-13T04:44:31.082809Z","shell.execute_reply":"2022-03-13T04:46:24.599761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras \nfrom keras import layers\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2022-03-13T04:46:24.603385Z","iopub.execute_input":"2022-03-13T04:46:24.603685Z","iopub.status.idle":"2022-03-13T04:46:24.659007Z","shell.execute_reply.started":"2022-03-13T04:46:24.603648Z","shell.execute_reply":"2022-03-13T04:46:24.657726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Embedding layers\nfrom keras.layers import Add, Activation, Lambda, BatchNormalization, Concatenate, Dropout,\nInput, Embedding, Dot, Reshape, Dense, Flatten\n\ndef RecommenderNet():\n    embedding_size = 128\n    \n    user = Input(name = \"user\", shape=[1]) # Input(): keras 텐서를 인스턴스화하는 데 사용.\n    user_embedding = Embedding(name = \"user_embedding\", \n                          input_dim = n_users,\n                          output_dim = embedding_size)(user)\n    anime = Input(name=\"anime\", shape=[1])\n    anime_embedding = Embedding(name = \"anime_embedding\",\n                               input_dim = n_animes,\n                               output_dim = embedding_size)(anime)\n    \n    # x = concatenate()([user_embedding, anime_embedding])\n    x = Dot(name = \"dot_product\", normalize=True, axes=2)([user_embedding, anime_embedding])\n    x = Flatten()(x)\n    \n    x = Dense(1, kernel_initializer = \"he_normal\")(x)     # kernerl_initializer: 가중치 초기화 함수\n    x = BatchNormalization()(x)\n    x = Activation(\"sigmoid\")(x)\n    \n    model = Model(inputs=[user,anime], outputs =x)\n    # mae: mean absolute error, mse: mean sqaured error\n    model.compile(loss=\"binary_crossentropy\", metrics=[\"mae\",\",mse\"], optimizer=\"Adam\")\n    \n    return model\n\nif TPU_INIT:\n    with tpu_strategy.scope():\n        model = RecommenderNet()\nelse:\n    model = RecommenderNet()\n    \nmodel.summary()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-13T08:50:21.283376Z","iopub.execute_input":"2022-03-13T08:50:21.283826Z","iopub.status.idle":"2022-03-13T08:50:21.319467Z","shell.execute_reply.started":"2022-03-13T08:50:21.283719Z","shell.execute_reply":"2022-03-13T08:50:21.318572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Callbacks\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROplateau\n\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005\nbatch_size = 10000\n\nif TPU_INIT:\n    max_lr = max_lr * tpu_strategy.num_replicas_in_sync\n    batch_size = batch_size * tpu_strategy.num_replicas_in_sync\n    \nrampup_epochs = 5\nsustain_epochs = 0\nexp_decay = .8\n\ndef lrfn(epoch):\n    if epoch < rampup_epochs:\n        return (max_lr - start_lr)/ rampup_epochs * epoch + start_lr\n    elif epoch < rampup_epochs + sustain_epochs:\n        return max_lr\n    else:\n        return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n    \nlr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=0)\n\ncheckpoint_filepath = \"./weights.h5\"\n\nmodel_checkpoints = ModelCheckpoint(filepath = checkpoint_filepath,\n                                   save_weights_only = True, # False이면 모델 전체를 저장. True이면 가중치만 저장\n                                    monitor = \"val_loss\",\n                                    mode = \"min\",# auto/ min/ max (monitor가 val_acc이면 max, val_loss이면 min)\n                                    save_best_only = True #\n                                   )\nearly_stopping = EarlyStopping(patience = 3, monitor=\"val_loss\",\n                              mode=\"min\", restore_best_weights =True) # False이면 훈련 마지막 단계에서 얻은 모델 가중치가 사용됨. True이면 epoch 중 최상의 값으로 가중치 복원함,\nmy_callbacks = [\n    model_checkpoints,\n    lr_callback,\n    early_stopping,\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model training\nhistory = model.fit(\n    x = X_train_array,\n    y = y_train,\n    batch_size = batch_size,\n    epochs = 20,\n    verbose = 1, # show progress bar\n    validation_data = (X_test_array, y_test),\n    callbacks = my_callbacks\n)\n\nmodel.load_weights(checkpoint_filepath)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training results\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(history.history[\"loss\"][0:-2])\nplt.plot(history.history[\"val_loss\"][0:-2])\nplt.title(\"model loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\",\"test\"], loc = \"upper left\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extracting weights from model\n","metadata":{}},{"cell_type":"code","source":"def extract_weights(name, model):\n    weight_layer = model.get_layer(name)\n    weights = weight_layer.get_weights()[0]\n    weights = weights / np.linalg.norm(weights, axis=1).reshape((-1,1))\n    return weights\n\nanime_weights = extract_weights(\"anime_embedding\", model)\nuser_weights = extract_weights(\"user_embedding\", model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**anime meta data**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(INPUT_DIR + \"/anime.csv\", low_memory = True)\ndf = df.replace(\"Unknown\", np.nan)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fixing Names\ndef getAnimeName(anime_id):\n    try:\n        name = df[df.anime_id == anime_id].eng_version.values[0]\n        if name is np.nan:\n            name = df[df.anime_id == anime_id].Name.values[0]\n    except:\n        print(\"error\")\n    return name\n\ndf[\"anime_id\"] = df[\"MAL_ID\"]\ndf[\"eng_version\"] = df[\"English name\"]\ndf[\"eng_version\"] = df.anime_id.apply(lambda x: getAnimeName(x))\n\ndf.sort_values(by = [\"Score\"],\n              inplace=True, \n               ascending = False, \n               kind = \"quicksort\",\n               na_position = \"last\" \n              )\ndf = df[[\"anime_id\", \"eng_version\",\"Score\",\"Genders\"\n         ,\"Episodes\",\"Type\", \"Premiered\",\"Members\"]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getAnimeFrame(anime):\n    if isinstance(anime, int):\n        return df[df.anime_id == anime]\n    if isinstance(anime, str):\n        return df[df.eng_version == anime]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**sypnopsis data**","metadata":{}},{"cell_type":"code","source":"cols = [\"MAL_ID\", \"Name\", \"Genders\",\"sypnopsis\"]\nsypnopsis_df = pd.read_csv(INPUT_DIR + \"/anime_with_synopsis.csv\", usecols=cols)\n\ndef getSypnopsis(anime):\n    if isinstance(anime, int):\n        return sypnopsis_df[sypnopsis_df.MAL_ID == anime].sypnopsis.values[0]\n    if isinstance(anime, str):\n        return sypnopsis_df[sypnopsis_df.Name == anime].sypnopsis.values[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task1: Finding Similar Animes (Item Based Rcommendation)","metadata":{}},{"cell_type":"code","source":"# pd.reset_option(\"all\")\npd.reset_option(\"max_colwidth\", None)\n\ndef find_similar_animes(name, n = 10, return_dist = False, neg=False):\n    try:\n        index = getAnimeFrame(name).anime_id.values[0]\n        encoded_index = anime2anime_encoded.get(index)\n        weights = anime_weights\n        \n        dists = np.dot(weigths, weights[encoded_index])\n        sorted_dists = np.argsort(dists)\n        \n        n += 1\n        \n        if neg:\n            closest = sorted_dists[:n]\n        else:\n            closest = sorted_dists[-n:]\n            \n        print(\"animes closest to {}\".format(name))\n        \n        if return_dist:\n            return dists, closest\n        \n        rindex = df\n        \n        SimilarityArr = []\n        \n        for close in closest:\n            decoded_id = anime_encoded2anime.get(close)\n            sypnopsis = getSypnopsis(decoded_id)\n            anime_frame = getAnimeFrame(decoded_id)\n            \n            anime_name = anime_frame.eng_version.values[0]\n            genre = anime_frame.Genders.values[0]\n            similarity = dists[close]\n            SimilarityArr.append({\"anime_id\":decoded_id, \"name\":anime_name,\n                                 \"similarity\": similarity, \"genre\":genre,\n                                 \"sypnopsis\":sypnopsis})\n        Frame = pd.DataFrame(SimilarityArr).sort_values(by=\"similarity\", ascending = False)\n        return Frame[Frame.anime_id != index].drop(['anime_id'], axis=1)\n    \n    except:\n        print('{}!, Not Found in Anime list'.format(name))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_similar_animes(\"Dragon Ball Z\", n=5, neg=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_similar_animes(\"Your name\", n=5, neg= False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Task2: Finding Similar Users (User Based Recommendation)","metadata":{}},{"cell_type":"code","source":"print(\"> picking up random user\")\n\n# 500개 미만의 평가를 한 유저 한 명을 랜덤추출\nratings_per_user = rating_df.groupby(\"user_id\").size()\nrandom_user = ratings_per_user[ratings_per_user < 500].sample(1, random_state=None).index[0]\nprint(\"> user id\", random_user)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option(\"max_colwidth\", None)\n\ndef find_similar_users(item_input, n=10,return_dist=False, neg=False):\n    try:\n        index = item_input\n        encoded_index = user2user_encoded.get(index)\n        weights = user_weights\n    \n        dists = np.dot(weights, weights[encoded_index])\n        sorted_dists = np.argsort(dists)\n        \n        n = n + 1\n        \n        if neg:\n            closest = sorted_dists[:n]\n        else:\n            closest = sorted_dists[-n:]\n\n        print('> users similar to #{}'.format(item_input))\n\n        if return_dist:\n            return dists, closest\n        \n        rindex = df\n        SimilarityArr = []\n        \n        for close in closest:\n            similarity = dists[close]\n\n            if isinstance(item_input, int):\n                decoded_id = user_encoded2user.get(close)\n                SimilarityArr.append({\"similar_users\": decoded_id, \n                                      \"similarity\": similarity})\n\n        Frame = pd.DataFrame(SimilarityArr).sort_values(by=\"similarity\", \n                                                        ascending=False)\n        \n        return Frame\n    \n    except:\n        print('{}!, Not Found in User list'.format(name))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similar_users = find_similar_users(int(random_user), n=5, neg=False)\n\nsimilar_users = similar_user[similar_users.similarity>0.4]\nsimilar_users = similar_user[similar_users.similar_users != random_user]\nsimilar_users.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### User Preferences\n","metadata":{}},{"cell_type":"code","source":"from worldcloud import WorldCloud\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef showWordCloud(all_genres):\n    genres_cloud = WordClud(width=700, height=400,\n                           background_color = \"white\",\n                           colormap=\"gnuplot\").generate_from_frequencies(all_genres)\n    plt.figure(figsize=(10,8))\n    plt.imshow(genres_cloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\n    \ndef getFavGenre(frame, plot = False):\n    frame.dropna(inplace=False)\n    all_genres = defaultdict(int)\n    \n    genres_list = []\n    for genres in frame[\"Genders\"]:\n        if isinstance(genres, str):\n            for genre in genres.split(\",\"):\n                genres_list.append(genre)\n                all_genres[genre.strip()] += 1\n    if plot:\n        showWordCloud(all_genres)\n    \n    return genres_list\n\ndef get_user_preferences(user_id, plot = False, verbose = 0):\n    animes_watched_by_user = rating_df[rating_df.user_id==user_id]\n    user_rating_percentile = np.percentile(animes_watched_by_user.rating, 75)\n    animes_watched_by_user = animes_watched_by_user[animes_watched_by_user.rating >= user_rating_percentile]\n    top_animes_user = (\n        animes_watched_by_user.sort_values(by=\"rating\", ascending = False).anime_id_values\n    )\n    anime_df_rows = df[df[\"anime_id\"].isin(top_anime_user)]\n    anime_df_rows = animes_df_rows[[\"eng_version\", \"Genders\"]]\n    \n    if verbose != 0:\n        print(\"> User #{} has rated {} movies (avg. rating = {:.1f})\".format(\n            user_id, len(animes_watched_by_user),\n            animes_watched_by_user[\"rating\"].mean(),\n        ))\n        print(\"> preferred genres\")\n        \n    if plot:\n        getFavGenre(anime_df_rows, plot)\n        \n    return anime_df_rows","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_pref = get_user_preferences(random_user, plot=True, verbose=1)\nprint(\"> animes highly rated by this user\")\n\npd.DataFrame(user_pref).head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Task3: Recommending animes for a user","metadata":{}},{"cell_type":"code","source":"def get_recommended_animes(similar_users, n=10):\n    recommended_animes = []\n    anime_list = []\n    \n    for user_id in similar_users.similar_users.values:\n        pref_list = get_user_preferences(int(user_id), verbose=0)\n        pref_list = pref_list[~ pref_list.eng_version.isin(user_pref.eng_version.values)]\n        anime_list.append(pref_list.eng_version.values)\n        \n    anime_list = pd.DataFrame(anime_list)\n    sorted_list = pd.DataFrame(pd.Series(anime_list.values.ravel()).value_counts()).head(n)\n    \n    for i, anime_name in enumerate(sorted_list.index):        \n        n_user_pref = sorted_list[sorted_list.index == anime_name].values[0][0]\n        if isinstance(anime_name, str):\n            try:\n                frame = getAnimeFrame(anime_name)\n                anime_id = frame.anime_id.values[0]\n                genre = frame.Genders.values[0]\n                sypnopsis = getSypnopsis(int(anime_id))\n                recommended_animes.append({#\"anime_id\": anime_id ,\n                                            \"n\": n_user_pref,\n                                            \"anime_name\": anime_name, \n                                            \"Genders\": genre, \n                                            \"sypnopsis\": sypnopsis})\n            except:\n                pass\n    \n    return pd.DataFrame(recommended_animes)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recommended_animes = get_recommended_animes(similar_users, n= 10)\ngetFavGenre(recommended_animes, plot = True)\n\nprint(\"\\n> Top recommendations for user: {}\".format(random_user))\nrecommended_animes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ranking Based Recommendation","metadata":{}},{"cell_type":"code","source":"print(\"Showing recommendations for user: {}\".format(random_user))\nprint(\"===\" * 25)\n\nanimes_watched_by_user = rating_df[rating_df.user_id==random_user]\nanime_not_watched_df = df[\n    ~df[\"anime_id\"].isin(animes_watched_by_user.anime_id.values)\n]\n\nanime_not_watched = list(\n    set(anime_not_watched_df['anime_id']).intersection(set(anime2anime_encoded.keys()))\n)\n\nanime_not_watched = [[anime2anime_encoded.get(x)] for x in anime_not_watched]\n\nuser_encoder = user2user_encoded.get(random_user)\n\nuser_anime_array = np.hstack(\n    ([[user_encoder]] * len(anime_not_watched), anime_not_watched)\n)\n\nuser_anime_array = [user_anime_array[:, 0], user_anime_array[:, 1]]\nratings = model.predict(user_anime_array).flatten()\n\ntop_ratings_indices = (-ratings).argsort()[:10]\n\nrecommended_anime_ids = [\n    anime_encoded2anime.get(anime_not_watched[x][0]) for x in top_ratings_indices\n]\n\nResults = []\ntop_rated_ids = []\n\nfor index, anime_id in enumerate(anime_not_watched):\n    rating = ratings[index]\n    id_ = anime_encoded2anime.get(anime_id[0])\n    \n    if id_ in recommended_anime_ids:\n        top_rated_ids.append(id_)\n        try:\n            condition = (df.anime_id == id_)\n            name = df[condition]['eng_version'].values[0]\n            genre = df[condition].Genders.values[0]\n            score = df[condition].Score.values[0]\n            sypnopsis = getSypnopsis(int(id_))\n        except:\n            continue\n            \n        Results.append({#\"anime_id\": id_, \n                        \"name\": name, \n                        \"pred_rating\": rating,\n                        \"genre\": genre, \n                        'sypnopsis': sypnopsis})\n\nprint(\"---\" * 25)\nprint(\"> Top 10 anime recommendations\")\nprint(\"---\" * 25)\n\n\nResults = pd.DataFrame(Results).sort_values(by='pred_rating', ascending=False)\nResults","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"anime_model.h5\")\n\nfrom IPython.display import FileLink\nFileLink(r\"./anime_model.h5\")","metadata":{},"execution_count":null,"outputs":[]}]}