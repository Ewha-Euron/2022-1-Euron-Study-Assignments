{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5432e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T03:46:00.616554Z",
     "start_time": "2022-05-20T03:46:00.607579Z"
    }
   },
   "source": [
    "#### Task: premise 문장을 참고해 hypothesis 문장이 참인지(Entailment), 거짓인지(Contradiction), 혹은 참/거짓 여부를 알 수 없는 문장인지(Neutral)를 판별하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88008210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노트북: Hugging Fasce를 활용한 모델링: (https://dacon.io/competitions/official/235875/codeshare/4520?page=1&dtype=recent)\n",
    "\n",
    "'''\n",
    "Hugging Face?\n",
    "hugging face는 자연어처리를 위해 다양한 트랜스포머 모델과 학습 스트립트를 제공하는 모듈이다. \n",
    "트랜스포머 사용시 layer, model등을 선언하거나 스크립트를 구현하는 과정을 간단화할 수 있다 \n",
    "layer.py -> transformer.models를 이용한다. \n",
    "models에는 트랜스포머 기반의 다양한 모델을 pytorch, tensorflow로 각각 구현해 두었다. \n",
    "trainer에는 학습에 필요한 optim, F, variable의 기능들이 구현되어있다. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fae3bf41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T03:54:34.992994Z",
     "start_time": "2022-05-20T03:54:34.983021Z"
    }
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f02798c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T03:54:35.411873Z",
     "start_time": "2022-05-20T03:54:35.279228Z"
    }
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "PATH =  './open/open/'\n",
    "\n",
    "train = pd.read_csv(os.path.join(PATH, 'train_data.csv'), encoding='utf-8')\n",
    "test = pd.read_csv(os.path.join(PATH, 'test_data.csv'), encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "495e298e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T03:54:54.180667Z",
     "start_time": "2022-05-20T03:54:54.151745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24998 entries, 0 to 24997\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   index       24998 non-null  int64 \n",
      " 1   premise     24998 non-null  object\n",
      " 2   hypothesis  24998 non-null  object\n",
      " 3   label       24998 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 781.3+ KB\n",
      "None\n",
      "\n",
      "Train Columns:  Index(['index', 'premise', 'hypothesis', 'label'], dtype='object')\n",
      "Train Label: \n",
      "entailment       8561\n",
      "contradiction    8489\n",
      "neutral          7948\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Train Null: \n",
      "index         0\n",
      "premise       0\n",
      "hypothesis    0\n",
      "label         0\n",
      "dtype: int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...</td>\n",
       "      <td>씨름의 여자들의 놀이이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...</td>\n",
       "      <td>자작극을 벌인 이는 3명이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.</td>\n",
       "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
       "      <td>원주민들은 종합대책에 만족했다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...</td>\n",
       "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            premise  \\\n",
       "0      0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...   \n",
       "1      1  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...   \n",
       "2      2                    이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.   \n",
       "3      3  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
       "4      4  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...   \n",
       "\n",
       "                                hypothesis          label  \n",
       "0                           씨름의 여자들의 놀이이다.  contradiction  \n",
       "1                         자작극을 벌인 이는 3명이다.  contradiction  \n",
       "2  예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.     entailment  \n",
       "3                        원주민들은 종합대책에 만족했다.        neutral  \n",
       "4       이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.        neutral  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data 파악\n",
    "print(train.info(), end='\\n\\n')\n",
    "print('Train Columns: ', train.columns)\n",
    "print('Train Label: ', train['label'].value_counts(), sep='\\n', end='\\n\\n')\n",
    "print('Train Null: ', train.isnull().sum(), sep='\\n', end='\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cde9db7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T03:54:57.028051Z",
     "start_time": "2022-05-20T03:54:57.007107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1666 entries, 0 to 1665\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   index       1666 non-null   int64 \n",
      " 1   premise     1666 non-null   object\n",
      " 2   hypothesis  1666 non-null   object\n",
      " 3   label       1666 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 52.2+ KB\n",
      "None\n",
      "Test Columns:  Index(['index', 'premise', 'hypothesis', 'label'], dtype='object')\n",
      "Test Label: \n",
      "answer    1666\n",
      "Name: label, dtype: int64\n",
      "Test Null: \n",
      "index         0\n",
      "premise       0\n",
      "hypothesis    0\n",
      "label         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# test data 파악\n",
    "print(test.info())\n",
    "print('Test Columns: ', test.columns)\n",
    "print('Test Label: ', test['label'].value_counts(), sep='\\n')\n",
    "print('Test Null: ', test.isnull().sum(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "366c39db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T03:55:36.468547Z",
     "start_time": "2022-05-20T03:55:36.199268Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x750 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 간단한 EDA\n",
    "\n",
    "feature = train['label']\n",
    "\n",
    "plt.figure(figsize=(10,7.5))\n",
    "plt.title('Label Count', fontsize=20)\n",
    "\n",
    "temp = feature.value_counts()\n",
    "plt.bar(temp.keys(), temp.values, width=0.5, color='b', alpha=0.5)\n",
    "plt.text(-0.05, temp.values[0]+20, s=temp.values[0])\n",
    "plt.text(0.95, temp.values[1]+20, s=temp.values[1])\n",
    "plt.text(1.95, temp.values[2]+20, s=temp.values[2])\n",
    "\n",
    "plt.xticks(temp.keys(), fontsize=12) # x축 값, 폰트 크기 설정\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # 레이아웃 설정\n",
    "plt.show() # 그래프 나타내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c166039",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T03:57:04.561900Z",
     "start_time": "2022-05-20T03:57:04.286636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Premise Length:  90\n",
      "Min Premise Length:  19\n",
      "Mean Premise Lenght:  45.406552524201935 \n",
      "\n",
      "Max Hypothesis Length:  103\n",
      "Min Hypothesis Length:  5\n",
      "Mean Hypothesis Lenght:  24.924433954716378\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x750 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# raw data 전제 -> premise의 길이 확인\n",
    "max_len = np.max(train['premise'].str.len())\n",
    "min_len = np.min(train['premise'].str.len())\n",
    "mean_len = np.mean(train['premise'].str.len())\n",
    "\n",
    "print('Max Premise Length: ', max_len)\n",
    "print('Min Premise Length: ', min_len)\n",
    "print('Mean Premise Lenght: ', mean_len, '\\n')\n",
    "\n",
    "# hypothesis -> 전제보다 보통 길이가 더 짧음\n",
    "max_len = np.max(train['hypothesis'].str.len())\n",
    "min_len = np.min(train['hypothesis'].str.len())\n",
    "mean_len = np.mean(train['hypothesis'].str.len())\n",
    "\n",
    "print('Max Hypothesis Length: ', max_len)\n",
    "print('Min Hypothesis Length: ', min_len)\n",
    "print('Mean Hypothesis Lenght: ', mean_len)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "plt.figure(figsize=(10,7.5))\n",
    "plt.title('Premise Length', fontsize=20)\n",
    "\n",
    "plt.hist(train['premise'].str.len(), alpha=0.5, color='orange')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # 레이아웃 설정\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6ffca4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T03:57:28.337301Z",
     "start_time": "2022-05-20T03:57:28.278460Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서 소년이나 장정들이 ...</td>\n",
       "      <td>씨름의 여자들의 놀이이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나 ...</td>\n",
       "      <td>자작극을 벌인 이는 3명이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다</td>\n",
       "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
       "      <td>원주민들은 종합대책에 만족했다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면 이런 상황에서는 ...</td>\n",
       "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            premise  \\\n",
       "0      0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서 소년이나 장정들이 ...   \n",
       "1      1  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나 ...   \n",
       "2      2                     이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다   \n",
       "3      3  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
       "4      4  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면 이런 상황에서는 ...   \n",
       "\n",
       "                                hypothesis          label  \n",
       "0                           씨름의 여자들의 놀이이다.  contradiction  \n",
       "1                         자작극을 벌인 이는 3명이다.  contradiction  \n",
       "2  예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.     entailment  \n",
       "3                        원주민들은 종합대책에 만족했다.        neutral  \n",
       "4       이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.        neutral  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text cleaning에 해당하는 기본 전처리\n",
    "\n",
    "# 숫자, 힌글이 아닌 단어 제거하기\n",
    "train['premise'] = train['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
    "test['premise'] = test['premise'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72291952",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T03:58:42.949715Z",
     "start_time": "2022-05-20T03:58:27.166934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages (from transformers) (4.63.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages (from transformers) (2022.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages (from transformers) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages (from transformers) (4.8.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Installing collected packages: filelock, tokenizers, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.7.0 huggingface-hub-0.6.0 tokenizers-0.12.1 transformers-4.19.2\n"
     ]
    }
   ],
   "source": [
    "# 모델은 트랜스포머 패키지를 이용 -> hugging face 제공 라이브러리\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c943e90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T03:59:06.094801Z",
     "start_time": "2022-05-20T03:58:59.790666Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d44d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed & device 설정\n",
    "def seed_everything(seed:int = 1004):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfbe79ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T04:07:49.620989Z",
     "start_time": "2022-05-20T04:05:32.589936Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [00:00<00:00, 376kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 243k/243k [00:00<00:00, 308kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 734k/734k [00:01<00:00, 621kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████| 173/173 [00:00<00:00, 86.7kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 547/547 [00:00<00:00, 548kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 1.25G/1.25G [01:57<00:00, 11.5MB/s]\n",
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (18): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (19): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (20): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (21): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (22): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (23): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hugging face hub의 토크나이저와 모델을 가져다 쓴다.\n",
    "# 라벨은 Entailment, contradiction, neutral 로 3가지이다.\n",
    "\n",
    "MODEL_NAME = 'klue/roberta-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = 3\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "print(model)\n",
    "print(config) # 모델 아키텍처 확인..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecd2f702",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T04:08:17.268033Z",
     "start_time": "2022-05-20T04:08:15.294313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0, 18607,  6354, 27135,  6573,  2119,  2079,  1537,  2534,  2145,\n",
      "         5869,  2069,  1567,  1295,  1513,  4000,   575,  2170,  8179,  2062,\n",
      "            2,  5709,  1537,  2534,  2119,  5869,  2119,  1122,  2118,  4047,\n",
      "         2062,    18,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "[CLS] 뻔한 스토리에서 이정도의 전율과 감동을 줄 수 있다는 것에 놀랐다 [SEP] 아무런 전율도 감동도 받지 못했다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# tokenizing\n",
    "# premise와 hypothesis는 동일한 텐서로 합쳐저, sep을 통해 구분된다\n",
    "\n",
    "train_dataset, eval_dataset = train_test_split(train, test_size=0.2, shuffle=True, stratify=train['label'])\n",
    "\n",
    "tokenized_train = tokenizer(\n",
    "    list(train_dataset['premise']),\n",
    "    list(train_dataset['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256, # Max_Length = 190\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "tokenized_eval = tokenizer(\n",
    "    list(eval_dataset['premise']),\n",
    "    list(eval_dataset['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "print(tokenized_train['input_ids'][0])\n",
    "print(tokenizer.decode(tokenized_train['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "215eb05a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T04:08:19.354453Z",
     "start_time": "2022-05-20T04:08:19.341487Z"
    }
   },
   "outputs": [],
   "source": [
    "class BERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pair_dataset, label):\n",
    "        self.pair_dataset = pair_dataset\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "        item['label'] = torch.tensor(self.label[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2278d644",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T04:08:22.475105Z",
     "start_time": "2022-05-20T04:08:22.461142Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_to_num(label):\n",
    "    label_dict = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2, \"answer\": 3}\n",
    "    num_label = []\n",
    "\n",
    "    for v in label:\n",
    "        num_label.append(label_dict[v])\n",
    "    \n",
    "    return num_label\n",
    "\n",
    "\n",
    "train_label = label_to_num(train_dataset['label'].values)\n",
    "eval_label = label_to_num(eval_dataset['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02690443",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T04:08:23.015659Z",
     "start_time": "2022-05-20T04:08:22.997707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19998\n",
      "{'input_ids': tensor([    0,  3953,  2073,  4230, 15849,  2205,  2307,  4318,  3819,  5825,\n",
      "        11800,     2,  4318,  2154,  1560,  2219,  3606,    18,     2,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0]), 'label': tensor(1)}\n",
      "[CLS] 시설은 매우 청결하며 위치 또한 훌륭합니다 [SEP] 위치만 좋습니다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = BERTDataset(tokenized_train, train_label)\n",
    "eval_dataset = BERTDataset(tokenized_eval, eval_label)\n",
    "\n",
    "print(train_dataset.__len__())\n",
    "print(train_dataset.__getitem__(19997))\n",
    "print(tokenizer.decode(train_dataset.__getitem__(19997)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff6924f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T04:08:50.232853Z",
     "start_time": "2022-05-20T04:08:50.221883Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "  \"\"\" validation을 위한 metrics function \"\"\"\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  probs = pred.predictions\n",
    "\n",
    "  # calculate accuracy using sklearn's function\n",
    "  acc = accuracy_score(labels, preds) # 리더보드 평가에는 포함되지 않습니다.\n",
    "\n",
    "  return {\n",
    "      'accuracy': acc,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed7463fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T04:08:50.610842Z",
     "start_time": "2022-05-20T04:08:50.550008Z"
    }
   },
   "outputs": [],
   "source": [
    "training_ars = TrainingArguments(\n",
    "    output_dir='./result',\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_total_limit=5,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 500,\n",
    "    load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_ars,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43c0101e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T04:11:16.463670Z",
     "start_time": "2022-05-20T04:08:50.918021Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cse\\.conda\\envs\\jhenv1\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 19998\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='4375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/4375 : < :, Epoch 0.00/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-1fea62ac79fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./result/best_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m         )\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1552\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1554\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1556\u001b[0m                 if (\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2182\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2183\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2185\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2213\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2214\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2215\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2216\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2217\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1212\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1214\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1215\u001b[0m         )\n\u001b[0;32m   1216\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    855\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m         )\n\u001b[0;32m    859\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    528\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m                 )\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m         )\n\u001b[0;32m    416\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         )\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# add attentions if we output them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\jhenv1\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 학습은 진행 X\n",
    "trainer.train()\n",
    "model.save_pretrained('./result/best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1904ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54896280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
