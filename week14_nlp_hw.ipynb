{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week14_nlp_hw.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nowionlyseedaylight/2022-1-Euron-Study-Assignments/blob/Week_14/week14_nlp_hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 week14 과제는 **13주차의 Subwords 실습**으로 구성되어 있습니다.\n",
        "\n",
        "📌 위키독스의 딥러닝을 이용한 자연어 처리 입문 교재 실습, 관련 블로그 등의 문서 자료로 구성되어 있는 과제입니다. \n",
        "\n",
        "📌 안내된 링크에 맞추어 **직접 코드를 따라 치면서 (필사)** 해당 nlp task 의 기본적인 라이브러리와 메서드를 숙지해보시면 좋을 것 같습니다😊 필수라고 체크한 부분은 과제에 반드시 포함시켜주시고, 선택으로 체크한 부분은 자율적으로 스터디 하시면 됩니다.\n",
        "\n",
        "📌 궁금한 사항은 깃허브 이슈나, 카톡방, 세션 발표 시작 이전 시간 등을 활용하여 자유롭게 공유해주세요!"
      ],
      "metadata": {
        "id": "BX3ac8Ag1RPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk colab 환경에서 실행시 필요한 코드입니다. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "4JEquLR91VBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **subwords**"
      ],
      "metadata": {
        "id": "HfTr_BPwGc8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👀 **내용 복습** \n",
        "\n",
        "* 단어 단위의 분석 - 글자 단위의 분석 - (일부는 단어 일부는 글자 단위의 분석을 진행 :  hybrid)\n",
        "* 기존 word 기반 모델은 (OOV problem + 국가마다 다른 언어체계) 단점이 존재 👉  character 기반 모델 등장 배경\n",
        "  * 그러나  Pure Character level model 의 수행시간이 너무 오래걸린다는 단점 존재 \n",
        "  * 따라서 단어를 좀 더 쪼개서 보는 word piece 모델이 등장\n",
        "* subword model 👉  word level 모델과 구조는 동일하나 더 작은 word 단위로 나누어서 보는 word pieces 를 사용하고 있는 모델 \n",
        "  * BPE : subword tokenizer 알고리즘 \n",
        "  * word piece :  BPE 변형 알고리즘 👉 pre-segmentation + BPE → 빈번하게 등장하는 단어들에 대해 먼저 단어사전에 추가하고 이후에 BPE 를 적용\n",
        "  * sentence piece : 중국어 등 단어로 구분이 어려운 언어의 경우 raw text 에서 바로 character-level 로 나뉘어짐 \n",
        "  * hybrid : 일부는 글자단위, 일부는 단어 단위로 분석 \n",
        "\n",
        "* FastText : word2vec 을 이을 차세대 word vector learning library 로 하나의 단어에 여러 단어들이 존재하는 것으로 간주하여 학습한다.\n",
        "\n",
        "![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbu49K3%2FbtrCOKUFG7Y%2FtbffQ7RkugJ1X2kcyEYGgk%2Fimg.png)"
      ],
      "metadata": {
        "id": "kUpIB0rlG9vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🥰 **이하 예제를 실습하시면 됩니다.**\n",
        "\n",
        "**1-(1)~(4) 는 필수과제, (5) 의 2개는 선택과제입니다.**\n"
      ],
      "metadata": {
        "id": "Kq8aMYKGPQR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔹 1-(1) BPE\n",
        "\n",
        "* 대표적인 subword 분리 알고리즘 \n",
        "* 3) 코드실습 부분을 위주로 필사해주시면 됩니다.\n",
        "\n",
        "\n",
        "📌 [Byte Pair Encoding](https://wikidocs.net/22592)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eB5XfXsWHBHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) 기존 방식"
      ],
      "metadata": {
        "id": "L_R1AqYTFvO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary\n",
        "# 훈련 데이터에 있는 단어와 등장 빈도수\n",
        "low : 5, lower : 2, newest : 6, widest : 3\n",
        "\n",
        "# vocabulary\n",
        "low, lower, newest, widest\n",
        "\n",
        "# 테스트 단계에서 'lowest'와 같은 새로운 형태가 등장하면 oov 문제가 발생하게 됨."
      ],
      "metadata": {
        "id": "dpCBqBpuHACO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) BPE 방식\n",
        "\n",
        "딕셔너리의 모든 단어들을 character level로 분리함.\n"
      ],
      "metadata": {
        "id": "56Vqt67EGCkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, collections\n",
        "from IPython.display import display, Markdown, Latex"
      ],
      "metadata": {
        "id": "l7nuCDkkGIYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BPE 실행 횟수: 10"
      ],
      "metadata": {
        "id": "qLSWqIs0HC9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_merges = 10"
      ],
      "metadata": {
        "id": "vu78dTY-HCnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = {'l o w </w>' : 5,\n",
        "         'l o w e r </w>' : 2,\n",
        "         'n e w e s t </w>':6,\n",
        "         'w i d e s t </w>':3\n",
        "         }"
      ],
      "metadata": {
        "id": "RWx2_cXPHHIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(dictionary):\n",
        "    # 유니그램의 pair들의 빈도수를 카운트\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in dictionary.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    print('현재 pair들의 빈도수 :', dict(pairs))\n",
        "    return pairs\n",
        "\n",
        "def merge_dictionary(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "bpe_codes = {}\n",
        "bpe_codes_reverse = {}\n",
        "\n",
        "for i in range(num_merges):\n",
        "    display(Markdown(\"### Iteration {}\".format(i + 1)))\n",
        "    pairs = get_stats(dictionary)\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    dictionary = merge_dictionary(best, dictionary)\n",
        "\n",
        "    bpe_codes[best] = i\n",
        "    bpe_codes_reverse[best[0] + best[1]] = best\n",
        "\n",
        "    print(\"new merge: {}\".format(best))\n",
        "    print(\"dictionary: {}\".format(dictionary))"
      ],
      "metadata": {
        "id": "fBGdZOkSHIWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bpe_codes)"
      ],
      "metadata": {
        "id": "lnD0UphhHNTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* OOV에 대처하기"
      ],
      "metadata": {
        "id": "mAtjvw6LHT9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as a tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def encode(orig):\n",
        "    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\"\"\"\n",
        "\n",
        "    word = tuple(orig) + ('</w>',)\n",
        "    display(Markdown(\"__word split into characters:__ <tt>{}</tt>\".format(word)))\n",
        "\n",
        "    pairs = get_pairs(word)    \n",
        "\n",
        "    if not pairs:\n",
        "        return orig\n",
        "\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        display(Markdown(\"__Iteration {}:__\".format(iteration)))\n",
        "\n",
        "        print(\"bigrams in the word: {}\".format(pairs))\n",
        "        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float('inf')))\n",
        "        print(\"candidate for merging: {}\".format(bigram))\n",
        "        if bigram not in bpe_codes:\n",
        "            display(Markdown(\"__Candidate not in BPE merges, algorithm stops.__\"))\n",
        "            break\n",
        "        first, second = bigram\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            try:\n",
        "                j = word.index(first, i)\n",
        "                new_word.extend(word[i:j])\n",
        "                i = j\n",
        "            except:\n",
        "                new_word.extend(word[i:])\n",
        "                break\n",
        "\n",
        "            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                new_word.append(first+second)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        new_word = tuple(new_word)\n",
        "        word = new_word\n",
        "        print(\"word after merging: {}\".format(word))\n",
        "        if len(word) == 1:\n",
        "            break\n",
        "        else:\n",
        "            pairs = get_pairs(word)\n",
        "\n",
        "    # 특별 토큰인 </w>는 출력하지 않는다.\n",
        "    if word[-1] == '</w>':\n",
        "        word = word[:-1]\n",
        "    elif word[-1].endswith('</w>'):\n",
        "        word = word[:-1] + (word[-1].replace('</w>',''),)\n",
        "\n",
        "    return word"
      ],
      "metadata": {
        "id": "iBCjH5JQHWTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"loki\")"
      ],
      "metadata": {
        "id": "ZoecW6lsHYrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"lowest\")"
      ],
      "metadata": {
        "id": "VVEGnZEGHZxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"lowing\")"
      ],
      "metadata": {
        "id": "MxT6NeCzHd2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"highing\")"
      ],
      "metadata": {
        "id": "jlDShGnTHhCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔹 1-(2) Subword Text\n",
        "\n",
        "* 텐서플로우를 통해 사용할 수 있는 서브워드 토크나이저\n",
        "* BPE와 유사한 알고리즘인 Wordpiece Model을 채택한 패키지\n",
        "* IMDB 리뷰 토큰화, 네이버 영화 리뷰 토큰화 부분을 필사해주시면 됩니다. \n",
        "\n",
        "```\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_df['review'], target_vocab_size=2**13)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "📌 [SubwordTextEncoder](https://wikidocs.net/86792)"
      ],
      "metadata": {
        "id": "5rVWh1eYPs_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. IMDB 리뷰 토큰화하기"
      ],
      "metadata": {
        "id": "rQfYtG50IDwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "HftoL18APttz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")\n",
        "\n",
        "train_df = pd.read_csv('IMDb_Reviews.csv')"
      ],
      "metadata": {
        "id": "ZR-5eTyDIFsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['review']"
      ],
      "metadata": {
        "id": "BtteH1LFIG1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_df['review'], target_vocab_size=2**13)"
      ],
      "metadata": {
        "id": "z3Wa2LaPIIBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.subwords[:100])"
      ],
      "metadata": {
        "id": "UAiJI1xLIJLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df['review'][20])"
      ],
      "metadata": {
        "id": "5jy-weFwIKVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenized sample question: {}'.format(tokenizer.encode(train_df['review'][20])))"
      ],
      "metadata": {
        "id": "j3GOAkpLILZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df에 존재하는 문장 중 일부를 발췌\n",
        "sample_string = \"It's mind-blowing to me that this film was even made.\"\n",
        "\n",
        "# 인코딩한 결과를 tokenized_string에 저장\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('정수 인코딩 후의 문장 : {}'.format(tokenized_string))\n",
        "\n",
        "# 이를 다시 디코딩\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('기존 문장 : {}'.format(original_string))"
      ],
      "metadata": {
        "id": "XToHX5M_IMoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 집합의 크기(Vocab size) :', tokenizer.vocab_size)"
      ],
      "metadata": {
        "id": "nBgAhy2AIOJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
      ],
      "metadata": {
        "id": "hkOlLazfIPZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 앞서 실습한 문장에 even 뒤에 임의로 xyz 추가\n",
        "sample_string = \"It's mind-blowing to me that this film was evenxyz made.\"\n",
        "\n",
        "# 인코딩한 결과를 tokenized_string에 저장\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('정수 인코딩 후의 문장 : {}'.format(tokenized_string))\n",
        "\n",
        "# 이를 다시 디코딩\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('기존 문장 : {}'.format(original_string))"
      ],
      "metadata": {
        "id": "KM36MghXIQmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
      ],
      "metadata": {
        "id": "Z4wLCThcISEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 네이버 영화 리뷰 토큰화하기"
      ],
      "metadata": {
        "id": "HRyF2kyzIU40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "UPdck1h3ITvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "train_data = pd.read_table('ratings_train.txt')"
      ],
      "metadata": {
        "id": "LWOOv-m0IWlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.isnull().sum())"
      ],
      "metadata": {
        "id": "oj5WI5wBIX2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
      ],
      "metadata": {
        "id": "aSY0nX80IY5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_data['document'], target_vocab_size=2**13)"
      ],
      "metadata": {
        "id": "3k4h-qgmIaJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.subwords[:100])"
      ],
      "metadata": {
        "id": "kofPNcBnIc25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data['document'][20])"
      ],
      "metadata": {
        "id": "JFzWKdVVIew0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenized sample question: {}'.format(tokenizer.encode(train_data['document'][20])))"
      ],
      "metadata": {
        "id": "DQlHO5FRIgOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_string = train_data['document'][21]\n",
        "\n",
        "# 인코딩한 결과를 tokenized_string에 저장\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('정수 인코딩 후의 문장 : {}'.format(tokenized_string))\n",
        "\n",
        "# 이를 다시 디코딩\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('기존 문장 : {}'.format(original_string))"
      ],
      "metadata": {
        "id": "-sgxzj9nIh1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
      ],
      "metadata": {
        "id": "ZwfS7A4QIjit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔹 1-(3) SentencePiece\n",
        "\n",
        "* ``import sentencepiece as spm`` BPE를 포함하여 기타 서브워드 토크나이징 알고리즘들을 내장한 라이브러리 \n",
        "* IMDB 리뷰 토큰화, 네이버 영화 리뷰 토큰화 부분을 필사해주시면 됩니다. \n",
        "\n",
        "\n",
        "📌 [SubwordTextEncoder](https://wikidocs.net/86657)"
      ],
      "metadata": {
        "id": "s-FBc3jI2kLH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0sLOZxu06Eo"
      },
      "outputs": [],
      "source": [
        "pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMDB 리뷰 토큰화하기"
      ],
      "metadata": {
        "id": "CC8gmIvKI1M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import csv"
      ],
      "metadata": {
        "id": "sjd8gpGtI0Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"
      ],
      "metadata": {
        "id": "9JYx16jtI36c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('IMDb_Reviews.csv')\n",
        "train_df['review']"
      ],
      "metadata": {
        "id": "mbU11T6LI4Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('리뷰 개수 :',len(train_df)) # 리뷰 개수 출력"
      ],
      "metadata": {
        "id": "qE6JN6dOI5dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('imdb_review.txt', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(train_df['review']))"
      ],
      "metadata": {
        "id": "B4yCW-gUI6vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.Train('--input=imdb_review.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')"
      ],
      "metadata": {
        "id": "69SLEMgrI8Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 인자가 의미하는 바는 다음과 같습니다.\n",
        "\n",
        "* input : 학습시킬 파일\n",
        "\n",
        "* model_prefix : 만들어질 모델 이름\n",
        "\n",
        "* vocab_size : 단어 집합의 크기\n",
        "\n",
        "* model_type : 사용할 모델 (unigram(default), bpe, char, word)\n",
        "\n",
        "* max_sentence_length: 문장의 최대 길이\n",
        "* pad_id, pad_piece: pad token id, 값\n",
        "* unk_id, unk_piece: unknown token id, 값\n",
        "* bos_id, bos_piece: begin of sentence token id, 값\n",
        "* eos_id, eos_piece: end of sequence token id, 값\n",
        "* user_defined_symbols: 사용자 정의 토큰"
      ],
      "metadata": {
        "id": "w1TbLrcxJARX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_list = pd.read_csv('imdb.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "vocab_list.sample(10)"
      ],
      "metadata": {
        "id": "jNtiDxltI-CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab_list)"
      ],
      "metadata": {
        "id": "H6dvgrGtJqoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "vocab_file = \"imdb.model\"\n",
        "sp.load(vocab_file)"
      ],
      "metadata": {
        "id": "H3EdWah0Jruy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* encode_as_pieces : 문장을 입력하면 서브 워드 시퀀스로 변환합니다.\n",
        "\n",
        "* encode_as_ids : 문장을 입력하면 정수 시퀀스로 변환합니다."
      ],
      "metadata": {
        "id": "uuyMtTWDJvpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = [\n",
        "  \"I didn't at all think of it this way.\",\n",
        "  \"I have waited a long time for someone to film\"\n",
        "]\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  print(sp.encode_as_pieces(line))\n",
        "  print(sp.encode_as_ids(line))\n",
        "  print()"
      ],
      "metadata": {
        "id": "Sp6-vxB-Js8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.GetPieceSize()"
      ],
      "metadata": {
        "id": "G6nDtjB1JuPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.IdToPiece(430)"
      ],
      "metadata": {
        "id": "9BFJpyLKJ15K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.PieceToId('▁character')"
      ],
      "metadata": {
        "id": "Pe0xXDeSJ3D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.DecodeIds([41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91])"
      ],
      "metadata": {
        "id": "WoDCJsTMJ4T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.DecodePieces(['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film'])"
      ],
      "metadata": {
        "id": "9QSvOEGdJ5v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sp.encode('I have waited a long time for someone to film', out_type=str))\n",
        "print(sp.encode('I have waited a long time for someone to film', out_type=int))"
      ],
      "metadata": {
        "id": "YR4oAeYlJ746"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "네이버 영화 리뷰 토큰화하기"
      ],
      "metadata": {
        "id": "3ElGy8HGJ_CL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import urllib.request\n",
        "import csv"
      ],
      "metadata": {
        "id": "36J3ef-nKAIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
      ],
      "metadata": {
        "id": "zlXpbgD-KBZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naver_df = pd.read_table('ratings.txt')\n",
        "naver_df[:5]"
      ],
      "metadata": {
        "id": "XxVYjLpmKCas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('리뷰 개수 :',len(naver_df)) # 리뷰 개수 출력"
      ],
      "metadata": {
        "id": "2oFA-1kaKDtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(naver_df.isnull().values.any())"
      ],
      "metadata": {
        "id": "uiqNMEiEKE2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naver_df = naver_df.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "print(naver_df.isnull().values.any()) # Null 값이 존재하는지 확인"
      ],
      "metadata": {
        "id": "6vT5LK4JKF6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('리뷰 개수 :',len(naver_df)) # 리뷰 개수 출력"
      ],
      "metadata": {
        "id": "SDLatiPWKHTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('naver_review.txt', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(naver_df['document']))"
      ],
      "metadata": {
        "id": "VlESCQ25KIrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.Train('--input=naver_review.txt --model_prefix=naver --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')"
      ],
      "metadata": {
        "id": "zXpnbv8VKKPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_list = pd.read_csv('naver.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "vocab_list[:10]"
      ],
      "metadata": {
        "id": "HXRuxH5VKM2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_list.sample(10)"
      ],
      "metadata": {
        "id": "OaFPdhEQKOZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab_list)"
      ],
      "metadata": {
        "id": "OcQKfkg0KP0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "vocab_file = \"naver.model\"\n",
        "sp.load(vocab_file)"
      ],
      "metadata": {
        "id": "x75d-EphKRuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = [\n",
        "  \"뭐 이딴 것도 영화냐.\",\n",
        "  \"진짜 최고의 영화입니다 ㅋㅋ\",\n",
        "]\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  print(sp.encode_as_pieces(line))\n",
        "  print(sp.encode_as_ids(line))\n",
        "  print()"
      ],
      "metadata": {
        "id": "Rio4jgK-KTHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.GetPieceSize()"
      ],
      "metadata": {
        "id": "mWfQUkXsKUZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.IdToPiece(4)"
      ],
      "metadata": {
        "id": "ynw9YNl6KV5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.PieceToId('영화')"
      ],
      "metadata": {
        "id": "TCmiSMrXKXfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.DecodeIds([54, 200, 821, 85])"
      ],
      "metadata": {
        "id": "XkXWTZdPKZTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.DecodePieces(['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ'])"
      ],
      "metadata": {
        "id": "xCfTgLZlKafh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sp.encode('진짜 최고의 영화입니다 ㅋㅋ', out_type=str))\n",
        "print(sp.encode('진짜 최고의 영화입니다 ㅋㅋ', out_type=int))"
      ],
      "metadata": {
        "id": "cTe_gkWnKbxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dj_atzibKc5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔹 1-(4) FastText\n",
        "\n",
        "* 4-2) 부분을 필사해주시면 됩니다.\n",
        "\n",
        "\n",
        "📌 [FastText](https://wikidocs.net/22883)\n",
        "\n",
        "\n",
        "--\n",
        "\n",
        "\n",
        "➕ 한국어 fastText 적용 예제 참고 블로그 (참고용으로만 봐주세요! 필수 과제 아님)\n",
        "* [1](https://inahjeon.dev/fasttext/) \n",
        "* [2](https://vhrehfdl.tistory.com/77)\n",
        "* [3](https://joyhong.tistory.com/137) "
      ],
      "metadata": {
        "id": "USJlH-FQ2zyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**실습으로 비교하는 Word2Vec Vs. FastText**"
      ],
      "metadata": {
        "id": "aKkQ23MRKlXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Word2Vec"
      ],
      "metadata": {
        "id": "nt6aBuIiKnXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(\"electrofishing\")"
      ],
      "metadata": {
        "id": "qcmFu2-K2yBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 이처럼 Word2Vec는 학습 데이터에 존재하지 않는 단어. 즉, 모르는 단어에 대해서는 임베딩 벡터가 존재하지 않기 때문에 단어의 유사도를 계산할 수 없습니다."
      ],
      "metadata": {
        "id": "YLDvSntSKsrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) FastText"
      ],
      "metadata": {
        "id": "1Pi-SjgzKqwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)"
      ],
      "metadata": {
        "id": "i6OEw17AKpHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(\"electrofishing\")"
      ],
      "metadata": {
        "id": "7aNIkerFKu0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec는 학습하지 않은 단어에 대해서 유사한 단어를 찾아내지 못 했지만, FastText는 유사한 단어를 계산해서 출력하고 있음을 볼 수 있습니다."
      ],
      "metadata": {
        "id": "ab1xwcEZKxPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**한국어에서의 FastText**"
      ],
      "metadata": {
        "id": "OpXgCOlSKy8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔹 선택과제 \n",
        "\n",
        "\n",
        "1. [Huggingface Tokenizer](https://wikidocs.net/99893)\n",
        " * 자연어 처리 스타트업 허깅페이스가 개발한 패키지 tokenizers\n",
        " * 자주 등장하는 서브워드들을 하나의 토큰으로 취급하는 다양한 서브워드 토크나이저를 제공\n",
        " * BERT의 WordPiece Tokenizer 실습 - 네이버 영화리뷰 \n",
        "\n",
        "\n",
        "2. [Classifying Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)\n",
        " * 파이토치 실습 \n",
        " * 글자단위 RNN 모델로 18개 언어에서 유래한 수천 개의 성을 학습하고 철자를 기반으로 이름이 어떤 언어에서 유래했는지 예측해보는 문제\n",
        "\n"
      ],
      "metadata": {
        "id": "tkX9sIsE3z3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xcgptSxJ3zhP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}