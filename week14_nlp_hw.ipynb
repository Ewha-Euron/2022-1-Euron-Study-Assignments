{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week14_nlp_hw.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nowionlyseedaylight/2022-1-Euron-Study-Assignments/blob/Week_14/week14_nlp_hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“Œ week14 ê³¼ì œëŠ” **13ì£¼ì°¨ì˜ Subwords ì‹¤ìŠµ**ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ìœ„í‚¤ë…ìŠ¤ì˜ ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ êµì¬ ì‹¤ìŠµ, ê´€ë ¨ ë¸”ë¡œê·¸ ë“±ì˜ ë¬¸ì„œ ìë£Œë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” ê³¼ì œì…ë‹ˆë‹¤. \n",
        "\n",
        "ğŸ“Œ ì•ˆë‚´ëœ ë§í¬ì— ë§ì¶”ì–´ **ì§ì ‘ ì½”ë“œë¥¼ ë”°ë¼ ì¹˜ë©´ì„œ (í•„ì‚¬)** í•´ë‹¹ nlp task ì˜ ê¸°ë³¸ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë©”ì„œë“œë¥¼ ìˆ™ì§€í•´ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ğŸ˜Š í•„ìˆ˜ë¼ê³  ì²´í¬í•œ ë¶€ë¶„ì€ ê³¼ì œì— ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì£¼ì‹œê³ , ì„ íƒìœ¼ë¡œ ì²´í¬í•œ ë¶€ë¶„ì€ ììœ¨ì ìœ¼ë¡œ ìŠ¤í„°ë”” í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ê¶ê¸ˆí•œ ì‚¬í•­ì€ ê¹ƒí—ˆë¸Œ ì´ìŠˆë‚˜, ì¹´í†¡ë°©, ì„¸ì…˜ ë°œí‘œ ì‹œì‘ ì´ì „ ì‹œê°„ ë“±ì„ í™œìš©í•˜ì—¬ ììœ ë¡­ê²Œ ê³µìœ í•´ì£¼ì„¸ìš”!"
      ],
      "metadata": {
        "id": "BX3ac8Ag1RPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk colab í™˜ê²½ì—ì„œ ì‹¤í–‰ì‹œ í•„ìš”í•œ ì½”ë“œì…ë‹ˆë‹¤. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "4JEquLR91VBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **subwords**"
      ],
      "metadata": {
        "id": "HfTr_BPwGc8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‘€ **ë‚´ìš© ë³µìŠµ** \n",
        "\n",
        "* ë‹¨ì–´ ë‹¨ìœ„ì˜ ë¶„ì„ - ê¸€ì ë‹¨ìœ„ì˜ ë¶„ì„ - (ì¼ë¶€ëŠ” ë‹¨ì–´ ì¼ë¶€ëŠ” ê¸€ì ë‹¨ìœ„ì˜ ë¶„ì„ì„ ì§„í–‰ :  hybrid)\n",
        "* ê¸°ì¡´ word ê¸°ë°˜ ëª¨ë¸ì€ (OOV problem + êµ­ê°€ë§ˆë‹¤ ë‹¤ë¥¸ ì–¸ì–´ì²´ê³„) ë‹¨ì ì´ ì¡´ì¬ ğŸ‘‰  character ê¸°ë°˜ ëª¨ë¸ ë“±ì¥ ë°°ê²½\n",
        "  * ê·¸ëŸ¬ë‚˜  Pure Character level model ì˜ ìˆ˜í–‰ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ê±¸ë¦°ë‹¤ëŠ” ë‹¨ì  ì¡´ì¬ \n",
        "  * ë”°ë¼ì„œ ë‹¨ì–´ë¥¼ ì¢€ ë” ìª¼ê°œì„œ ë³´ëŠ” word piece ëª¨ë¸ì´ ë“±ì¥\n",
        "* subword model ğŸ‘‰  word level ëª¨ë¸ê³¼ êµ¬ì¡°ëŠ” ë™ì¼í•˜ë‚˜ ë” ì‘ì€ word ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ì„œ ë³´ëŠ” word pieces ë¥¼ ì‚¬ìš©í•˜ê³  ìˆëŠ” ëª¨ë¸ \n",
        "  * BPE : subword tokenizer ì•Œê³ ë¦¬ì¦˜ \n",
        "  * word piece :  BPE ë³€í˜• ì•Œê³ ë¦¬ì¦˜ ğŸ‘‰ pre-segmentation + BPE â†’ ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ì— ëŒ€í•´ ë¨¼ì € ë‹¨ì–´ì‚¬ì „ì— ì¶”ê°€í•˜ê³  ì´í›„ì— BPE ë¥¼ ì ìš©\n",
        "  * sentence piece : ì¤‘êµ­ì–´ ë“± ë‹¨ì–´ë¡œ êµ¬ë¶„ì´ ì–´ë ¤ìš´ ì–¸ì–´ì˜ ê²½ìš° raw text ì—ì„œ ë°”ë¡œ character-level ë¡œ ë‚˜ë‰˜ì–´ì§ \n",
        "  * hybrid : ì¼ë¶€ëŠ” ê¸€ìë‹¨ìœ„, ì¼ë¶€ëŠ” ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ì„ \n",
        "\n",
        "* FastText : word2vec ì„ ì´ì„ ì°¨ì„¸ëŒ€ word vector learning library ë¡œ í•˜ë‚˜ì˜ ë‹¨ì–´ì— ì—¬ëŸ¬ ë‹¨ì–´ë“¤ì´ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ í•™ìŠµí•œë‹¤.\n",
        "\n",
        "![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbu49K3%2FbtrCOKUFG7Y%2FtbffQ7RkugJ1X2kcyEYGgk%2Fimg.png)"
      ],
      "metadata": {
        "id": "kUpIB0rlG9vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ¥° **ì´í•˜ ì˜ˆì œë¥¼ ì‹¤ìŠµí•˜ì‹œë©´ ë©ë‹ˆë‹¤.**\n",
        "\n",
        "**1-(1)~(4) ëŠ” í•„ìˆ˜ê³¼ì œ, (5) ì˜ 2ê°œëŠ” ì„ íƒê³¼ì œì…ë‹ˆë‹¤.**\n"
      ],
      "metadata": {
        "id": "Kq8aMYKGPQR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ğŸ”¹ 1-(1) BPE\n",
        "\n",
        "* ëŒ€í‘œì ì¸ subword ë¶„ë¦¬ ì•Œê³ ë¦¬ì¦˜ \n",
        "* 3) ì½”ë“œì‹¤ìŠµ ë¶€ë¶„ì„ ìœ„ì£¼ë¡œ í•„ì‚¬í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "ğŸ“Œ [Byte Pair Encoding](https://wikidocs.net/22592)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eB5XfXsWHBHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) ê¸°ì¡´ ë°©ì‹"
      ],
      "metadata": {
        "id": "L_R1AqYTFvO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary\n",
        "# í›ˆë ¨ ë°ì´í„°ì— ìˆëŠ” ë‹¨ì–´ì™€ ë“±ì¥ ë¹ˆë„ìˆ˜\n",
        "low : 5, lower : 2, newest : 6, widest : 3\n",
        "\n",
        "# vocabulary\n",
        "low, lower, newest, widest\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì—ì„œ 'lowest'ì™€ ê°™ì€ ìƒˆë¡œìš´ í˜•íƒœê°€ ë“±ì¥í•˜ë©´ oov ë¬¸ì œê°€ ë°œìƒí•˜ê²Œ ë¨."
      ],
      "metadata": {
        "id": "dpCBqBpuHACO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) BPE ë°©ì‹\n",
        "\n",
        "ë”•ì…”ë„ˆë¦¬ì˜ ëª¨ë“  ë‹¨ì–´ë“¤ì„ character levelë¡œ ë¶„ë¦¬í•¨.\n"
      ],
      "metadata": {
        "id": "56Vqt67EGCkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, collections\n",
        "from IPython.display import display, Markdown, Latex"
      ],
      "metadata": {
        "id": "l7nuCDkkGIYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BPE ì‹¤í–‰ íšŸìˆ˜: 10"
      ],
      "metadata": {
        "id": "qLSWqIs0HC9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_merges = 10"
      ],
      "metadata": {
        "id": "vu78dTY-HCnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = {'l o w </w>' : 5,\n",
        "         'l o w e r </w>' : 2,\n",
        "         'n e w e s t </w>':6,\n",
        "         'w i d e s t </w>':3\n",
        "         }"
      ],
      "metadata": {
        "id": "RWx2_cXPHHIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(dictionary):\n",
        "    # ìœ ë‹ˆê·¸ë¨ì˜ pairë“¤ì˜ ë¹ˆë„ìˆ˜ë¥¼ ì¹´ìš´íŠ¸\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in dictionary.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    print('í˜„ì¬ pairë“¤ì˜ ë¹ˆë„ìˆ˜ :', dict(pairs))\n",
        "    return pairs\n",
        "\n",
        "def merge_dictionary(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "bpe_codes = {}\n",
        "bpe_codes_reverse = {}\n",
        "\n",
        "for i in range(num_merges):\n",
        "    display(Markdown(\"### Iteration {}\".format(i + 1)))\n",
        "    pairs = get_stats(dictionary)\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    dictionary = merge_dictionary(best, dictionary)\n",
        "\n",
        "    bpe_codes[best] = i\n",
        "    bpe_codes_reverse[best[0] + best[1]] = best\n",
        "\n",
        "    print(\"new merge: {}\".format(best))\n",
        "    print(\"dictionary: {}\".format(dictionary))"
      ],
      "metadata": {
        "id": "fBGdZOkSHIWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bpe_codes)"
      ],
      "metadata": {
        "id": "lnD0UphhHNTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* OOVì— ëŒ€ì²˜í•˜ê¸°"
      ],
      "metadata": {
        "id": "mAtjvw6LHT9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as a tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def encode(orig):\n",
        "    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\"\"\"\n",
        "\n",
        "    word = tuple(orig) + ('</w>',)\n",
        "    display(Markdown(\"__word split into characters:__ <tt>{}</tt>\".format(word)))\n",
        "\n",
        "    pairs = get_pairs(word)    \n",
        "\n",
        "    if not pairs:\n",
        "        return orig\n",
        "\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        display(Markdown(\"__Iteration {}:__\".format(iteration)))\n",
        "\n",
        "        print(\"bigrams in the word: {}\".format(pairs))\n",
        "        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float('inf')))\n",
        "        print(\"candidate for merging: {}\".format(bigram))\n",
        "        if bigram not in bpe_codes:\n",
        "            display(Markdown(\"__Candidate not in BPE merges, algorithm stops.__\"))\n",
        "            break\n",
        "        first, second = bigram\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            try:\n",
        "                j = word.index(first, i)\n",
        "                new_word.extend(word[i:j])\n",
        "                i = j\n",
        "            except:\n",
        "                new_word.extend(word[i:])\n",
        "                break\n",
        "\n",
        "            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                new_word.append(first+second)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        new_word = tuple(new_word)\n",
        "        word = new_word\n",
        "        print(\"word after merging: {}\".format(word))\n",
        "        if len(word) == 1:\n",
        "            break\n",
        "        else:\n",
        "            pairs = get_pairs(word)\n",
        "\n",
        "    # íŠ¹ë³„ í† í°ì¸ </w>ëŠ” ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
        "    if word[-1] == '</w>':\n",
        "        word = word[:-1]\n",
        "    elif word[-1].endswith('</w>'):\n",
        "        word = word[:-1] + (word[-1].replace('</w>',''),)\n",
        "\n",
        "    return word"
      ],
      "metadata": {
        "id": "iBCjH5JQHWTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"loki\")"
      ],
      "metadata": {
        "id": "ZoecW6lsHYrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"lowest\")"
      ],
      "metadata": {
        "id": "VVEGnZEGHZxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"lowing\")"
      ],
      "metadata": {
        "id": "MxT6NeCzHd2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"highing\")"
      ],
      "metadata": {
        "id": "jlDShGnTHhCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ğŸ”¹ 1-(2) Subword Text\n",
        "\n",
        "* í…ì„œí”Œë¡œìš°ë¥¼ í†µí•´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì €\n",
        "* BPEì™€ ìœ ì‚¬í•œ ì•Œê³ ë¦¬ì¦˜ì¸ Wordpiece Modelì„ ì±„íƒí•œ íŒ¨í‚¤ì§€\n",
        "* IMDB ë¦¬ë·° í† í°í™”, ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° í† í°í™” ë¶€ë¶„ì„ í•„ì‚¬í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤. \n",
        "\n",
        "```\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_df['review'], target_vocab_size=2**13)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "ğŸ“Œ [SubwordTextEncoder](https://wikidocs.net/86792)"
      ],
      "metadata": {
        "id": "5rVWh1eYPs_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. IMDB ë¦¬ë·° í† í°í™”í•˜ê¸°"
      ],
      "metadata": {
        "id": "rQfYtG50IDwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "HftoL18APttz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")\n",
        "\n",
        "train_df = pd.read_csv('IMDb_Reviews.csv')"
      ],
      "metadata": {
        "id": "ZR-5eTyDIFsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['review']"
      ],
      "metadata": {
        "id": "BtteH1LFIG1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_df['review'], target_vocab_size=2**13)"
      ],
      "metadata": {
        "id": "z3Wa2LaPIIBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.subwords[:100])"
      ],
      "metadata": {
        "id": "UAiJI1xLIJLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df['review'][20])"
      ],
      "metadata": {
        "id": "5jy-weFwIKVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenized sample question: {}'.format(tokenizer.encode(train_df['review'][20])))"
      ],
      "metadata": {
        "id": "j3GOAkpLILZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dfì— ì¡´ì¬í•˜ëŠ” ë¬¸ì¥ ì¤‘ ì¼ë¶€ë¥¼ ë°œì·Œ\n",
        "sample_string = \"It's mind-blowing to me that this film was even made.\"\n",
        "\n",
        "# ì¸ì½”ë”©í•œ ê²°ê³¼ë¥¼ tokenized_stringì— ì €ì¥\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ ë¬¸ì¥ : {}'.format(tokenized_string))\n",
        "\n",
        "# ì´ë¥¼ ë‹¤ì‹œ ë””ì½”ë”©\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('ê¸°ì¡´ ë¬¸ì¥ : {}'.format(original_string))"
      ],
      "metadata": {
        "id": "XToHX5M_IMoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°(Vocab size) :', tokenizer.vocab_size)"
      ],
      "metadata": {
        "id": "nBgAhy2AIOJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
      ],
      "metadata": {
        "id": "hkOlLazfIPZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì•ì„œ ì‹¤ìŠµí•œ ë¬¸ì¥ì— even ë’¤ì— ì„ì˜ë¡œ xyz ì¶”ê°€\n",
        "sample_string = \"It's mind-blowing to me that this film was evenxyz made.\"\n",
        "\n",
        "# ì¸ì½”ë”©í•œ ê²°ê³¼ë¥¼ tokenized_stringì— ì €ì¥\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ ë¬¸ì¥ : {}'.format(tokenized_string))\n",
        "\n",
        "# ì´ë¥¼ ë‹¤ì‹œ ë””ì½”ë”©\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('ê¸°ì¡´ ë¬¸ì¥ : {}'.format(original_string))"
      ],
      "metadata": {
        "id": "KM36MghXIQmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
      ],
      "metadata": {
        "id": "Z4wLCThcISEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° í† í°í™”í•˜ê¸°"
      ],
      "metadata": {
        "id": "HRyF2kyzIU40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "UPdck1h3ITvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "train_data = pd.read_table('ratings_train.txt')"
      ],
      "metadata": {
        "id": "LWOOv-m0IWlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.isnull().sum())"
      ],
      "metadata": {
        "id": "oj5WI5wBIX2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.dropna(how = 'any') # Null ê°’ì´ ì¡´ì¬í•˜ëŠ” í–‰ ì œê±°\n",
        "print(train_data.isnull().values.any()) # Null ê°’ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"
      ],
      "metadata": {
        "id": "aSY0nX80IY5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_data['document'], target_vocab_size=2**13)"
      ],
      "metadata": {
        "id": "3k4h-qgmIaJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.subwords[:100])"
      ],
      "metadata": {
        "id": "kofPNcBnIc25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data['document'][20])"
      ],
      "metadata": {
        "id": "JFzWKdVVIew0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenized sample question: {}'.format(tokenizer.encode(train_data['document'][20])))"
      ],
      "metadata": {
        "id": "DQlHO5FRIgOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_string = train_data['document'][21]\n",
        "\n",
        "# ì¸ì½”ë”©í•œ ê²°ê³¼ë¥¼ tokenized_stringì— ì €ì¥\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ ë¬¸ì¥ : {}'.format(tokenized_string))\n",
        "\n",
        "# ì´ë¥¼ ë‹¤ì‹œ ë””ì½”ë”©\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('ê¸°ì¡´ ë¬¸ì¥ : {}'.format(original_string))"
      ],
      "metadata": {
        "id": "-sgxzj9nIh1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
      ],
      "metadata": {
        "id": "ZwfS7A4QIjit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ğŸ”¹ 1-(3) SentencePiece\n",
        "\n",
        "* ``import sentencepiece as spm`` BPEë¥¼ í¬í•¨í•˜ì—¬ ê¸°íƒ€ ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì§• ì•Œê³ ë¦¬ì¦˜ë“¤ì„ ë‚´ì¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ \n",
        "* IMDB ë¦¬ë·° í† í°í™”, ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° í† í°í™” ë¶€ë¶„ì„ í•„ì‚¬í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤. \n",
        "\n",
        "\n",
        "ğŸ“Œ [SubwordTextEncoder](https://wikidocs.net/86657)"
      ],
      "metadata": {
        "id": "s-FBc3jI2kLH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0sLOZxu06Eo"
      },
      "outputs": [],
      "source": [
        "pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMDB ë¦¬ë·° í† í°í™”í•˜ê¸°"
      ],
      "metadata": {
        "id": "CC8gmIvKI1M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import csv"
      ],
      "metadata": {
        "id": "sjd8gpGtI0Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"
      ],
      "metadata": {
        "id": "9JYx16jtI36c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('IMDb_Reviews.csv')\n",
        "train_df['review']"
      ],
      "metadata": {
        "id": "mbU11T6LI4Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ë¦¬ë·° ê°œìˆ˜ :',len(train_df)) # ë¦¬ë·° ê°œìˆ˜ ì¶œë ¥"
      ],
      "metadata": {
        "id": "qE6JN6dOI5dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('imdb_review.txt', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(train_df['review']))"
      ],
      "metadata": {
        "id": "B4yCW-gUI6vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.Train('--input=imdb_review.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')"
      ],
      "metadata": {
        "id": "69SLEMgrI8Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê° ì¸ìê°€ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "* input : í•™ìŠµì‹œí‚¬ íŒŒì¼\n",
        "\n",
        "* model_prefix : ë§Œë“¤ì–´ì§ˆ ëª¨ë¸ ì´ë¦„\n",
        "\n",
        "* vocab_size : ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°\n",
        "\n",
        "* model_type : ì‚¬ìš©í•  ëª¨ë¸ (unigram(default), bpe, char, word)\n",
        "\n",
        "* max_sentence_length: ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´\n",
        "* pad_id, pad_piece: pad token id, ê°’\n",
        "* unk_id, unk_piece: unknown token id, ê°’\n",
        "* bos_id, bos_piece: begin of sentence token id, ê°’\n",
        "* eos_id, eos_piece: end of sequence token id, ê°’\n",
        "* user_defined_symbols: ì‚¬ìš©ì ì •ì˜ í† í°"
      ],
      "metadata": {
        "id": "w1TbLrcxJARX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_list = pd.read_csv('imdb.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "vocab_list.sample(10)"
      ],
      "metadata": {
        "id": "jNtiDxltI-CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab_list)"
      ],
      "metadata": {
        "id": "H6dvgrGtJqoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "vocab_file = \"imdb.model\"\n",
        "sp.load(vocab_file)"
      ],
      "metadata": {
        "id": "H3EdWah0Jruy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* encode_as_pieces : ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´ ì„œë¸Œ ì›Œë“œ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "* encode_as_ids : ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "uuyMtTWDJvpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = [\n",
        "  \"I didn't at all think of it this way.\",\n",
        "  \"I have waited a long time for someone to film\"\n",
        "]\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  print(sp.encode_as_pieces(line))\n",
        "  print(sp.encode_as_ids(line))\n",
        "  print()"
      ],
      "metadata": {
        "id": "Sp6-vxB-Js8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.GetPieceSize()"
      ],
      "metadata": {
        "id": "G6nDtjB1JuPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.IdToPiece(430)"
      ],
      "metadata": {
        "id": "9BFJpyLKJ15K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.PieceToId('â–character')"
      ],
      "metadata": {
        "id": "Pe0xXDeSJ3D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.DecodeIds([41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91])"
      ],
      "metadata": {
        "id": "WoDCJsTMJ4T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.DecodePieces(['â–I', 'â–have', 'â–wa', 'ited', 'â–a', 'â–long', 'â–time', 'â–for', 'â–someone', 'â–to', 'â–film'])"
      ],
      "metadata": {
        "id": "9QSvOEGdJ5v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sp.encode('I have waited a long time for someone to film', out_type=str))\n",
        "print(sp.encode('I have waited a long time for someone to film', out_type=int))"
      ],
      "metadata": {
        "id": "YR4oAeYlJ746"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° í† í°í™”í•˜ê¸°"
      ],
      "metadata": {
        "id": "3ElGy8HGJ_CL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import urllib.request\n",
        "import csv"
      ],
      "metadata": {
        "id": "36J3ef-nKAIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
      ],
      "metadata": {
        "id": "zlXpbgD-KBZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naver_df = pd.read_table('ratings.txt')\n",
        "naver_df[:5]"
      ],
      "metadata": {
        "id": "XxVYjLpmKCas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ë¦¬ë·° ê°œìˆ˜ :',len(naver_df)) # ë¦¬ë·° ê°œìˆ˜ ì¶œë ¥"
      ],
      "metadata": {
        "id": "2oFA-1kaKDtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(naver_df.isnull().values.any())"
      ],
      "metadata": {
        "id": "uiqNMEiEKE2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naver_df = naver_df.dropna(how = 'any') # Null ê°’ì´ ì¡´ì¬í•˜ëŠ” í–‰ ì œê±°\n",
        "print(naver_df.isnull().values.any()) # Null ê°’ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"
      ],
      "metadata": {
        "id": "6vT5LK4JKF6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ë¦¬ë·° ê°œìˆ˜ :',len(naver_df)) # ë¦¬ë·° ê°œìˆ˜ ì¶œë ¥"
      ],
      "metadata": {
        "id": "SDLatiPWKHTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('naver_review.txt', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(naver_df['document']))"
      ],
      "metadata": {
        "id": "VlESCQ25KIrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.Train('--input=naver_review.txt --model_prefix=naver --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')"
      ],
      "metadata": {
        "id": "zXpnbv8VKKPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_list = pd.read_csv('naver.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "vocab_list[:10]"
      ],
      "metadata": {
        "id": "HXRuxH5VKM2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_list.sample(10)"
      ],
      "metadata": {
        "id": "OaFPdhEQKOZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab_list)"
      ],
      "metadata": {
        "id": "OcQKfkg0KP0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "vocab_file = \"naver.model\"\n",
        "sp.load(vocab_file)"
      ],
      "metadata": {
        "id": "x75d-EphKRuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = [\n",
        "  \"ë­ ì´ë”´ ê²ƒë„ ì˜í™”ëƒ.\",\n",
        "  \"ì§„ì§œ ìµœê³ ì˜ ì˜í™”ì…ë‹ˆë‹¤ ã…‹ã…‹\",\n",
        "]\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  print(sp.encode_as_pieces(line))\n",
        "  print(sp.encode_as_ids(line))\n",
        "  print()"
      ],
      "metadata": {
        "id": "Rio4jgK-KTHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.GetPieceSize()"
      ],
      "metadata": {
        "id": "mWfQUkXsKUZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.IdToPiece(4)"
      ],
      "metadata": {
        "id": "ynw9YNl6KV5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.PieceToId('ì˜í™”')"
      ],
      "metadata": {
        "id": "TCmiSMrXKXfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.DecodeIds([54, 200, 821, 85])"
      ],
      "metadata": {
        "id": "XkXWTZdPKZTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.DecodePieces(['â–ì§„ì§œ', 'â–ìµœê³ ì˜', 'â–ì˜í™”ì…ë‹ˆë‹¤', 'â–á„á„'])"
      ],
      "metadata": {
        "id": "xCfTgLZlKafh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sp.encode('ì§„ì§œ ìµœê³ ì˜ ì˜í™”ì…ë‹ˆë‹¤ ã…‹ã…‹', out_type=str))\n",
        "print(sp.encode('ì§„ì§œ ìµœê³ ì˜ ì˜í™”ì…ë‹ˆë‹¤ ã…‹ã…‹', out_type=int))"
      ],
      "metadata": {
        "id": "cTe_gkWnKbxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dj_atzibKc5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ğŸ”¹ 1-(4) FastText\n",
        "\n",
        "* 4-2) ë¶€ë¶„ì„ í•„ì‚¬í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "ğŸ“Œ [FastText](https://wikidocs.net/22883)\n",
        "\n",
        "\n",
        "--\n",
        "\n",
        "\n",
        "â• í•œêµ­ì–´ fastText ì ìš© ì˜ˆì œ ì°¸ê³  ë¸”ë¡œê·¸ (ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ë´ì£¼ì„¸ìš”! í•„ìˆ˜ ê³¼ì œ ì•„ë‹˜)\n",
        "* [1](https://inahjeon.dev/fasttext/) \n",
        "* [2](https://vhrehfdl.tistory.com/77)\n",
        "* [3](https://joyhong.tistory.com/137) "
      ],
      "metadata": {
        "id": "USJlH-FQ2zyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ì‹¤ìŠµìœ¼ë¡œ ë¹„êµí•˜ëŠ” Word2Vec Vs. FastText**"
      ],
      "metadata": {
        "id": "aKkQ23MRKlXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Word2Vec"
      ],
      "metadata": {
        "id": "nt6aBuIiKnXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(\"electrofishing\")"
      ],
      "metadata": {
        "id": "qcmFu2-K2yBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ì´ì²˜ëŸ¼ Word2VecëŠ” í•™ìŠµ ë°ì´í„°ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´. ì¦‰, ëª¨ë¥´ëŠ” ë‹¨ì–´ì— ëŒ€í•´ì„œëŠ” ì„ë² ë”© ë²¡í„°ê°€ ì¡´ì¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë‹¨ì–´ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "YLDvSntSKsrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) FastText"
      ],
      "metadata": {
        "id": "1Pi-SjgzKqwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)"
      ],
      "metadata": {
        "id": "i6OEw17AKpHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(\"electrofishing\")"
      ],
      "metadata": {
        "id": "7aNIkerFKu0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2VecëŠ” í•™ìŠµí•˜ì§€ ì•Šì€ ë‹¨ì–´ì— ëŒ€í•´ì„œ ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ì°¾ì•„ë‚´ì§€ ëª» í–ˆì§€ë§Œ, FastTextëŠ” ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ê³„ì‚°í•´ì„œ ì¶œë ¥í•˜ê³  ìˆìŒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "ab1xwcEZKxPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**í•œêµ­ì–´ì—ì„œì˜ FastText**"
      ],
      "metadata": {
        "id": "OpXgCOlSKy8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ğŸ”¹ ì„ íƒê³¼ì œ \n",
        "\n",
        "\n",
        "1. [Huggingface Tokenizer](https://wikidocs.net/99893)\n",
        " * ìì—°ì–´ ì²˜ë¦¬ ìŠ¤íƒ€íŠ¸ì—… í—ˆê¹…í˜ì´ìŠ¤ê°€ ê°œë°œí•œ íŒ¨í‚¤ì§€ tokenizers\n",
        " * ìì£¼ ë“±ì¥í•˜ëŠ” ì„œë¸Œì›Œë“œë“¤ì„ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ì·¨ê¸‰í•˜ëŠ” ë‹¤ì–‘í•œ ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì €ë¥¼ ì œê³µ\n",
        " * BERTì˜ WordPiece Tokenizer ì‹¤ìŠµ - ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° \n",
        "\n",
        "\n",
        "2. [Classifying Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)\n",
        " * íŒŒì´í† ì¹˜ ì‹¤ìŠµ \n",
        " * ê¸€ìë‹¨ìœ„ RNN ëª¨ë¸ë¡œ 18ê°œ ì–¸ì–´ì—ì„œ ìœ ë˜í•œ ìˆ˜ì²œ ê°œì˜ ì„±ì„ í•™ìŠµí•˜ê³  ì² ìë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¦„ì´ ì–´ë–¤ ì–¸ì–´ì—ì„œ ìœ ë˜í–ˆëŠ”ì§€ ì˜ˆì¸¡í•´ë³´ëŠ” ë¬¸ì œ\n",
        "\n"
      ],
      "metadata": {
        "id": "tkX9sIsE3z3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xcgptSxJ3zhP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}