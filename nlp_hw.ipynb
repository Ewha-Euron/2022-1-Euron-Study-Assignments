{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week13_nlp_hw.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "📌 week13 과제는 **12주차의 CNN for NLP 실습**으로 구성되어 있습니다.\n",
        "\n",
        "📌 위키독스의 딥러닝을 이용한 자연어 처리 입문 교재 실습, 관련 블로그 등의 문서 자료로 구성되어 있는 과제입니다. \n",
        "\n",
        "📌 안내된 링크에 맞추어 **직접 코드를 따라 치면서 (필사)** 해당 nlp task 의 기본적인 라이브러리와 메서드를 숙지해보시면 좋을 것 같습니다😊 필수라고 체크한 부분은 과제에 반드시 포함시켜주시고, 선택으로 체크한 부분은 자율적으로 스터디 하시면 됩니다.\n",
        "\n",
        "📌 궁금한 사항은 깃허브 이슈나, 카톡방, 세션 발표 시작 이전 시간 등을 활용하여 자유롭게 공유해주세요!"
      ],
      "metadata": {
        "id": "xVSKpAq2EMYr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIohmafuECcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f484110-584a-4c9e-e7fa-1783bddef6ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "# nltk colab 환경에서 실행시 필요한 코드입니다. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1️⃣ **CNN (Convolutional Neural Network)**"
      ],
      "metadata": {
        "id": "HfTr_BPwGc8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👀 **내용 복습** \n",
        "* CNN은 Computer Vison 분야에서 많은 발전을 이뤘던 신경망입니다. 이번 주차에서는 CNN을 NLP에서 어떤 식으로 사용할 수 있을지에 대해 실습하겠습니다.\n",
        "\n",
        "* 참고 자료\n",
        " * [합성곱 신경망(Convolution Neural Network)](https://wikidocs.net/64066)\n",
        " * [자연어 처리를 위한 1D CNN(1D Convolutional Neural Networks)](https://wikidocs.net/80437)\n",
        " * [NLP를 위한 CNN (1): Understanding CNN for NLP](https://reniew.github.io/25/) \n",
        "👉 해당 블로그는 목차의 이어지는 글 전체를 읽어보셔도 좋을 것 같습니다.\n",
        " * [NLP를 위한 Convolutional Neural Networks](https://m.blog.naver.com/rkdwnsdud555/221222217300) 👉 위의 블로그와 같은 원문을 바탕으로 작성된 글입니다. 이미지가 깨져서 안 보일 때 같이 참고하시면 될 듯 합니다."
      ],
      "metadata": {
        "id": "kUpIB0rlG9vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🥰 **이하 예제를 실습하시면 됩니다.**\n"
      ],
      "metadata": {
        "id": "Kq8aMYKGPQR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 1-(1) **IMDB 리뷰 분류**하기\n",
        "\n",
        "* 해당 데이터는 지난 번 GRU Model 실습 때 다뤄보셨을 것입니다. 데이터에 대한 설명은 [IMDB 리뷰 감성 분류하기](https://wikidocs.net/24586)를 참고하시면 됩니다.\n",
        "\n",
        "📌 [1D CNN으로 IMDB 리뷰 분류하기](https://wikidocs.net/80783)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eB5XfXsWHBHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터에 대한 이해"
      ],
      "metadata": {
        "id": "O8tTu9dWJkLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import imdb"
      ],
      "metadata": {
        "id": "Rnrlgd29JlKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
        "\n",
        "print('훈련용 리뷰 개수 : {}'.format(len(X_train)))\n",
        "print('테스트용 리뷰 개수 : {}'.format(len(X_test)))\n",
        "num_classes = len(set(y_train))\n",
        "print('카테고리 : {}'.format(num_classes))"
      ],
      "metadata": {
        "id": "sXrRjAW4Jmef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('첫번째 훈련용 리뷰 :',X_train[0])\n",
        "print('첫번째 훈련용 리뷰의 레이블 :',y_train[0])"
      ],
      "metadata": {
        "id": "ZY2eNe_IJoGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_length = [len(review) for review in X_train]\n",
        "\n",
        "print('리뷰의 최대 길이 : {}'.format(np.max(reviews_length)))\n",
        "print('리뷰의 평균 길이 : {}'.format(np.mean(reviews_length)))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.boxplot(reviews_length)\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(len_result, bins=50)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K4BnTv5DJpVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
        "print(\"각 레이블에 대한 빈도수:\")\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ],
      "metadata": {
        "id": "dLdck0GIJrCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = imdb.get_word_index()\n",
        "index_to_word = {}\n",
        "for key, value in word_to_index.items():\n",
        "    index_to_word[value+3] = key"
      ],
      "metadata": {
        "id": "D92actQrJsYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('빈도수 상위 1등 단어 : {}'.format(index_to_word[4]))"
      ],
      "metadata": {
        "id": "0MwgCjZ2Jto3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('빈도수 상위 3938등 단어 : {}'.format(index_to_word[3941]))"
      ],
      "metadata": {
        "id": "bgABPPKIJu1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "  index_to_word[index] = token\n",
        "\n",
        "print(' '.join([index_to_word[index] for index in X_train[0]]))"
      ],
      "metadata": {
        "id": "gogBKPhsJwFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "vocab_size = 10000\n",
        "max_len = 500\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "AOockYwJJxO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(GRU(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('GRU_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "EYgBBrbnJyqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('GRU_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "id": "jea-mAi_J0LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "  # 알파벳과 숫자를 제외하고 모두 제거 및 알파벳 소문자화\n",
        "  new_sentence = re.sub('[^0-9a-zA-Z ]', '', new_sentence).lower()\n",
        "  encoded = []\n",
        "\n",
        "  # 띄어쓰기 단위 토큰화 후 정수 인코딩\n",
        "  for word in new_sentence.split():\n",
        "    try :\n",
        "      # 단어 집합의 크기를 10,000으로 제한.\n",
        "      if word_to_index[word] <= 10000:\n",
        "        encoded.append(word_to_index[word]+3)\n",
        "      else:\n",
        "      # 10,000 이상의 숫자는 <unk> 토큰으로 변환.\n",
        "        encoded.append(2)\n",
        "    # 단어 집합에 없는 단어는 <unk> 토큰으로 변환.\n",
        "    except KeyError:\n",
        "      encoded.append(2)\n",
        "\n",
        "  pad_sequence = pad_sequences([encoded], maxlen=max_len)\n",
        "  score = float(loaded_model.predict(pad_sequence)) # 예측\n",
        "\n",
        "  if(score > 0.5):\n",
        "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\".format(score * 100))\n",
        "  else:\n",
        "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\".format((1 - score) * 100))"
      ],
      "metadata": {
        "id": "WgQhxk4nJ1YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = \"This movie was just way too overrated. The fighting was not professional and in slow motion. I was expecting more from a 200 million budget movie. The little sister of T.Challa was just trying too hard to be funny. The story was really dumb as well. Don't watch this movie if you are going because others say its great unless you are a Black Panther fan or Marvels fan.\"\n",
        "\n",
        "sentiment_predict(test_input)"
      ],
      "metadata": {
        "id": "XuekeQFuJ3cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = \" I was lucky enough to be included in the group to see the advanced screening in Melbourne on the 15th of April, 2012. And, firstly, I need to say a big thank-you to Disney and Marvel Studios. \\\n",
        "Now, the film... how can I even begin to explain how I feel about this film? It is, as the title of this review says a 'comic book triumph'. I went into the film with very, very high expectations and I was not disappointed. \\\n",
        "Seeing Joss Whedon's direction and envisioning of the film come to life on the big screen is perfect. The script is amazingly detailed and laced with sharp wit a humor. The special effects are literally mind-blowing and the action scenes are both hard-hitting and beautifully choreographed.\"\n",
        "\n",
        "sentiment_predict(test_input)"
      ],
      "metadata": {
        "id": "Snh4dtfoJ60N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 데이터에 대한 전처리"
      ],
      "metadata": {
        "id": "S5hIGkF3IYkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "dpCBqBpuHACO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=vocab_size)"
      ],
      "metadata": {
        "id": "eZU2XcG1IPV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[:5])"
      ],
      "metadata": {
        "id": "4TqfGPnPIQ3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 200\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "Dxu3Yz8nISKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('X_train의 크기(shape) :',X_train.shape)\n",
        "print('X_test의 크기(shape) :',X_test.shape)"
      ],
      "metadata": {
        "id": "D98-4B3NITrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[:5])"
      ],
      "metadata": {
        "id": "VYWROdvYIU-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 1D CNN으로 IMDB 리뷰 분류하기"
      ],
      "metadata": {
        "id": "HlCPxjwRIbAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "embedding_dim = 256 # 임베딩 벡터의 차원\n",
        "dropout_ratio = 0.3 # 드롭아웃 비율\n",
        "num_filters = 256 # 커널의 수\n",
        "kernel_size = 3 # 커널의 크기\n",
        "hidden_units = 128 # 뉴런의 수\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(Dropout(dropout_ratio))\n",
        "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(hidden_units, activation='relu'))\n",
        "model.add(Dropout(dropout_ratio))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc])"
      ],
      "metadata": {
        "id": "-KwMk2niIWmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "id": "_EimMWLPIgTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 1-(2) **스팸 메일 분류**하기\n",
        "* 데이터나 데이터 전처리에 대한 설명은 [스팸 메일 분류하기](https://wikidocs.net/22894)를 참고하시면 됩니다.\n",
        "\n",
        "📌 [1D CNN으로 스팸 메일 분류하기](https://wikidocs.net/80787)"
      ],
      "metadata": {
        "id": "5rVWh1eYPs_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터, 데이터 전처리에 대한 설명"
      ],
      "metadata": {
        "id": "GKRmnmPeIzuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "CQEMfBodIzXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/10.%20RNN%20Text%20Classification/dataset/spam.csv\", filename=\"spam.csv\")\n",
        "data = pd.read_csv('spam.csv', encoding='latin1')\n",
        "print('총 샘플의 수 :',len(data))"
      ],
      "metadata": {
        "id": "2mjXg35jI2-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:5]"
      ],
      "metadata": {
        "id": "nrSI3koXI8Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del data['Unnamed: 2']\n",
        "del data['Unnamed: 3']\n",
        "del data['Unnamed: 4']\n",
        "data['v1'] = data['v1'].replace(['ham','spam'],[0,1])\n",
        "data[:5]"
      ],
      "metadata": {
        "id": "DbSTE_jSI9yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "BnYfC5tBI_J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('결측값 여부 :',data.isnull().values.any())"
      ],
      "metadata": {
        "id": "7ScokyyXJAwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('v2열의 유니크한 값 :',data['v2'].nunique())"
      ],
      "metadata": {
        "id": "jVsx2-29JCBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# v2 열에서 중복인 내용이 있다면 중복 제거\n",
        "data.drop_duplicates(subset=['v2'], inplace=True)\n",
        "print('총 샘플의 수 :',len(data))"
      ],
      "metadata": {
        "id": "aiPsqUR2JDPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['v1'].value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "D9bvQKVUJErG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('정상 메일과 스팸 메일의 개수')\n",
        "print(data.groupby('v1').size().reset_index(name='count'))"
      ],
      "metadata": {
        "id": "2YSlTiHCJF_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'정상 메일의 비율 = {round(data[\"v1\"].value_counts()[0]/len(data) * 100,3)}%')\n",
        "print(f'스팸 메일의 비율 = {round(data[\"v1\"].value_counts()[1]/len(data) * 100,3)}%')"
      ],
      "metadata": {
        "id": "kcLLSD-iJHVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = data['v2']\n",
        "y_data = data['v1']\n",
        "print('메일 본문의 개수: {}'.format(len(X_data)))\n",
        "print('레이블의 개수: {}'.format(len(y_data)))"
      ],
      "metadata": {
        "id": "X3EVGKC5JInU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=0, stratify=y_data)"
      ],
      "metadata": {
        "id": "WiViYZagJJ2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('--------훈련 데이터의 비율-----------')\n",
        "print(f'정상 메일 = {round(y_train.value_counts()[0]/len(y_train) * 100,3)}%')\n",
        "print(f'스팸 메일 = {round(y_train.value_counts()[1]/len(y_train) * 100,3)}%')"
      ],
      "metadata": {
        "id": "8sfU8P-GJL9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('--------테스트 데이터의 비율-----------')\n",
        "print(f'정상 메일 = {round(y_test.value_counts()[0]/len(y_test) * 100,3)}%')\n",
        "print(f'스팸 메일 = {round(y_test.value_counts()[1]/len(y_test) * 100,3)}%')"
      ],
      "metadata": {
        "id": "UW__BQXiJNVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_encoded = tokenizer.texts_to_sequences(X_train)\n",
        "print(X_train_encoded[:5])"
      ],
      "metadata": {
        "id": "9_CH-L5pJOhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = tokenizer.word_index\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "id": "lSpQbyDSJPjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 2\n",
        "total_cnt = len(word_to_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합(vocabulary)에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ],
      "metadata": {
        "id": "okd0_VSoJQsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index) + 1\n",
        "print('단어 집합의 크기: {}'.format((vocab_size)))"
      ],
      "metadata": {
        "id": "GSiDSTqyJSZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('메일의 최대 길이 : %d' % max(len(sample) for sample in X_train_encoded))\n",
        "print('메일의 평균 길이 : %f' % (sum(map(len, X_train_encoded))/len(X_train_encoded)))\n",
        "plt.hist([len(sample) for sample in X_data], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3PZxd1BSJUC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 189\n",
        "X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)\n",
        "print(\"훈련 데이터의 크기(shape):\", X_train_padded.shape)"
      ],
      "metadata": {
        "id": "C5fB8XOiJVLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "embedding_dim = 32\n",
        "hidden_units = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train_padded, y_train, epochs=4, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "_MlU9Nc7JWoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_encoded = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_encoded, maxlen = max_len)\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test_padded, y_test)[1]))"
      ],
      "metadata": {
        "id": "EuUMUokxJctk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(history.history['acc']) + 1)\n",
        "plt.plot(epochs, history.history['loss'])\n",
        "plt.plot(epochs, history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uZuN0b1PJd-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1D CNN으로 스팸 메일 분류하기"
      ],
      "metadata": {
        "id": "I-wg5H02I3j4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding, Dropout, MaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 32\n",
        "dropout_ratio = 0.3\n",
        "num_filters = 32\n",
        "kernel_size = 5\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(Dropout(dropout_ratio))\n",
        "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(dropout_ratio))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor = 'val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit(X_train_padded, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[es, mc])"
      ],
      "metadata": {
        "id": "HftoL18APttz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_encoded = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_encoded, maxlen = max_len)\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test_padded, y_test)[1]))"
      ],
      "metadata": {
        "id": "fmmlVLubItAw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}