{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week13_nlp_hw.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“Œ week13 ê³¼ì œëŠ” **12ì£¼ì°¨ì˜ CNN for NLP ì‹¤ìŠµ**ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ìœ„í‚¤ë…ìŠ¤ì˜ ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ êµì¬ ì‹¤ìŠµ, ê´€ë ¨ ë¸”ë¡œê·¸ ë“±ì˜ ë¬¸ì„œ ìë£Œë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” ê³¼ì œì…ë‹ˆë‹¤. \n",
        "\n",
        "ğŸ“Œ ì•ˆë‚´ëœ ë§í¬ì— ë§ì¶”ì–´ **ì§ì ‘ ì½”ë“œë¥¼ ë”°ë¼ ì¹˜ë©´ì„œ (í•„ì‚¬)** í•´ë‹¹ nlp task ì˜ ê¸°ë³¸ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë©”ì„œë“œë¥¼ ìˆ™ì§€í•´ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ğŸ˜Š í•„ìˆ˜ë¼ê³  ì²´í¬í•œ ë¶€ë¶„ì€ ê³¼ì œì— ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì£¼ì‹œê³ , ì„ íƒìœ¼ë¡œ ì²´í¬í•œ ë¶€ë¶„ì€ ììœ¨ì ìœ¼ë¡œ ìŠ¤í„°ë”” í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ê¶ê¸ˆí•œ ì‚¬í•­ì€ ê¹ƒí—ˆë¸Œ ì´ìŠˆë‚˜, ì¹´í†¡ë°©, ì„¸ì…˜ ë°œí‘œ ì‹œì‘ ì´ì „ ì‹œê°„ ë“±ì„ í™œìš©í•˜ì—¬ ììœ ë¡­ê²Œ ê³µìœ í•´ì£¼ì„¸ìš”!"
      ],
      "metadata": {
        "id": "xVSKpAq2EMYr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIohmafuECcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f484110-584a-4c9e-e7fa-1783bddef6ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "# nltk colab í™˜ê²½ì—ì„œ ì‹¤í–‰ì‹œ í•„ìš”í•œ ì½”ë“œì…ë‹ˆë‹¤. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1ï¸âƒ£ **CNN (Convolutional Neural Network)**"
      ],
      "metadata": {
        "id": "HfTr_BPwGc8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‘€ **ë‚´ìš© ë³µìŠµ** \n",
        "* CNNì€ Computer Vison ë¶„ì•¼ì—ì„œ ë§ì€ ë°œì „ì„ ì´ë¤˜ë˜ ì‹ ê²½ë§ì…ë‹ˆë‹¤. ì´ë²ˆ ì£¼ì°¨ì—ì„œëŠ” CNNì„ NLPì—ì„œ ì–´ë–¤ ì‹ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆì„ì§€ì— ëŒ€í•´ ì‹¤ìŠµí•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "* ì°¸ê³  ìë£Œ\n",
        " * [í•©ì„±ê³± ì‹ ê²½ë§(Convolution Neural Network)](https://wikidocs.net/64066)\n",
        " * [ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ 1D CNN(1D Convolutional Neural Networks)](https://wikidocs.net/80437)\n",
        " * [NLPë¥¼ ìœ„í•œ CNN (1): Understanding CNN for NLP](https://reniew.github.io/25/) \n",
        "ğŸ‘‰ í•´ë‹¹ ë¸”ë¡œê·¸ëŠ” ëª©ì°¨ì˜ ì´ì–´ì§€ëŠ” ê¸€ ì „ì²´ë¥¼ ì½ì–´ë³´ì…”ë„ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
        " * [NLPë¥¼ ìœ„í•œ Convolutional Neural Networks](https://m.blog.naver.com/rkdwnsdud555/221222217300) ğŸ‘‰ ìœ„ì˜ ë¸”ë¡œê·¸ì™€ ê°™ì€ ì›ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ëœ ê¸€ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ê°€ ê¹¨ì ¸ì„œ ì•ˆ ë³´ì¼ ë•Œ ê°™ì´ ì°¸ê³ í•˜ì‹œë©´ ë  ë“¯ í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "kUpIB0rlG9vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ¥° **ì´í•˜ ì˜ˆì œë¥¼ ì‹¤ìŠµí•˜ì‹œë©´ ë©ë‹ˆë‹¤.**\n"
      ],
      "metadata": {
        "id": "Kq8aMYKGPQR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 1-(1) **IMDB ë¦¬ë·° ë¶„ë¥˜**í•˜ê¸°\n",
        "\n",
        "* í•´ë‹¹ ë°ì´í„°ëŠ” ì§€ë‚œ ë²ˆ GRU Model ì‹¤ìŠµ ë•Œ ë‹¤ë¤„ë³´ì…¨ì„ ê²ƒì…ë‹ˆë‹¤. ë°ì´í„°ì— ëŒ€í•œ ì„¤ëª…ì€ [IMDB ë¦¬ë·° ê°ì„± ë¶„ë¥˜í•˜ê¸°](https://wikidocs.net/24586)ë¥¼ ì°¸ê³ í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ [1D CNNìœ¼ë¡œ IMDB ë¦¬ë·° ë¶„ë¥˜í•˜ê¸°](https://wikidocs.net/80783)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eB5XfXsWHBHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë°ì´í„°ì— ëŒ€í•œ ì´í•´"
      ],
      "metadata": {
        "id": "O8tTu9dWJkLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import imdb"
      ],
      "metadata": {
        "id": "Rnrlgd29JlKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
        "\n",
        "print('í›ˆë ¨ìš© ë¦¬ë·° ê°œìˆ˜ : {}'.format(len(X_train)))\n",
        "print('í…ŒìŠ¤íŠ¸ìš© ë¦¬ë·° ê°œìˆ˜ : {}'.format(len(X_test)))\n",
        "num_classes = len(set(y_train))\n",
        "print('ì¹´í…Œê³ ë¦¬ : {}'.format(num_classes))"
      ],
      "metadata": {
        "id": "sXrRjAW4Jmef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ì²«ë²ˆì§¸ í›ˆë ¨ìš© ë¦¬ë·° :',X_train[0])\n",
        "print('ì²«ë²ˆì§¸ í›ˆë ¨ìš© ë¦¬ë·°ì˜ ë ˆì´ë¸” :',y_train[0])"
      ],
      "metadata": {
        "id": "ZY2eNe_IJoGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_length = [len(review) for review in X_train]\n",
        "\n",
        "print('ë¦¬ë·°ì˜ ìµœëŒ€ ê¸¸ì´ : {}'.format(np.max(reviews_length)))\n",
        "print('ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ : {}'.format(np.mean(reviews_length)))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.boxplot(reviews_length)\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(len_result, bins=50)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K4BnTv5DJpVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
        "print(\"ê° ë ˆì´ë¸”ì— ëŒ€í•œ ë¹ˆë„ìˆ˜:\")\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ],
      "metadata": {
        "id": "dLdck0GIJrCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = imdb.get_word_index()\n",
        "index_to_word = {}\n",
        "for key, value in word_to_index.items():\n",
        "    index_to_word[value+3] = key"
      ],
      "metadata": {
        "id": "D92actQrJsYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ë¹ˆë„ìˆ˜ ìƒìœ„ 1ë“± ë‹¨ì–´ : {}'.format(index_to_word[4]))"
      ],
      "metadata": {
        "id": "0MwgCjZ2Jto3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ë¹ˆë„ìˆ˜ ìƒìœ„ 3938ë“± ë‹¨ì–´ : {}'.format(index_to_word[3941]))"
      ],
      "metadata": {
        "id": "bgABPPKIJu1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "  index_to_word[index] = token\n",
        "\n",
        "print(' '.join([index_to_word[index] for index in X_train[0]]))"
      ],
      "metadata": {
        "id": "gogBKPhsJwFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "vocab_size = 10000\n",
        "max_len = 500\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "AOockYwJJxO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(GRU(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('GRU_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "EYgBBrbnJyqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('GRU_model.h5')\n",
        "print(\"\\n í…ŒìŠ¤íŠ¸ ì •í™•ë„: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "id": "jea-mAi_J0LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "  # ì•ŒíŒŒë²³ê³¼ ìˆ«ìë¥¼ ì œì™¸í•˜ê³  ëª¨ë‘ ì œê±° ë° ì•ŒíŒŒë²³ ì†Œë¬¸ìí™”\n",
        "  new_sentence = re.sub('[^0-9a-zA-Z ]', '', new_sentence).lower()\n",
        "  encoded = []\n",
        "\n",
        "  # ë„ì–´ì“°ê¸° ë‹¨ìœ„ í† í°í™” í›„ ì •ìˆ˜ ì¸ì½”ë”©\n",
        "  for word in new_sentence.split():\n",
        "    try :\n",
        "      # ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë¥¼ 10,000ìœ¼ë¡œ ì œí•œ.\n",
        "      if word_to_index[word] <= 10000:\n",
        "        encoded.append(word_to_index[word]+3)\n",
        "      else:\n",
        "      # 10,000 ì´ìƒì˜ ìˆ«ìëŠ” <unk> í† í°ìœ¼ë¡œ ë³€í™˜.\n",
        "        encoded.append(2)\n",
        "    # ë‹¨ì–´ ì§‘í•©ì— ì—†ëŠ” ë‹¨ì–´ëŠ” <unk> í† í°ìœ¼ë¡œ ë³€í™˜.\n",
        "    except KeyError:\n",
        "      encoded.append(2)\n",
        "\n",
        "  pad_sequence = pad_sequences([encoded], maxlen=max_len)\n",
        "  score = float(loaded_model.predict(pad_sequence)) # ì˜ˆì¸¡\n",
        "\n",
        "  if(score > 0.5):\n",
        "    print(\"{:.2f}% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\".format(score * 100))\n",
        "  else:\n",
        "    print(\"{:.2f}% í™•ë¥ ë¡œ ë¶€ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\".format((1 - score) * 100))"
      ],
      "metadata": {
        "id": "WgQhxk4nJ1YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = \"This movie was just way too overrated. The fighting was not professional and in slow motion. I was expecting more from a 200 million budget movie. The little sister of T.Challa was just trying too hard to be funny. The story was really dumb as well. Don't watch this movie if you are going because others say its great unless you are a Black Panther fan or Marvels fan.\"\n",
        "\n",
        "sentiment_predict(test_input)"
      ],
      "metadata": {
        "id": "XuekeQFuJ3cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = \" I was lucky enough to be included in the group to see the advanced screening in Melbourne on the 15th of April, 2012. And, firstly, I need to say a big thank-you to Disney and Marvel Studios. \\\n",
        "Now, the film... how can I even begin to explain how I feel about this film? It is, as the title of this review says a 'comic book triumph'. I went into the film with very, very high expectations and I was not disappointed. \\\n",
        "Seeing Joss Whedon's direction and envisioning of the film come to life on the big screen is perfect. The script is amazingly detailed and laced with sharp wit a humor. The special effects are literally mind-blowing and the action scenes are both hard-hitting and beautifully choreographed.\"\n",
        "\n",
        "sentiment_predict(test_input)"
      ],
      "metadata": {
        "id": "Snh4dtfoJ60N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. ë°ì´í„°ì— ëŒ€í•œ ì „ì²˜ë¦¬"
      ],
      "metadata": {
        "id": "S5hIGkF3IYkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "dpCBqBpuHACO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=vocab_size)"
      ],
      "metadata": {
        "id": "eZU2XcG1IPV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[:5])"
      ],
      "metadata": {
        "id": "4TqfGPnPIQ3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 200\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "Dxu3Yz8nISKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('X_trainì˜ í¬ê¸°(shape) :',X_train.shape)\n",
        "print('X_testì˜ í¬ê¸°(shape) :',X_test.shape)"
      ],
      "metadata": {
        "id": "D98-4B3NITrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[:5])"
      ],
      "metadata": {
        "id": "VYWROdvYIU-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 1D CNNìœ¼ë¡œ IMDB ë¦¬ë·° ë¶„ë¥˜í•˜ê¸°"
      ],
      "metadata": {
        "id": "HlCPxjwRIbAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "embedding_dim = 256 # ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›\n",
        "dropout_ratio = 0.3 # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
        "num_filters = 256 # ì»¤ë„ì˜ ìˆ˜\n",
        "kernel_size = 3 # ì»¤ë„ì˜ í¬ê¸°\n",
        "hidden_units = 128 # ë‰´ëŸ°ì˜ ìˆ˜\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(Dropout(dropout_ratio))\n",
        "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(hidden_units, activation='relu'))\n",
        "model.add(Dropout(dropout_ratio))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc])"
      ],
      "metadata": {
        "id": "-KwMk2niIWmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n í…ŒìŠ¤íŠ¸ ì •í™•ë„: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "id": "_EimMWLPIgTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 1-(2) **ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜**í•˜ê¸°\n",
        "* ë°ì´í„°ë‚˜ ë°ì´í„° ì „ì²˜ë¦¬ì— ëŒ€í•œ ì„¤ëª…ì€ [ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜í•˜ê¸°](https://wikidocs.net/22894)ë¥¼ ì°¸ê³ í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ [1D CNNìœ¼ë¡œ ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜í•˜ê¸°](https://wikidocs.net/80787)"
      ],
      "metadata": {
        "id": "5rVWh1eYPs_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë°ì´í„°, ë°ì´í„° ì „ì²˜ë¦¬ì— ëŒ€í•œ ì„¤ëª…"
      ],
      "metadata": {
        "id": "GKRmnmPeIzuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "CQEMfBodIzXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/10.%20RNN%20Text%20Classification/dataset/spam.csv\", filename=\"spam.csv\")\n",
        "data = pd.read_csv('spam.csv', encoding='latin1')\n",
        "print('ì´ ìƒ˜í”Œì˜ ìˆ˜ :',len(data))"
      ],
      "metadata": {
        "id": "2mjXg35jI2-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:5]"
      ],
      "metadata": {
        "id": "nrSI3koXI8Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del data['Unnamed: 2']\n",
        "del data['Unnamed: 3']\n",
        "del data['Unnamed: 4']\n",
        "data['v1'] = data['v1'].replace(['ham','spam'],[0,1])\n",
        "data[:5]"
      ],
      "metadata": {
        "id": "DbSTE_jSI9yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "BnYfC5tBI_J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ê²°ì¸¡ê°’ ì—¬ë¶€ :',data.isnull().values.any())"
      ],
      "metadata": {
        "id": "7ScokyyXJAwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('v2ì—´ì˜ ìœ ë‹ˆí¬í•œ ê°’ :',data['v2'].nunique())"
      ],
      "metadata": {
        "id": "jVsx2-29JCBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# v2 ì—´ì—ì„œ ì¤‘ë³µì¸ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì¤‘ë³µ ì œê±°\n",
        "data.drop_duplicates(subset=['v2'], inplace=True)\n",
        "print('ì´ ìƒ˜í”Œì˜ ìˆ˜ :',len(data))"
      ],
      "metadata": {
        "id": "aiPsqUR2JDPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['v1'].value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "D9bvQKVUJErG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ì •ìƒ ë©”ì¼ê³¼ ìŠ¤íŒ¸ ë©”ì¼ì˜ ê°œìˆ˜')\n",
        "print(data.groupby('v1').size().reset_index(name='count'))"
      ],
      "metadata": {
        "id": "2YSlTiHCJF_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'ì •ìƒ ë©”ì¼ì˜ ë¹„ìœ¨ = {round(data[\"v1\"].value_counts()[0]/len(data) * 100,3)}%')\n",
        "print(f'ìŠ¤íŒ¸ ë©”ì¼ì˜ ë¹„ìœ¨ = {round(data[\"v1\"].value_counts()[1]/len(data) * 100,3)}%')"
      ],
      "metadata": {
        "id": "kcLLSD-iJHVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = data['v2']\n",
        "y_data = data['v1']\n",
        "print('ë©”ì¼ ë³¸ë¬¸ì˜ ê°œìˆ˜: {}'.format(len(X_data)))\n",
        "print('ë ˆì´ë¸”ì˜ ê°œìˆ˜: {}'.format(len(y_data)))"
      ],
      "metadata": {
        "id": "X3EVGKC5JInU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=0, stratify=y_data)"
      ],
      "metadata": {
        "id": "WiViYZagJJ2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('--------í›ˆë ¨ ë°ì´í„°ì˜ ë¹„ìœ¨-----------')\n",
        "print(f'ì •ìƒ ë©”ì¼ = {round(y_train.value_counts()[0]/len(y_train) * 100,3)}%')\n",
        "print(f'ìŠ¤íŒ¸ ë©”ì¼ = {round(y_train.value_counts()[1]/len(y_train) * 100,3)}%')"
      ],
      "metadata": {
        "id": "8sfU8P-GJL9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('--------í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¹„ìœ¨-----------')\n",
        "print(f'ì •ìƒ ë©”ì¼ = {round(y_test.value_counts()[0]/len(y_test) * 100,3)}%')\n",
        "print(f'ìŠ¤íŒ¸ ë©”ì¼ = {round(y_test.value_counts()[1]/len(y_test) * 100,3)}%')"
      ],
      "metadata": {
        "id": "UW__BQXiJNVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_encoded = tokenizer.texts_to_sequences(X_train)\n",
        "print(X_train_encoded[:5])"
      ],
      "metadata": {
        "id": "9_CH-L5pJOhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = tokenizer.word_index\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "id": "lSpQbyDSJPjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 2\n",
        "total_cnt = len(word_to_index) # ë‹¨ì–´ì˜ ìˆ˜\n",
        "rare_cnt = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸\n",
        "total_freq = 0 # í›ˆë ¨ ë°ì´í„°ì˜ ì „ì²´ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì´ í•©\n",
        "rare_freq = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ì˜ ì´ í•©\n",
        "\n",
        "# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ì˜ ìŒ(pair)ì„ keyì™€ valueë¡œ ë°›ëŠ”ë‹¤.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ìœ¼ë©´\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"ë‹¨ì–´ ì§‘í•©(vocabulary)ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:\", (rare_freq / total_freq)*100)"
      ],
      "metadata": {
        "id": "okd0_VSoJQsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index) + 1\n",
        "print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°: {}'.format((vocab_size)))"
      ],
      "metadata": {
        "id": "GSiDSTqyJSZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ë©”ì¼ì˜ ìµœëŒ€ ê¸¸ì´ : %d' % max(len(sample) for sample in X_train_encoded))\n",
        "print('ë©”ì¼ì˜ í‰ê·  ê¸¸ì´ : %f' % (sum(map(len, X_train_encoded))/len(X_train_encoded)))\n",
        "plt.hist([len(sample) for sample in X_data], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3PZxd1BSJUC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 189\n",
        "X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)\n",
        "print(\"í›ˆë ¨ ë°ì´í„°ì˜ í¬ê¸°(shape):\", X_train_padded.shape)"
      ],
      "metadata": {
        "id": "C5fB8XOiJVLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "embedding_dim = 32\n",
        "hidden_units = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train_padded, y_train, epochs=4, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "_MlU9Nc7JWoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_encoded = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_encoded, maxlen = max_len)\n",
        "print(\"\\n í…ŒìŠ¤íŠ¸ ì •í™•ë„: %.4f\" % (model.evaluate(X_test_padded, y_test)[1]))"
      ],
      "metadata": {
        "id": "EuUMUokxJctk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(history.history['acc']) + 1)\n",
        "plt.plot(epochs, history.history['loss'])\n",
        "plt.plot(epochs, history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uZuN0b1PJd-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1D CNNìœ¼ë¡œ ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜í•˜ê¸°"
      ],
      "metadata": {
        "id": "I-wg5H02I3j4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding, Dropout, MaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 32\n",
        "dropout_ratio = 0.3\n",
        "num_filters = 32\n",
        "kernel_size = 5\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(Dropout(dropout_ratio))\n",
        "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(dropout_ratio))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor = 'val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit(X_train_padded, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[es, mc])"
      ],
      "metadata": {
        "id": "HftoL18APttz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_encoded = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_encoded, maxlen = max_len)\n",
        "print(\"\\n í…ŒìŠ¤íŠ¸ ì •í™•ë„: %.4f\" % (model.evaluate(X_test_padded, y_test)[1]))"
      ],
      "metadata": {
        "id": "fmmlVLubItAw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}