정리하자면, Gradient Descent을 이용해 손실 함수가 최소화되는 가중치 w를 수정해나간다. 
여기서 계산방식의 효율이 높은 analytic gradient 방식으로 chain rule을 이용한다. 
이 계산 방식을 3가지의 예제를 통해 익힐 것이며, 여기서 쓰이는게 바로 Back propagation(오차 역전파)이다. 
결과값에서 거꾸로 거슬러서 가중치 w를 계산해주는 방식이다. 

(*주의)
오차역전파는 가중치를 계산하는 방식으로 생각하고, 경사 하강법은 가중치를 업데이트하는 방식으로 생각하면 된다. 
그래서 이번 4장은 업데이트가 아닌 가중치를 계산하는 방법만 다룬다.

AlexNet이나 Neural Turing Machine을 보면 input에서 loss function를 도출할 때까지의 노드가 굉장히 많다.
이런 방대한 모델의 가중치 계산을 일일이 하게 되면 오래 걸리지만, Back propagtion을 이용하면 chain rule을 통해 반복 계산으로 전부 구할 수 있다.

강의 예시 (작성하려다가 gitbhub에 Latex 적용 어떻게 하는지 몰라서 스킵합니다.)
간단한 함수 하나를 설정하고 이 함수 모델에 따라 반대로 거슬러 올라가면서 가중치를 계산해준다. (=Back propagation)
최종 값은 df/df = 1이다.
(중략)
chain rule을 이용해서 합성 함수의 미분을 풀어낸다. 이 때 global gradient * local gradient로 표현한다.
이런 식으로 계산 과정을 통해서 합성 함수의 미분을 풀어내는 과정을 보여준다.

간단한 함수를 다룬 뒤에는 sigmoid 함수에 back propagation을 적용하는 것을 보여준다.

예시 함수들에 back propagation을 적용하는 것들을 적기에는 text로 작성하는 것이 너무 시간이 오래 걸리고 latex와 이미지 첨부가 안 돼서 스킵.

어쨋든 이런 식으로 계산한 것들을 보면 사실 sigmoid 함수를 이용해 일반화가 가능하다는 것을 알 수 있다.
위 과정들에서 chain rule을 통해 구해온 것들은 사실 sigmoid gate를 통해 한 번에 계산이 가능하고 일반화된 공식이 있다.

add gate: gradient distributor (현재의 가중치를 다음 단계로 뿌려줌)
max gate: gradient router (큰 값에 가중치를 부여)
mul gate: gradient switcher (서로의 값을 바꿔줌)
이렇게 gate들을 일반화 시킬 수 있다.

이미지 데이터를 다루고자 할 때는 모든 픽셀 값들을 1차원 array로 재배치 하기 때문에 차원이 굉장이 높다. class도 여러 개인 이미지 데이터의 가중치는 매우 높은 차원의 행렬 데이터일 것이기 때문에 이 때는 Jacobian matrix(행렬의 미분 연산)을 이용한다.
그렇게 하면 diagonal의 특성에 따라 대각 행렬만 계산하면 돼서 연산이 좀 간단해진다.

다음 예시로는 L2 regularization function f를 다룬다.
여기서 f(x, W)의 x는 n차원, W는 n*n 차원이다. 이 예씨는 2장에서의 linear regression의 예시ㅣ로 접해본 구성이다. 클래스가 3개라서 w의 차원이 x차원보다 많다.
L2의 최종값은 0.116이 나오고, 출력 노드의 미분값은 1이다. L2 노드 이전의 [0.22, 0.26]은 W, x의 행렬 연산 결과값이다.
f(q)로 함수를 지정하면 f(q) = sum {q^2}이고, 이는 2q로 미분되므로 가중치는 *2를 해주어 [0.44, 0.52]이다.
mul gate에서는 서로가 서로의 계수이므로 값을 교환하는 방식으로 일반화 가능했었다. 따라서 [0.2, 0.4] * [0.44, 0.52]를 서로 행렬 계산해서 2*2 행렬로 만들어 준다. (W가 2*2 행렬임)
이후 결과값은 [[0.2*0.44, 0.4*0.44], [0.2*0.52, 0.4*0.52]]의 값이 나온다.
x에는 local gradient를 w값으로 하여 계산하고, Wx에서 x1에 들어가는 w값이 0.1, -0.3임을 고려해 transpose를 해준다.
이러면 최종 결과 값은 (01*0.44) + (-0.3*0.52) = -0.112, (0.5*0.44) + (0.8*0.52) = 0.636이다.

위와 같은 방식으로 backpropagation을 수행한다는 것을 배웠다.

Neural Network를 구성할 때 이제 단순한 Linear regression만 하는 것이 아니라 hidden layor를 추가해 모델을 구축한다.
hidden layor를 input node와 output node 사이에 100개를 추가한 것은 2-layor NN이라고 부른다. (헷갈릴 것 같다....)
이를 통해 NN과 같은 모델은 각각 하나의 클래스에 하나의 분류기로만 이용해서 그 특징에만 부합한 클래스를 부여했던 것이 hidden layor를 포함함으로써 한 개의 클래스에도 여러 개의 분류기가 생겨 다양한 특징을 잡아 분류할 수 있게 되었다는 발전이 있다!
이런 방식으로 2-layor, 3-layor NN을 만들 때, 함수는 max 함수를 이용하게 된다. <-- 여기서 함수는 활성화 함수를 말하는데 이전에는 sigmoid를 많이 사용했으나 이제는 relu를 더 많이 사용한다.
(이런 식의 NN 구조가 인체의 신경망이랑 비슷하지 않느냐 근데 헷갈리지는 말라는 이야기)

활성화 함수들 (Activation function)의 종류가 다양하다. sigmoid, tanh, ReLu, Leaky ReLu, Maxout, ELU 함수들의 그래프를 소개했다.
NN 모델의 구조를 간단하게 보여주는데 여러 개의 노드들이 있는 layor를 거쳐 값이 도출되는 과정을 설명한다. 이런 방식을 Fully Connected (FC)라고 한다.

4강은 Back propagation으로 W를 계산하는 방법을 배우고 NN 모델 맛보기를 했다.
다음 강의에서는 CNN의 구조를 살펴본다.