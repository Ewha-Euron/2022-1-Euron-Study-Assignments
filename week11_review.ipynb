{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week10_nlp_hw.ipynbì˜ ì‚¬ë³¸",
      "provenance": [],
      "collapsed_sections": [
        "-vPZn15zBHIv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“Œ week10 ê³¼ì œëŠ” **9ì£¼ì°¨ì˜ Machine Translation, Seq2Seq and Attention**ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ìœ„í‚¤ë…ìŠ¤ì˜ ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ êµì¬ ì‹¤ìŠµ, \bí…ì„œí”Œë¡œìš° ë° ì¼€ë¼ìŠ¤ ë“±ì˜ ê³µì‹ ë¬¸ì„œ ìë£Œë¡œ êµ¬ì„±ë˜ì–´ìˆëŠ” ê³¼ì œì…ë‹ˆë‹¤. \n",
        "\n",
        "ğŸ“Œ ì•ˆë‚´ëœ ë§í¬ì— ë§ì¶”ì–´ **ì§ì ‘ ì½”ë“œë¥¼ ë”°ë¼ ì¹˜ë©´ì„œ (í•„ì‚¬)** í•´ë‹¹ nlp task ì˜ ê¸°ë³¸ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë©”ì„œë“œë¥¼ ìˆ™ì§€í•´ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ğŸ˜Š í•„ìˆ˜ë¼ê³  ì²´í¬í•œ ë¶€ë¶„ì€ ê³¼ì œì— ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì£¼ì‹œê³ , ì„ íƒìœ¼ë¡œ ì²´í¬í•œ ë¶€ë¶„ì€ ììœ¨ì ìœ¼ë¡œ ìŠ¤í„°ë”” í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ê¶ê¸ˆí•œ ì‚¬í•­ì€ ê¹ƒí—ˆë¸Œ ì´ìŠˆë‚˜, ì¹´í†¡ë°©, ì„¸ì…˜ ë°œí‘œ ì‹œì‘ ì´ì „ ì‹œê°„ ë“±ì„ í™œìš©í•˜ì—¬ ììœ ë¡­ê²Œ ê³µìœ í•´ì£¼ì„¸ìš”!"
      ],
      "metadata": {
        "id": "QhUHfXkPAORh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk colab í™˜ê²½ì—ì„œ ì‹¤í–‰ì‹œ í•„ìš”í•œ ì½”ë“œì…ë‹ˆë‹¤. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "3XjTSbcxBB6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1ï¸âƒ£ **Seq2Seq**\n",
        "\n"
      ],
      "metadata": {
        "id": "-vPZn15zBHIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‘€ **ë‚´ìš© ë³µìŠµ Seq2Seq** \n",
        "\n",
        "* ì…ë ¥ëœ ì‹œí€€ìŠ¤ë¡œë¶€í„° ë‹¤ë¥¸ ë„ë©”ì¸ì˜ ì‹œí€€ìŠ¤ë¥¼ ì¶œë ¥í•˜ëŠ” ë¶„ì•¼ \n",
        "  * ì±—ë´‡ : (ì§ˆë¬¸) - (ëŒ€ë‹µ) \n",
        "  * ê¸°ê³„ë²ˆì—­ : (ì…ë ¥ë¬¸ì¥) - (ë²ˆì—­ë¬¸ì¥) \n",
        "* ì´ ì™¸ì—ë„ Text summarization, Speech to Text ë“±ì— ì“°ì´ëŠ” ëª¨ë¸ "
      ],
      "metadata": {
        "id": "cfTJoGzkEBlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "ğŸ”¹ **seq2seq**\n",
        "\n",
        "* [ê°œë…ë³µìŠµ](https://wikidocs.net/24996) \n",
        "\n",
        "ğŸ“Œ [word-Level NMT](https://wikidocs.net/86900) ğŸ‘‰ í•„ìˆ˜ \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ğŸ“Œ [BLEU](https://wikidocs.net/31695) ğŸ‘‰ ì„ íƒ(ê¶Œì¥) \n",
        "  * def function ìœ¼ë¡œ êµ¬í˜„ í•˜ëŠ” ë°©ë²•, nltk íŒ¨í‚¤ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ë°©ë²• \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jQLViVEQmKvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "import urllib3\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "Vg32oNQhmOpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "http = urllib3.PoolManager()\n",
        "url = 'http://www.manythings.org/anki/fra-eng.zip'\n",
        "filename = 'fra-eng.zip'\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, filename)\n",
        "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:\n",
        "    shutil.copyfileobj(r, out_file)\n",
        "\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path)"
      ],
      "metadata": {
        "id": "FGxBTM7aXHSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 33000"
      ],
      "metadata": {
        "id": "iJksiYc-Y6zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_ascii(s):\n",
        "  # í”„ë‘ìŠ¤ì–´ ì•…ì„¼íŠ¸(accent) ì‚­ì œ\n",
        "  # ì˜ˆì‹œ : 'dÃ©jÃ  dinÃ©' -> deja dine\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                   if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(sent):\n",
        "  # ì•…ì„¼íŠ¸ ì œê±° í•¨ìˆ˜ í˜¸ì¶œ\n",
        "  sent = to_ascii(sent.lower())\n",
        "\n",
        "  # ë‹¨ì–´ì™€ êµ¬ë‘ì  ì‚¬ì´ì— ê³µë°± ì¶”ê°€.\n",
        "  # ex) \"I am a student.\" => \"I am a student .\"\n",
        "  sent = re.sub(r\"([?.!,Â¿])\", r\" \\1\", sent)\n",
        "\n",
        "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") ì´ë“¤ì„ ì œì™¸í•˜ê³ ëŠ” ì „ë¶€ ê³µë°±ìœ¼ë¡œ ë³€í™˜.\n",
        "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "\n",
        "  # ë‹¤ìˆ˜ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜\n",
        "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "  return sent"
      ],
      "metadata": {
        "id": "yx5c7tSugFWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "pJIWoJooXIj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\n",
        "en_sent = u\"Have you had dinner?\"\n",
        "fr_sent = u\"Avez-vous dÃ©jÃ  dinÃ©?\"\n",
        "\n",
        "print('ì „ì²˜ë¦¬ ì „ ì˜ì–´ ë¬¸ì¥ :', en_sent)\n",
        "print('ì „ì²˜ë¦¬ í›„ ì˜ì–´ ë¬¸ì¥ :',preprocess_sentence(en_sent))\n",
        "print('ì „ì²˜ë¦¬ ì „ í”„ë‘ìŠ¤ì–´ ë¬¸ì¥ :', fr_sent)\n",
        "print('ì „ì²˜ë¦¬ í›„ í”„ë‘ìŠ¤ì–´ ë¬¸ì¥ :', preprocess_sentence(fr_sent))"
      ],
      "metadata": {
        "id": "K3eWNQ4sgHpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocessed_data():\n",
        "  encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "  with open(\"fra.txt\", \"r\") as lines:\n",
        "    for i, line in enumerate(lines):\n",
        "      # source ë°ì´í„°ì™€ target ë°ì´í„° ë¶„ë¦¬\n",
        "      src_line, tar_line, _ = line.strip().split('\\t')\n",
        "\n",
        "      # source ë°ì´í„° ì „ì²˜ë¦¬\n",
        "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "      # target ë°ì´í„° ì „ì²˜ë¦¬\n",
        "      tar_line = preprocess_sentence(tar_line)\n",
        "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
        "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
        "\n",
        "      encoder_input.append(src_line)\n",
        "      decoder_input.append(tar_line_in)\n",
        "      decoder_target.append(tar_line_out)\n",
        "\n",
        "      if i == num_samples - 1:\n",
        "        break\n",
        "\n",
        "  return encoder_input, decoder_input, decoder_target"
      ],
      "metadata": {
        "id": "u7jkKNp5gJjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
        "print('ì¸ì½”ë”ì˜ ì…ë ¥ :',sents_en_in[:5])\n",
        "print('ë””ì½”ë”ì˜ ì…ë ¥ :',sents_fra_in[:5])\n",
        "print('ë””ì½”ë”ì˜ ë ˆì´ë¸” :',sents_fra_out[:5])"
      ],
      "metadata": {
        "id": "BWHJQwszgMnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ì¸ì½”ë”ì˜ ì…ë ¥ : [['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.'], ['hi', '.']]\n",
        "ë””ì½”ë”ì˜ ì…ë ¥ : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!'], ['<sos>', 'salut', '.']]\n",
        "ë””ì½”ë”ì˜ ë ˆì´ë¸” : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>'], ['salut', '.', '<eos>']]"
      ],
      "metadata": {
        "id": "g4UZUdjKgOw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_en.fit_on_texts(sents_en_in)\n",
        "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
        "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
        "\n",
        "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
        "\n",
        "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
        "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
        "\n",
        "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
        "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
      ],
      "metadata": {
        "id": "558lsiuggQUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ì¸ì½”ë”ì˜ ì…ë ¥ì˜ í¬ê¸°(shape) :',encoder_input.shape)\n",
        "print('ë””ì½”ë”ì˜ ì…ë ¥ì˜ í¬ê¸°(shape) :',decoder_input.shape)\n",
        "print('ë””ì½”ë”ì˜ ë ˆì´ë¸”ì˜ í¬ê¸°(shape) :',decoder_target.shape)"
      ],
      "metadata": {
        "id": "13lXDo2rgRcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
        "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
        "print(\"ì˜ì–´ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° : {:d}, í”„ë‘ìŠ¤ì–´ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° : {:d}\".format(src_vocab_size, tar_vocab_size))"
      ],
      "metadata": {
        "id": "s1axDkc6gTG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_to_index = tokenizer_en.word_index\n",
        "index_to_src = tokenizer_en.index_word\n",
        "tar_to_index = tokenizer_fra.word_index\n",
        "index_to_tar = tokenizer_fra.index_word"
      ],
      "metadata": {
        "id": "c1C0EDumgUhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print('ëœë¤ ì‹œí€€ìŠ¤ :',indices)"
      ],
      "metadata": {
        "id": "VDSZ-kmCgVlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]"
      ],
      "metadata": {
        "id": "-Jmsz3bMgXXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input[30997]"
      ],
      "metadata": {
        "id": "QT7l6d6ggZKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array([  5,   7, 638,   1,   0,   0,   0,   0], dtype=int32)"
      ],
      "metadata": {
        "id": "vH96GLRkgbap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input[30997]"
      ],
      "metadata": {
        "id": "7McylDPBgc3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array([  2,  18,   5,  16, 173,   1,   0,   0,   0,   0,   0,   0,   0,\n",
        "         0,   0,   0], dtype=int32)"
      ],
      "metadata": {
        "id": "yNqiM56Agdq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target[30997]"
      ],
      "metadata": {
        "id": "XK3Gfe7-gemc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array([ 18,   5,  16, 173,   1,   3,   0,   0,   0,   0,   0,   0,   0,\n",
        "         0,   0,   0], dtype=int32)"
      ],
      "metadata": {
        "id": "IlIs7MG7gfkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_of_val = int(33000*0.1)\n",
        "print('ê²€ì¦ ë°ì´í„°ì˜ ê°œìˆ˜ :',n_of_val)"
      ],
      "metadata": {
        "id": "n53c8IhIggwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ],
      "metadata": {
        "id": "occu4FeOgiIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('í›ˆë ¨ source ë°ì´í„°ì˜ í¬ê¸° :',encoder_input_train.shape)\n",
        "print('í›ˆë ¨ target ë°ì´í„°ì˜ í¬ê¸° :',decoder_input_train.shape)\n",
        "print('í›ˆë ¨ target ë ˆì´ë¸”ì˜ í¬ê¸° :',decoder_target_train.shape)\n",
        "print('í…ŒìŠ¤íŠ¸ source ë°ì´í„°ì˜ í¬ê¸° :',encoder_input_test.shape)\n",
        "print('í…ŒìŠ¤íŠ¸ target ë°ì´í„°ì˜ í¬ê¸° :',decoder_input_test.shape)\n",
        "print('í…ŒìŠ¤íŠ¸ target ë ˆì´ë¸”ì˜ í¬ê¸° :',decoder_target_test.shape)"
      ],
      "metadata": {
        "id": "wKDJTrQggjt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "xZXAlbvuglHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì¸ì½”ë”\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # ì„ë² ë”© ì¸µ\n",
        "enc_masking = Masking(mask_value=0.0)(enc_emb) # íŒ¨ë”© 0ì€ ì—°ì‚°ì—ì„œ ì œì™¸\n",
        "encoder_lstm = LSTM(hidden_units, return_state=True) # ìƒíƒœê°’ ë¦¬í„´ì„ ìœ„í•´ return_stateëŠ” True\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # ì€ë‹‰ ìƒíƒœì™€ ì…€ ìƒíƒœë¥¼ ë¦¬í„´\n",
        "encoder_states = [state_h, state_c] # ì¸ì½”ë”ì˜ ì€ë‹‰ ìƒíƒœì™€ ì…€ ìƒíƒœë¥¼ ì €ì¥"
      ],
      "metadata": {
        "id": "RH3bmombgl05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë””ì½”ë”\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # ì„ë² ë”© ì¸µ\n",
        "dec_emb = dec_emb_layer(decoder_inputs) # íŒ¨ë”© 0ì€ ì—°ì‚°ì—ì„œ ì œì™¸\n",
        "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
        "\n",
        "# ìƒíƒœê°’ ë¦¬í„´ì„ ìœ„í•´ return_stateëŠ” True, ëª¨ë“  ì‹œì ì— ëŒ€í•´ì„œ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ return_sequencesëŠ” True\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True) \n",
        "\n",
        "# ì¸ì½”ë”ì˜ ì€ë‹‰ ìƒíƒœë¥¼ ì´ˆê¸° ì€ë‹‰ ìƒíƒœ(initial_state)ë¡œ ì‚¬ìš©\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
        "                                     initial_state=encoder_states)\n",
        "\n",
        "# ëª¨ë“  ì‹œì ì˜ ê²°ê³¼ì— ëŒ€í•´ì„œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ì¶œë ¥ì¸µì„ í†µí•´ ë‹¨ì–´ ì˜ˆì¸¡\n",
        "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# ëª¨ë¸ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì„ ì •ì˜.\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "y1k5Eisqgndd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size=128, epochs=50)"
      ],
      "metadata": {
        "id": "T7xVYk0ygpZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì¸ì½”ë”\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# ë””ì½”ë” ì„¤ê³„ ì‹œì‘\n",
        "# ì´ì „ ì‹œì ì˜ ìƒíƒœë¥¼ ë³´ê´€í•  í…ì„œ\n",
        "decoder_state_input_h = Input(shape=(hidden_units,))\n",
        "decoder_state_input_c = Input(shape=(hidden_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# í›ˆë ¨ ë•Œ ì‚¬ìš©í–ˆë˜ ì„ë² ë”© ì¸µì„ ì¬ì‚¬ìš©\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ìœ„í•´ ì´ì „ ì‹œì ì˜ ìƒíƒœë¥¼ í˜„ ì‹œì ì˜ ì´ˆê¸° ìƒíƒœë¡œ ì‚¬ìš©\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# ëª¨ë“  ì‹œì ì— ëŒ€í•´ì„œ ë‹¨ì–´ ì˜ˆì¸¡\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# ìˆ˜ì •ëœ ë””ì½”ë”\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "metadata": {
        "id": "Jy3Y9-kJgsSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "  # ì…ë ¥ìœ¼ë¡œë¶€í„° ì¸ì½”ë”ì˜ ë§ˆì§€ë§‰ ì‹œì ì˜ ìƒíƒœ(ì€ë‹‰ ìƒíƒœ, ì…€ ìƒíƒœ)ë¥¼ ì–»ìŒ\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # <SOS>ì— í•´ë‹¹í•˜ëŠ” ì •ìˆ˜ ìƒì„±\n",
        "  target_seq = np.zeros((1,1))\n",
        "  target_seq[0, 0] = tar_to_index['<sos>']\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "\n",
        "  # stop_conditionì´ Trueê°€ ë  ë•Œê¹Œì§€ ë£¨í”„ ë°˜ë³µ\n",
        "  # êµ¬í˜„ì˜ ê°„ì†Œí™”ë¥¼ ìœ„í•´ì„œ ì´ í•¨ìˆ˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ 1ë¡œ ê°€ì •í•©ë‹ˆë‹¤.\n",
        "  while not stop_condition:\n",
        "    # ì´ì  ì‹œì ì˜ ìƒíƒœ states_valueë¥¼ í˜„ ì‹œì ì˜ ì´ˆê¸° ìƒíƒœë¡œ ì‚¬ìš©\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‹¨ì–´ë¡œ ë³€í™˜\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "    # í˜„ì¬ ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡ ë¬¸ì¥ì— ì¶”ê°€\n",
        "    decoded_sentence += ' '+sampled_char\n",
        "\n",
        "    # <eos>ì— ë„ë‹¬í•˜ê±°ë‚˜ ì •í•´ì§„ ê¸¸ì´ë¥¼ ë„˜ìœ¼ë©´ ì¤‘ë‹¨.\n",
        "    if (sampled_char == '<eos>' or\n",
        "        len(decoded_sentence) > 50):\n",
        "        stop_condition = True\n",
        "\n",
        "    # í˜„ì¬ ì‹œì ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‹¤ìŒ ì‹œì ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì €ì¥\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    # í˜„ì¬ ì‹œì ì˜ ìƒíƒœë¥¼ ë‹¤ìŒ ì‹œì ì˜ ìƒíƒœë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì €ì¥\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "a34N9zpOgt1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì›ë¬¸ì˜ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "def seq_to_src(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0):\n",
        "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
        "  return sentence\n",
        "\n",
        "# ë²ˆì—­ë¬¸ì˜ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "def seq_to_tar(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
        "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "LU2HQrzLgvQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"ì…ë ¥ë¬¸ì¥ :\",seq_to_src(encoder_input_train[seq_index]))\n",
        "  print(\"ì •ë‹µë¬¸ì¥ :\",seq_to_tar(decoder_input_train[seq_index]))\n",
        "  print(\"ë²ˆì—­ë¬¸ì¥ :\",decoded_sentence[1:-5])\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "id": "fPRSjKEvgwTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"ì…ë ¥ë¬¸ì¥ :\",seq_to_src(encoder_input_test[seq_index]))\n",
        "  print(\"ì •ë‹µë¬¸ì¥ :\",seq_to_tar(decoder_input_test[seq_index]))\n",
        "  print(\"ë²ˆì—­ë¬¸ì¥ :\",decoded_sentence[1:-5])\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "id": "TLOEb6RVgyTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2ï¸âƒ£ Attention**"
      ],
      "metadata": {
        "id": "xUWWDwdiPLS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‘€ **ë‚´ìš©ë³µìŠµ attention** \n",
        "* RNNì— ê¸°ë°˜í•œ seq2seq ëª¨ë¸ì˜ ë‹¨ì ì„ ë³´ì™„í•œ ëª¨ë¸ \n",
        "  * ì •ë³´ì†ì‹¤, ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ í•´ê²° \n",
        "  * ì…ë ¥ë¬¸ì¥ì´ ê¸¸ë©´ ë²ˆì—­ í’ˆì§ˆì´ ë–¨ì–´ì§€ëŠ” í˜„ìƒ ë°©ì§€ \n",
        "\n",
        "*  ë””ì½”ë”ì—ì„œ ì¶œë ¥ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë§¤ ì‹œì (time step)ë§ˆë‹¤, ì¸ì½”ë”ì—ì„œì˜ ì „ì²´ ì…ë ¥ ë¬¸ì¥ ì¤‘, í•´ë‹¹ ì‹œì ì—ì„œ ì˜ˆì¸¡í•´ì•¼í•  ë‹¨ì–´ì™€ ì—°ê´€ì´ ìˆëŠ” ì…ë ¥ ë‹¨ì–´ ë¶€ë¶„ì„ ì¢€ ë” ì§‘ì¤‘(attention)í•´ì„œ ë³´ëŠ” ì›ë¦¬"
      ],
      "metadata": {
        "id": "9N0B4VknPkTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "ğŸ”¹ **2-(1)** ê¸°ë³¸\n",
        "\n",
        "* [ê°œë…ë³µìŠµ](https://wikidocs.net/22893) \n",
        "\n",
        "ğŸ“Œ [IMDB ë¦¬ë·° ê°ì„±ë¶„ë¥˜](https://wikidocs.net/48920) ğŸ‘‰ í•„ìˆ˜ \n",
        "\n",
        "\n",
        "\n",
        "ğŸ“Œ [TensorFlow ê³µì‹ë¬¸ì„œ - í…ìŠ¤íŠ¸ ë²ˆì—­](\n",
        "https://www.tensorflow.org/text/tutorials/nmt_with_attention) ğŸ‘‰ ì„ íƒ\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QWgla1BuPRqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "diaZweMyAxJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)"
      ],
      "metadata": {
        "id": "SuLBIlXTg75-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ë¦¬ë·°ì˜ ìµœëŒ€ ê¸¸ì´ : {}'.format(max(len(l) for l in X_train)))\n",
        "print('ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ : {}'.format(sum(map(len, X_train))/len(X_train)))"
      ],
      "metadata": {
        "id": "HASpFp4zg9KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 500\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "TdHkPCxcg-SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "cZ9KuRb3g_3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = Dense(units)\n",
        "    self.W2 = Dense(units)\n",
        "    self.V = Dense(1)\n",
        "\n",
        "  def call(self, values, query): # ë‹¨, keyì™€ valueëŠ” ê°™ìŒ\n",
        "    # query shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # score ê³„ì‚°ì„ ìœ„í•´ ë’¤ì—ì„œ í•  ë§ì…ˆì„ ìœ„í•´ì„œ ì°¨ì›ì„ ë³€ê²½í•´ì¤ë‹ˆë‹¤.\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "JtzmHMz1hBiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras import optimizers\n",
        "import os"
      ],
      "metadata": {
        "id": "KuDbxf4GhDYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
        "embedded_sequences = Embedding(vocab_size, 128, input_length=max_len, mask_zero = True)(sequence_input)"
      ],
      "metadata": {
        "id": "UgWDHu5hhFAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = Bidirectional(LSTM(64, dropout=0.5, return_sequences = True))(embedded_sequences)"
      ],
      "metadata": {
        "id": "R9BPT1SkhGNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional \\\n",
        "  (LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(lstm)"
      ],
      "metadata": {
        "id": "X44pSC94hHbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lstm.shape, forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)"
      ],
      "metadata": {
        "id": "Gh5tKbsFhIVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_h = Concatenate()([forward_h, backward_h]) # ì€ë‹‰ ìƒíƒœ\n",
        "state_c = Concatenate()([forward_c, backward_c]) # ì…€ ìƒíƒœ"
      ],
      "metadata": {
        "id": "00oUTiF_hKct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = BahdanauAttention(64) # ê°€ì¤‘ì¹˜ í¬ê¸° ì •ì˜\n",
        "context_vector, attention_weights = attention(lstm, state_h)"
      ],
      "metadata": {
        "id": "bUcPND16hLZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Dense(20, activation=\"relu\")(context_vector)\n",
        "dropout = Dropout(0.5)(dense1)\n",
        "output = Dense(1, activation=\"sigmoid\")(dropout)\n",
        "model = Model(inputs=sequence_input, outputs=output)"
      ],
      "metadata": {
        "id": "DEtyMHJIhMS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "U4PpLq7uhNMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs = 3, batch_size = 256, validation_data=(X_test, y_test), verbose=1)"
      ],
      "metadata": {
        "id": "4Efq5gd-hOgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n í…ŒìŠ¤íŠ¸ ì •í™•ë„: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "id": "pV9PGnNYhQXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ **2-(2)** Another task\n",
        "\n",
        "**ì•„ë˜ 3ê°€ì§€ ì˜ˆì œ ì¤‘ í•˜ë‚˜ë¥¼ ê³¨ë¼ í•„ì‚¬í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤**\n",
        "\n",
        "ğŸ’¨ Text summarization\n",
        "\n",
        "ğŸ“Œ [ì•„ë§ˆì¡´ ë¦¬ë·° ë°ì´í„°ë¡œ í…ìŠ¤íŠ¸ ìš”ì•½ êµ¬í˜„í•˜ê¸°](https://wikidocs.net/72820) \n",
        "\n",
        "ğŸ“Œ [ë‰´ìŠ¤ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ìš”ì•½](https://www.kaggle.com/code/sandeepbhogaraju/text-summarization-with-seq2seq-model/notebook)\n",
        "\n",
        "\n",
        "â•[KaKao Pororo](https://kakaobrain.github.io/pororo/index.html) ğŸ‘‰ ë°©í•™ í”„ë¡œì íŠ¸ ë•Œ í™œìš©í•´ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ :)\n",
        "  * Seq2Seq ì— í•´ë‹¹í•˜ëŠ” example ì°¸ê³ \n",
        "\n",
        "---\n",
        "\n",
        "ğŸ’¨ Chatbot\n",
        "\n",
        "\n",
        "ğŸ“Œ [í•œêµ­ì–´ ì±—ë´‡ êµ¬í˜„í•˜ê¸°](https://teddylee777.github.io/tensorflow/seq2seq-with-attention) \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TPX-WtSvPmm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "my3CWhEtXEj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ **2-(3)** ë…¼ë¬¸ì½ê¸° - cs224n 10ê°• ë§›ë³´ê¸° ğŸ‘‰ ì„ íƒ\n",
        "\n",
        "\n",
        "ğŸ“Œ [Bidirectional Attention Flow for Machine Comprehension](https://arxiv.org/abs/1611.01603v6)\n",
        "  * [ì •ë¦¬ ë¸”ë¡œê·¸](https://www.quantumdl.com/entry/10%EC%A3%BC%EC%B0%A82-Bidirectional-Attention-Flow-for-Machine-Comprehension-BiDAF)"
      ],
      "metadata": {
        "id": "bWqZjYiqPpyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UCFj71k-P_Jm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}