{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week10_김희숙_예습과제.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 인공지능 비트 트레이더 경진대회 시즌3"
      ],
      "metadata": {
        "id": "lFSEAyfvKMix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2위 ARIMA"
      ],
      "metadata": {
        "id": "A3hvAhI4KQPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ARIMA 에 VWAP 및 피쳐 엔지니어링을 통해 생성한 값 diff (open - VWAP)를 학습 데이터로 사용\n",
        "- 안전에 집중\n",
        "    - 수익을 많이 거두어도 나중에 잘못된 타이밍에 매도를 하게 되면, 복리에 하락이 겹쳐 큰 손해 발생\n"
      ],
      "metadata": {
        "id": "sU9oxaTHKkDk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEBQUN8iJwXa"
      },
      "outputs": [],
      "source": [
        "# ARIMA 모델을 사용하기 위해 statsmodels 제일 최신 버전을 설치\n",
        "!pip install statsmodels==0.12.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 기본\n",
        "# pandas와 numpy를 import \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# 2. 시각화\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# 3. 유틸\n",
        "# tqdm 패키지는 반복문에 대해 얼마나 진척되었는지를 가시적으로 확인\n",
        "# https://github.com/tqdm/tqdm 사용법 참고\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# 4. 경고 설정\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# 5. stats models\n",
        "# 시계열 모델을 위한 ARIMA\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "\n",
        "\n",
        "# 6. 구글 드라이브\n",
        "from google.colab import drive\n",
        "\n"
      ],
      "metadata": {
        "id": "0W2AgLGyK_P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 데이터를 다운 받지 못함\n",
        "### 대회 마감으로 참가 버튼이 활성화되지 않아 데이터를 다운 받지 못하였습니다."
      ],
      "metadata": {
        "id": "rCD5K7e3LNnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_x, train_y 를 sample_id 기준 하나로 합쳐서, train_z를 만듦\n",
        "# train x, y를 합치지 않아도 괜찮으나, 연속되는 변수 vwap, rsi, 등등을 만들기 위해 x, y를 합치고, 변수를 생성하고, x, y를 다시 분리하는 작업을 진행\n",
        "train_x[\"is_x\"] = 1\n",
        "train_y[\"is_x\"] = 0\n",
        "train_x_y = [train_x, train_y]\n",
        "train_list = [x.set_index('sample_id') for x in train_x_y]\n",
        "\n",
        "train_z = pd.concat(train_list, axis=0).rename_axis('sample_id').reset_index()"
      ],
      "metadata": {
        "id": "SdNzBF6ZLmAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VWAP**\n",
        "\n",
        "거래량 가중 평균가(VWAP, Volume Weighted Average Price)는 거래량에 가중치가 부여되어 특정 기간 동안의 평균 가격을 의미\n",
        "- 트레이딩에서 VWAP은 상승세와 하락세를 판단하는 지표로 사용\n",
        "- 가격(open)이 vwap보다 크면 상승세로 판단하고, 만약 가격이 VWAP 보다 작으면 하락세로 판단\n",
        "\n",
        "VWAP = sum(Volume*Price)/sum(Volume)\n",
        "\n",
        "* 참고 링크: https://academy.binance.com/ko/articles/volume-weighted-average-price-vwap-explained"
      ],
      "metadata": {
        "id": "_F9pZ-KvLz3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Pint**\n",
        "- 가격에서 VWAP을 뺀 값인 diff (open - VWAP)을 학습에 사용, 2가지 이점\n",
        "    * 안전한 지점을 찾을 수 있는 것 \n",
        "    * open - vwap이 양수이면, 상승 구간에 있기 때문에 안전한 지점을 찾을 수 있고, 적어도 손해는 안 볼 확률이 높아진다고 판단\n",
        "    * 제일 가격이 높은 지점을 찾는 것이 아닌 안전한 지점을 찾는 문제로 바꿈\n",
        "\n",
        "\n",
        "- 2개의 변수를 한 번에 사용할 수 있는 것\n",
        "    * 개인적으로 여러 강의와 코드 샘플들을 보았지만, 변수가 저렇게 많은데 단 1가지 feature만 학습에 사용\n",
        "    * 적어도 2개 이상의 변수를 함께 사용하면 조금 더 좋은 예측을 할것이라 생각"
      ],
      "metadata": {
        "id": "rExo2PrRMbVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vwap과, open에서 vwap을 뺀 값인 diff를 계산합니다.\n",
        "def make_vwap_and_diff(df):\n",
        "\n",
        "    # 1) VAWP 계산\n",
        "\n",
        "    # 일반적인 VWAP 공식에서 volume을 그대로 사용하지만, 여러번의 시도를 통해 tb_base_av 와 volume을 더했을 때 가장 좋은 volume이 나온다고 판단하영 사용하였습니다.\n",
        "    df[\"volume_tb_base_av\"] = df[\"tb_base_av\"] + df[\"volume\"]\n",
        "\n",
        "    # open하나만을 사용하기 보다는 open(시가), high(고가), low(저가) 3개의 평균을 price로 사용하였습니다.\n",
        "    df['volume_price'] = ((df['open'] + df['high'] + df['low']) / 3) * df['volume_tb_base_av']\n",
        "\n",
        "    # price와 volume의 곱의 합을 구해줍니다.\n",
        "    df['volume_price_sum'] = df.groupby(['sample_id'])['volume_price'].apply(lambda x: x.cumsum())\n",
        "\n",
        "    # volume의 합을 구해줍니다.\n",
        "    df['volume_sum'] = df.groupby(['sample_id'])['volume_tb_base_av'].apply(lambda x: x.cumsum())\n",
        "\n",
        "    # 2 변수의 나눗셈을 통해 vwap을 계산해줍니다.\n",
        "    df['vwap'] = df['volume_price_sum'] / df['volume_sum']\n",
        "\n",
        "\n",
        "    # 2) diff 계산\n",
        "    # 매도수익이 open을 통해 이루어진다고 알려져있기 때문에 open에서 vwap을 뺀 값을 diff로 사용했습니다.\n",
        "    df[\"diff\"] = df[\"open\"] - df[\"vwap\"]\n",
        "\n",
        "\n",
        "    return df\n",
        "    "
      ],
      "metadata": {
        "id": "p3A5NqCtLwnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 프레임에서 sample_id 에 따른 open을 반환하는 함수입니다.\n",
        "def get_open(df,sample_id):\n",
        "    \n",
        "    return df[df[\"sample_id\"] == sample_id]['open'].values\n",
        "    "
      ],
      "metadata": {
        "id": "hzCO8n5iM8Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 프레임에서 sample_id 에 따른 VWAP을 반환하는 함수입니다.\n",
        "def get_vwap(df,sample_id):\n",
        "    \n",
        "    return df[df[\"sample_id\"] == sample_id]['vwap'].values\n",
        "    "
      ],
      "metadata": {
        "id": "uyLB94pXM-Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 프레임에서 sample_id 에 따른 diff 반환하는 함수입니다.\n",
        "def get_diff(df,sample_id):\n",
        "    \n",
        "    return df[df[\"sample_id\"] == sample_id]['diff'].values\n",
        "    "
      ],
      "metadata": {
        "id": "8KZid2aYNA_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 프레임에서 sample_id 에 따른 rsi 반환하는 함수입니다.\n",
        "def get_rsi(df,sample_id):\n",
        "    \n",
        "    return df[df[\"sample_id\"] == sample_id]['rsi'].values\n",
        "    "
      ],
      "metadata": {
        "id": "w2mIopjpNGp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 프레임에서 sample_id 에 따른 col_name 값을 반환하는 함수입니다.\n",
        "def get_series(df,sample_id, col_name):\n",
        "    \n",
        "    return df[df[\"sample_id\"] == sample_id][col_name].values\n",
        "    "
      ],
      "metadata": {
        "id": "82Va1B6DNGll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WaW1zGrYNGgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RSI**\n",
        "- 상대강도지수(relative strength index)는 가격의 상승압력과 하락압력 간의 상대적인 강도를 나타냄\n",
        "    - 트레이딩에서 사용\n",
        "    - RSI 값이 30보다 작으면 초과매도로 판단하고, RSI 값이, 70 보다 크면 초과매수 상태로 판단\n",
        "    - 최고점과, 최하점을 찾기 쉽다는 장점\n",
        "RSI = AU/(AU+AD)\n",
        "\n",
        "\n",
        "\n",
        "- 천정과 바닥이 제대로 형성되지 않은 시장(RSI가 50근처 유지)에서는 유용하지 못함\n",
        "- 코인 데이터의 RSI 그래프를 그려보면 RSI 0 ~ 100 사이를 급변동\n",
        "\n",
        "* 안정적으로 투자하기 위해 65 초과인 상태를 초과매수국면으로 판단하고, 해당 시점 이후 50분동안은 투자하지 않도록 설정"
      ],
      "metadata": {
        "id": "WAvp6ai3NNUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RSI를 만들어줍니다.\n",
        "# 상승분, 하락분의 평균은 일반적으로 14일을 기준으로 생성합니다.\n",
        "def make_rsi(df, period=14):\n",
        "\n",
        "    # 전일 대비 상승분을 계산해줍니다. - 상승분이 0보다 크면 상승분을 넣고, 0보다 작거나 같으면 0을 넣어줍니다.\n",
        "    df[\"U\"] = np.where(df.groupby([\"sample_id\"])[\"open\"].diff(1) > 0, df.groupby([\"sample_id\"])[\"open\"].diff(1), 0)\n",
        "\n",
        "    # 전일 대비 하락분을 계산해줍니다. - 하락분이 0보다 작으면 하락분 * -1을 넣고, 0보다 크거나 같으면 0을 넣어줍니다.\n",
        "    df[\"D\"] = np.where(df.groupby([\"sample_id\"])[\"open\"].diff(1) < 0, df.groupby([\"sample_id\"])[\"open\"].diff(1) *(-1), 0)\n",
        "\n",
        "\n",
        "    # 전일 대비 상승분의 평균을 계산해줍니다.\n",
        "    ud_df = pd.DataFrame()\n",
        "    ud_df[\"sample_id\"] = df[\"sample_id\"]\n",
        "    ud_df[\"U\"] = df[\"U\"]\n",
        "    ud_df[\"D\"] = df[\"D\"]\n",
        "\n",
        "    # 상승분의 14일 평균을 구해줍니다.\n",
        "    df[\"AU\"] = ud_df.groupby([\"sample_id\"])[\"U\"].rolling( window=period, min_periods=period ).mean().reset_index()[\"U\"]\n",
        "    # 하락분의 14일 평균을 구해줍니다.\n",
        "    df[\"AD\"] = ud_df.groupby([\"sample_id\"])[\"D\"].rolling( window=period, min_periods=period ).mean().reset_index()[\"D\"]\n",
        "\n",
        "\n",
        "    # AU / (AU + AD) 의 백분율을 RSI 로 계산해줍니다.\n",
        "    RSI = df[\"AU\"] / (df[\"AU\"] + df[\"AD\"]) * 100\n",
        "    \n",
        "    df[\"rsi\"] = RSI\n",
        "    \n",
        "    return df\n",
        "    "
      ],
      "metadata": {
        "id": "HXl8ze96Nvvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 전처리 ###\n",
        "# 1. train, test의 sample_id 목록을 저장합니다.\n",
        "TRAIN_SAMPLE_ID_LIST = train_x[\"sample_id\"].unique().tolist()\n",
        "TEST_SAMPLE_ID_LIST = test_x[\"sample_id\"].unique().tolist()\n",
        "\n",
        "# 2. VWAP, diff 를 만들어줍니다.\n",
        "test_x = make_vwap_and_diff(test_x)\n",
        "train_z = make_vwap_and_diff(train_z)\n",
        "\n",
        "# 3. rsi 를 만들어줍니다.\n",
        "test_x = make_rsi(test_x, 14)\n",
        "train_z = make_rsi(train_z, 14)\n",
        "\n",
        "# 4. train x와 y를 분리합니다.\n",
        "train_x = train_z[train_z[\"is_x\"] == 1]\n",
        "train_y = train_z[train_z[\"is_x\"] == 0]\n",
        "\n",
        "split_drop_cols = [\"is_x\"]\n",
        "\n",
        "train_x = train_x.drop(columns=split_drop_cols, axis=1)\n",
        "train_y = train_y.drop(columns=split_drop_cols, axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "qiYdbIn9NwWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**모델학습**\n",
        "\n",
        "1) 모델\n",
        "- 모델은 diff (open - vwap)을 ARIMA를 통해 학습하여 생성\n",
        "\n",
        "\n",
        "2) 제약조건\n",
        "- vwap, rsi의 마지막 값을 제약조건의 기준으로 사용\n",
        "- vwap이 1보다 크면 open보다 vwap이 크다는 의미로 하향세에 접어들었다고 판단 투자하지 않음 (x의 open 의 마지막 값은 1입니다.)\n",
        "\n",
        "\n",
        "*rsi의 값이 65보다 크면 초과 매수 상태라고 판단하여 투자하지 않았습니다. 기본적으로 70 초과로 판단하는데 임의로 바꿀수 있는 값으로 조금 낮춰서 65로 시도했을때가 제일 좋아서 사용했습니다.*\n",
        "\n",
        "\n",
        "3) ARIMA와 p d q\n",
        "- 개인적으로 ARIMA 모델의 order(p, d, q)에 투자를 많이 함\n",
        "- ARIMA 모델의 AIC (아카이케 정보 기준) 가 가장 나오는 order를 brute force로 찾아보았지만, 2시간 30분 ~ 3시간이 걸림에도 불구하고, 점수가 오히려 낮아져서, 여러번의 시도로 4, 0, 1 이 제일 좋다고 판단\n",
        "\n",
        "pac, acf 분석을 통해 AR 2 모델이라고 판단했습니다. 분석에는 다음 영상을 참고했습니다. (https://www.youtube.com/watch?v=-vSzKfqcTDg&t=360s)\n",
        "*이탤릭체 텍스트*\n"
      ],
      "metadata": {
        "id": "6-S63f80NvWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "\n",
        "for sample_id in tqdm(TEST_SAMPLE_ID_LIST):\n",
        "\n",
        "    # 1. 데이터 로드\n",
        "    # 1) diff - 학습에 사용\n",
        "    diff_x = get_diff(test_x, sample_id)\n",
        "\n",
        "    # 2) vwap - 보조 지표로 사용\n",
        "    vwap_series = get_vwap(test_x, sample_id)\n",
        "\n",
        "    # 3) rsi - 보조 지표로 사용\n",
        "    rsi_series = get_rsi(test_x, sample_id)\n",
        "\n",
        "\n",
        "\n",
        "    # 2. ARIMA\n",
        "    # 1) 모델 정의\n",
        "    ARIMA_MODEL = {}\n",
        "    ARIMA_MODEL_FIT = {}\n",
        "\n",
        "    # 2) AR 모델 적용\n",
        "    try:\n",
        "      ARIMA_MODEL = ARIMA(diff_x, order = (4,0,1))\n",
        "      ARIMA_MODEL_FIT = ARIMA_MODEL.fit(trend = 'nc', full_output = True, disp = True)\n",
        "\n",
        "    # 3) 수렴하지 않을 경우 p d q 를 1, 1, 0으로 사용\n",
        "    except:\n",
        "      ARIMA_MODEL = ARIMA(diff_x, order = (1,1,0))\n",
        "      ARIMA_MODEL_FIT = ARIMA_MODEL.fit(trend = 'nc', full_output = True, disp = True)\n",
        "\n",
        "    # 4) ARIMA 예측\n",
        "    ARIMA_FORECAST  = ARIMA_MODEL_FIT.predict(1,120, typ='levels')\n",
        "\n",
        "\n",
        "\n",
        "    # 3. 데이처 처리\n",
        "    # 1) 최대 부분인 인덱스를 찾는데 해당 시점에 매도를 진행합니다.\n",
        "    sell_time = np.argmax(ARIMA_FORECAST)\n",
        "\n",
        "    # 2) 최대값을 찾습니다.\n",
        "    max_val = np.max(ARIMA_FORECAST)\n",
        "    \n",
        "    # 3) vwap의 마지막 값을 가져옵니다.\n",
        "    vwap_last_val = vwap_series[1379]\n",
        "\n",
        "    rsi_last_val = rsi_series[1379]\n",
        "\n",
        "\n",
        "\n",
        "    # 4. 투자 전략\n",
        "    buy_quantity = 0\n",
        "\n",
        "    # 1) 최대값이 0 보다 크면 가격이 vwap 보다 크다는 의미로, 투자합니다.\n",
        "    if  max_val > 0:\n",
        "        buy_quantity = 1\n",
        "\n",
        "\n",
        "    # 2) 만약 vwap 마지막 값이, 1보다 크면 가격이 1보다 작다는 의미로 하향세이기 때문에 투자하지 않습니다.\n",
        "    if vwap_last_val > 1 and sell_time < 50:\n",
        "        buy_quantity = 0\n",
        "\n",
        "    # 3) 만약 rsi의 값이 65 보다 크면, 초과매수 상태로 판단하여 투자하지 않습니다.\n",
        "    if rsi_last_val > 65 and sell_time < 50:\n",
        "        buy_quantity = 0\n",
        "\n",
        "\n",
        "\n",
        "    # 5. 결과\n",
        "    result_list = [\n",
        "                   sample_id,\n",
        "                   buy_quantity,\n",
        "                   sell_time\n",
        "                  ]\n",
        "\n",
        "    result.append(result_list)\n",
        "    "
      ],
      "metadata": {
        "id": "oxD7HoxXOQ-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 제출 및 확인 ###\n",
        "# 1. 학습 결과를 데이터 프레임으로 만듭니다.\n",
        "\n",
        "submit_columns = [\n",
        "                  \"sample_id\", \n",
        "                  \"buy_quantity\", \n",
        "                  \"sell_time\"\n",
        "                  ]\n",
        "\n",
        "\n",
        "submit = pd.DataFrame(data=result, columns=submit_columns)\n",
        "\n",
        "# 2. 결과 데이터 프레임 확인\n",
        "\n",
        "submit.head(10)\n",
        "\n",
        "# 3. 투자 개수 확인\n",
        "\n",
        "submit[submit[\"buy_quantity\"] == 1].shape[0]\n",
        "\n",
        "# 4. sell_time 50미만에서 구매하는 개수 확인\n",
        "cond1 = (submit[\"buy_quantity\"] == 1)\n",
        "cond2 = (submit[\"sell_time\"] < 50)\n",
        "\n",
        "submit[cond1 & cond2].shape[0]\n",
        "\n",
        "# 5. 제출\n",
        "# 파일의 이름을 지정해줍니다.\n",
        "FILE_NAME = \"/0603_ARIMA_DIFF_VWAP_RSI_65_UNDER_50_SUBMIT.csv\"\n",
        "\n",
        "# 제출경로에 파일을 생성해줍니다.\n",
        "RESULT_PATH = SUBMIT_PATH + FILE_NAME\n",
        "\n",
        "submit.to_csv(RESULT_PATH, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "m2zzPOKbOQ53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**public score를 다음과 같이 활용**\n",
        "\n",
        "- open에 vwap을 섞어 투자개수가 줄었음에도 점수가 올라가는 현상\n",
        "- vwap, rsi등 보조지표를 통해 sell_time 10, 20.. 50 미만은 투자하지 않기로 결정했음에도 점점 점수가 올라가는 현상\n",
        "- rsi 초과매수로 상태를 70 초과가 아닌, 65 초과로, 더 안정적으로 결정해도 점수가 상승하는 현상\n",
        "\n",
        "이렇게, 제약사항을 통해 보수적으로 투자했음에도 불구하고, 점수가 올라가는 경우에 집중하여, 해당 피쳐가 안정적으로 작용함을 판단"
      ],
      "metadata": {
        "id": "drm7u3AhOqCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "여러 아이디어\n",
        "\n",
        "- RANDOM BOX 분류 모델\n",
        "- open에 vwap을 섞은것에 더하여, RSI, 이동평균선등을 모두 섞어 만든 혼합 데이터\n",
        "- prophet, neural prophet, LSTM등 다양한 모델을 사용\n",
        "- 위 모델들의 결과를 bagging하는 방법을 사용해보았지만, ARIMA 단 하나만을 사용했을때보다 예측 성능이 낮아짐"
      ],
      "metadata": {
        "id": "IEGvFUWsOzry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "L_2u2IodOQ1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3위 Fbprophet"
      ],
      "metadata": {
        "id": "bZOabkyrPLrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- fbprophet에 grid search를 사용해 hyperparameter optimization함\n",
        "- 하나의 seasonality를 사용하는 것보다 여러 개를 사용하는 것이 validation 과정에서 MAPE에 좋은 영향을 주었기 때문에 seasonality를 추가하며 파라미터를 조정\n",
        "- 돈을 잃지 않는 것을 목적으로 threshold를 높게 잡았고(샘플 중 약 3%를 매수), 100%의 매수 방식을 유지\n",
        "- 전처리는 이미 scaling이 되어 있기 때문에 따로 진행을 하지 않음\n",
        "- rolling MA를 추가적으로 사용해서 보다 안정적인 예측을 해보는 것도 좋을 것 같음"
      ],
      "metadata": {
        "id": "d1T3x3DYPRz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!pip install -q fbprophet==0.7.1"
      ],
      "metadata": {
        "id": "EUM6qyCxP9YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Library Import ###\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import random\n",
        "from dateutil import parser\n",
        "from tqdm import tqdm\n",
        "from fbprophet import Prophet\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ],
      "metadata": {
        "id": "1a8_Tiq0OQyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = '2021-01-31 00:00:00'\n",
        "start_dt = datetime.datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')"
      ],
      "metadata": {
        "id": "lWQ-EhtaQCjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '.'\n",
        "train_x_df = pd.read_csv(data_path  + \"/train_x_df.csv\")\n",
        "train_y_df = pd.read_csv(data_path  + \"/train_y_df.csv\")\n",
        "test_x_df = pd.read_csv(data_path  + \"/test_x_df.csv\")"
      ],
      "metadata": {
        "id": "OLODZpIDQCbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df2d_to_array3d(df_2d):\n",
        "    # 입력 받은 2차원 데이터 프레임을 3차원 numpy array로 변경하는 함수\n",
        "    feature_size = df_2d.iloc[:,2:].shape[1]\n",
        "    time_size = len(df_2d.time.value_counts())\n",
        "    sample_size = len(df_2d.sample_id.value_counts())\n",
        "    sample_index = df_2d.sample_id.value_counts().index\n",
        "    array_3d = df_2d.iloc[:,2:].values.reshape([sample_size, time_size, feature_size])\n",
        "    return array_3d"
      ],
      "metadata": {
        "id": "u7KFIRINQCWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_array = df2d_to_array3d(train_x_df)\n",
        "train_y_array = df2d_to_array3d(train_y_df)\n",
        "test_x_array = df2d_to_array3d(test_x_df)"
      ],
      "metadata": {
        "id": "gwhA8xXZQCRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'''\n",
        "train_x_array {train_x_array.shape}\n",
        "train_y_array {train_y_array.shape}\n",
        "test_x_array {test_x_array.shape}\n",
        "''')"
      ],
      "metadata": {
        "id": "XRwo05zmQOR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### EDA ###\n",
        "def plot_series(x_series, y_series):\n",
        "    #입력 series와 출력 series를 연속적으로 연결하여 시각적으로 보여주는 코드 입니다.\n",
        "    plt.plot(x_series, label = 'input_series')\n",
        "    plt.plot(np.arange(len(x_series), len(x_series)+len(y_series)),\n",
        "             y_series, label = 'output_series')\n",
        "    plt.axhline(1, c = 'red')\n",
        "    plt.legend()\n",
        "\n",
        "idx = 42\n",
        "plot_series(train_x_array[idx,:,1], train_y_array[idx,:,1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "avbELMc8QOMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Parameter Optimization ###\n",
        "# fbprophet 모델에 MAPE를 기준으로 grid searching을 적용\n",
        "params_grid = {'changepoint_prior_scale':[0.1,0.01,0.001],\n",
        "               'n_changepoints' : [50,100,150],\n",
        "               'fourier_order' : [5,10],\n",
        "               'period':[0.1,0.3,0.5]\n",
        "               }\n",
        "grid = ParameterGrid(params_grid)\n",
        "cnt = 0\n",
        "for p in grid:\n",
        "    cnt = cnt+1\n",
        "\n",
        "print('Total Possible Models',cnt)"
      ],
      "metadata": {
        "id": "Y0GA_UU9QdN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
      ],
      "metadata": {
        "id": "4jwPeWzoQdJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAPE가 최소가 되는 값을 찾고, 추가적으로 2개의 seasonality를 더해서 모델 성능 높임\n",
        "\n",
        "model_parameters = pd.DataFrame(columns = ['idx',\n",
        "                                           'MAPE',\n",
        "                                           'changepoint_prior_scale',\n",
        "                                           'n_changepoints',\n",
        "                                           'fourier_order',\n",
        "                                           'period'\n",
        "                                           ])\n",
        "\n",
        "random.seed(42)\n",
        "samples = random.sample(range(7661),10)\n",
        "for idx in tqdm(samples, position=1):\n",
        "    x_series = train_x_array[idx,:,1]\n",
        "    y_series = train_y_array[idx,:,1]\n",
        "    x_df = pd.DataFrame()\n",
        "    x_df['ds'] = [start_dt + datetime.timedelta(minutes = time_min) for time_min in np.arange(1, x_series.shape[0]+1).tolist()]\n",
        "    x_df['y'] = x_series.tolist()\n",
        "    for p in tqdm(grid, position=0):\n",
        "        prophet = Prophet(seasonality_mode='multiplicative', \n",
        "                          yearly_seasonality=False,\n",
        "                          weekly_seasonality=False,\n",
        "                          daily_seasonality=False,\n",
        "                          changepoint_prior_scale=p['changepoint_prior_scale'],\n",
        "                          n_changepoints=p['n_changepoints'],\n",
        "                          )\n",
        "        prophet.add_seasonality(name='seasonality_1',period=p['period'],fourier_order=p['fourier_order'])\n",
        "        prophet.fit(x_df)\n",
        "\n",
        "        # 120분 테스트 데이터를 예측합니다.\n",
        "        future_data = prophet.make_future_dataframe(periods=120, freq='min')\n",
        "        forecast_data = prophet.predict(future_data)\n",
        "\n",
        "        pred_y = forecast_data.yhat.values[-120:]\n",
        "\n",
        "        MAPE = mean_absolute_percentage_error(y_series,pred_y)\n",
        "        model_parameters = model_parameters.append({'idx':idx,\n",
        "                                                    'MAPE':MAPE,\n",
        "                                                    'changepoint_prior_scale':p['changepoint_prior_scale'],\n",
        "                                                    'n_changepoints':p['n_changepoints'],\n",
        "                                                    'fourier_order':p['fourier_order'],\n",
        "                                                    'period':p['period']\n",
        "                                                    },ignore_index=True)"
      ],
      "metadata": {
        "id": "wZ9R3S21QdEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 42\n",
        "x_series = train_x_array[idx,:,1]\n",
        "  \n",
        "x_df = pd.DataFrame()\n",
        "x_df['ds'] = [start_dt + datetime.timedelta(minutes = time_min) for time_min in np.arange(1, x_series.shape[0]+1).tolist()]\n",
        "x_df['y'] = x_series.tolist()\n",
        "\n",
        "prophet = Prophet(seasonality_mode='multiplicative', \n",
        "          yearly_seasonality=False,\n",
        "          weekly_seasonality=False,\n",
        "          daily_seasonality=False,\n",
        "          changepoint_prior_scale=0.1,\n",
        "          n_changepoints=50\n",
        "          )\n",
        "prophet.add_seasonality(name='seasonality_1',period=0.1,fourier_order = 5)\n",
        "prophet.add_seasonality(name='seasonality_2', period=0.3,fourier_order = 5)\n",
        "prophet.add_seasonality(name='seasonality_3', period=0.01, fourier_order = 5)\n",
        "prophet.fit(x_df)\n",
        "\n",
        "future_data = prophet.make_future_dataframe(periods=120, freq='min')\n",
        "forecast_data = prophet.predict(future_data)\n",
        "\n",
        "pred_y = forecast_data.yhat.values[-120:]\n",
        "pred_y_lower = forecast_data.yhat_lower.values[-120:]\n",
        "pred_y_upper = forecast_data.yhat_upper.values[-120:]"
      ],
      "metadata": {
        "id": "Sf-4vSyLQdAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.hstack([train_x_array[idx,:,1], train_y_array[idx,:,1]]), label='true')\n",
        "plt.plot(np.hstack([train_x_array[idx,:,1], pred_y]), label='prediction')\n",
        "plt.legend()\n",
        "plt.axvline(1380, c='red')"
      ],
      "metadata": {
        "id": "ockZwxZYRKbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Training ###\n",
        "# 베이스라인 코드에 있었던 100%의 매수 방식을 그대로 유지. threshold는 상위 3% 정도를 매수하도록 조정\n",
        "test_pred_array = np.zeros([test_x_array.shape[0],\n",
        "                           120])\n",
        "for idx in tqdm(range(test_x_array.shape[0]), position=0):\n",
        "    \n",
        "    x_series = test_x_array[idx,:,1]\n",
        "  \n",
        "    x_df = pd.DataFrame()\n",
        "    x_df['ds'] = [start_dt + datetime.timedelta(minutes = time_min) for time_min in np.arange(1, x_series.shape[0]+1).tolist()]\n",
        "    x_df['y'] = x_series.tolist()\n",
        "\n",
        "    prophet = Prophet(seasonality_mode='multiplicative', \n",
        "              yearly_seasonality=False,\n",
        "              weekly_seasonality=False,\n",
        "              daily_seasonality=False,\n",
        "              changepoint_prior_scale=0.1,\n",
        "              n_changepoints=50\n",
        "              )\n",
        "    prophet.add_seasonality(name='seasonality_1',period=0.1,fourier_order = 5)\n",
        "    prophet.add_seasonality(name='seasonality_2', period=0.3,fourier_order = 5)\n",
        "    prophet.add_seasonality(name='seasonality_3', period=0.01, fourier_order = 5)\n",
        "    prophet.fit(x_df)\n",
        "\n",
        "    # 120분 테스트 데이터를 예측합니다.\n",
        "    future_data = prophet.make_future_dataframe(periods=120, freq='min')\n",
        "    forecast_data = prophet.predict(future_data)\n",
        "\n",
        "    pred_y = forecast_data.yhat.values[-120:]\n",
        "\n",
        "    test_pred_array[idx,:] = pred_y"
      ],
      "metadata": {
        "id": "Q1Te6SbMROra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def array_to_submission(x_array, pred_array):\n",
        "    # 입력 x_arrry와 출력 pred_arry를 통해서 \n",
        "    # buy_quantitiy와 sell_time을 결정\n",
        "    submission = pd.DataFrame(np.zeros([pred_array.shape[0],2], np.int64),\n",
        "                columns = ['buy_quantity', 'sell_time'])\n",
        "    submission = submission.reset_index()\n",
        "    submission.loc[:, 'buy_quantity'] = 0.1\n",
        "    \n",
        "    buy_price = []\n",
        "    for idx, sell_time in enumerate(np.argmax(pred_array, axis = 1)):\n",
        "        buy_price.append(pred_array[idx, sell_time])\n",
        "    buy_price = np.array(buy_price)\n",
        "    \n",
        "    submission.loc[:, 'buy_quantity'] = (buy_price > 1.07) * 1\n",
        "    submission['sell_time'] = np.argmax(pred_array, axis = 1)\n",
        "    submission.columns = ['sample_id','buy_quantity', 'sell_time']\n",
        "    submission['sample_id'] = submission['sample_id'].apply(lambda x: x+7661)\n",
        "    return submission"
      ],
      "metadata": {
        "id": "nIs09QSuROjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Submit ###\n",
        "submission = array_to_submission(test_x_array, test_pred_array)\n",
        "submission['buy_quantity'].value_counts()\n",
        "\n",
        "submission.to_csv('/content/gdrive/My Drive/data/bit_trader_season2.csv', index=False)"
      ],
      "metadata": {
        "id": "KYGNxxz7ROeM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}