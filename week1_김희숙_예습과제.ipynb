{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week1_김희숙_예습과제.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ego-Vision 손동작 인식 경진대회 필사"
      ],
      "metadata": {
        "id": "RBtKo-eaePXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "원본링크: https://dacon.io/competitions/official/235805/codeshare/3620"
      ],
      "metadata": {
        "id": "KCsgDNdFDX2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "대회에서 제공 중인 데이터(21G)를 다운받을 공간이 없어서 코드 실행은 하지 못했습니다."
      ],
      "metadata": {
        "id": "0Yz00NtZGDxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "V8KwHivLD7X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgcpJwO2dCcx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def list_files(startpath):\n",
        "    for root, dirs, files in os.walk(startpath):\n",
        "        level = root.replace(startpath, '').count(os.sep)\n",
        "        indent = ' ' * 4 * (level)\n",
        "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "        subindent = ' ' * 4 * (level + 1)\n",
        "        for f in files:\n",
        "            print('{}{}'.format(subindent, f))\n",
        "list_files('../')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import library"
      ],
      "metadata": {
        "id": "RVOUoY-vEOEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import logging\n",
        "import easydict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from natsort import natsorted\n",
        "from os.path import join as opj\n",
        "from ptflops import get_model_complexity_info\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
        "from PIL import Image\n",
        "\n",
        "# 딥러닝\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, grad_scaler\n",
        "from torchvision import transforms\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "ZDAxcVgSeS56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 및 학습 하이퍼파라미터 정의"
      ],
      "metadata": {
        "id": "c4loLAfTEThI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "노트북 작성자의 다른 대회 코드(https://dacon.io/competitions/official/235842/codeshare/3683)\n",
        "하이퍼파라미터 정의를 비슷하게 하심\n"
      ],
      "metadata": {
        "id": "DwisOIEDHZIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = easydict.EasyDict(\n",
        "    {'exp_num':'0',\n",
        "     'experiment':'Base',\n",
        "     'tag':'Default',\n",
        "\n",
        "     # Path settings\n",
        "     'data_path':'../data',\n",
        "     'fold':4,\n",
        "     'Kfold':5,\n",
        "     'model_path':'results/',\n",
        "\n",
        "     # Model parameter settings\n",
        "     'encoder_name':'regnety_040',\n",
        "     'drop_path_rate':0.2,\n",
        "     \n",
        "     # Training parameter settings\n",
        "     ## Base Parameter\n",
        "     'img_size':288,\n",
        "     'batch_size':16,\n",
        "     'epochs':60,\n",
        "     'optimizer':'Lamb',\n",
        "     'initial_lr':5e-6,\n",
        "     'weight_decay':1e-3,\n",
        "\n",
        "     ## Augmentation\n",
        "     'aug_ver':2,\n",
        "     'flipaug_ratio':0.3, # 0.1~0.5로 다양하게 시도\n",
        "     'margin':50,\n",
        "     'random_margin':True,\n",
        "\n",
        "     ## Scheduler\n",
        "     'scheduler':'cycle',\n",
        "     'warm_epoch':5,\n",
        "     ### Cosine Annealing\n",
        "     'min_lr':5e-6,\n",
        "     'tmax':145,\n",
        "     ### OnecycleLR\n",
        "     'max_lr':1e-3,\n",
        "\n",
        "     ## etc.\n",
        "     'patience':50,\n",
        "     'clipping':None,\n",
        "\n",
        "     # Hardware settings\n",
        "     'amp':True,\n",
        "     'multi_gpu':False,\n",
        "     'logging':False,\n",
        "     'num_workers':4,\n",
        "     'seed':42\n",
        "    })"
      ],
      "metadata": {
        "id": "dd0eAfhXeVIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dataset"
      ],
      "metadata": {
        "id": "Mu_sqTYRFOlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keypoint를 기준으로 이미지를 crop하기 위한 함수 정의\n",
        "# train과 test시 해당 함수가 적용된 crop이미지가 inputs으로 들어가게 됩니다.\n",
        "def crop_image(imges, point, margin=100):\n",
        "    image = np.array(Image.open(imges).convert('RGB')) # 이미지 불러옴\n",
        "    point = point['data'] # 크롭 point 지정\n",
        "    max_point = np.max(np.array(point), axis=0).astype(int) + margin\n",
        "    min_point = np.min(np.array(point), axis=0).astype(int) - margin\n",
        "    max_point = max_point[:-1] # remove Z order\n",
        "    min_point = min_point[:-1] # remove Z order\n",
        "\n",
        "    max_x, max_y = max_point\n",
        "    min_x, min_y = min_point\n",
        "    max_y += margin  # 손목까지\n",
        "    \n",
        "    # 데이터 포인트의 크기가 원 이미지를 넘어서는 경우를 방지\n",
        "    max_x = max_x if max_x < 1920 else 1920\n",
        "    max_y = max_y if max_y < 1080 else 1080\n",
        "    min_x = min_x if min_x > 0 else 0\n",
        "    min_y = min_y if min_y > 0 else 0\n",
        "    \n",
        "    crop_image = image[min_y:max_y, min_x:max_x]\n",
        "\n",
        "    return crop_image"
      ],
      "metadata": {
        "id": "IalulOeVeZSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader에서 사용할 dataframe 만들기\n",
        "train_path = '../data/train'\n",
        "train_folders = natsorted(glob(train_path + '/*'))\n",
        "\n",
        "answers = []\n",
        "for train_folder in train_folders:\n",
        "    json_path = glob(train_folder + '/*.json')[0]\n",
        "    js = json.load(open(json_path))\n",
        "    cat = js.get('action')[0]\n",
        "    cat_name = js.get('action')[1]\n",
        "    \n",
        "    images_list = glob(train_folder + '/*.png')\n",
        "    for image_name in images_list:\n",
        "        answers.append([image_name, cat, cat_name])\n",
        "\n",
        "answers = pd.DataFrame(answers, columns = ['train_path','answer', 'answer_name'])\n",
        "answers.to_csv('../data/df_train.csv', index=False)"
      ],
      "metadata": {
        "id": "KZw9NT5xH5sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스가 1개뿐인 폴더들 Augmentation해서 이미지 생성후 dataframe재정의\n",
        "# 새롭게 정의한 dataframe을 학습에 이용시 약간의 성능향상을 확인할 수 있었음.\n",
        "seed = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "data_path = '../data'\n",
        "df_train = pd.read_csv(opj(data_path, 'df_train.csv'))\n",
        "df_info = pd.read_csv(opj(data_path, 'hand_gesture_pose.csv'))\n",
        "df_train = df_train.merge(df_info[['pose_id', 'gesture_type', 'hand_type']],\n",
        "                        how='left', left_on='answer', right_on='pose_id')\n",
        "\n",
        "save_folder = 'train' \n",
        "for i in range(649, 649+5):\n",
        "    if not os.path.exists(opj(data_path, save_folder, str(i))):\n",
        "        os.makedirs(opj(data_path, save_folder, str(i)))"
      ],
      "metadata": {
        "id": "wNzT3UJBH89A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flip aug가능한 label : 131, 47 (one sample)\n",
        "oslabel_fliplabel = [(131,156), (47, 22)] # one sample label, flip label\n",
        "folders = ['649', '650'] # Train 648번 folder에 이은 number 생성\n",
        "for label, folder in tqdm(zip(oslabel_fliplabel, folders)):\n",
        "    idx = 0\n",
        "    os_label, f_label  = label[0], label[1]\n",
        "    one_sample = df_train[df_train['answer'] == os_label].reset_index(drop=True)\n",
        "    temp = df_train[df_train['answer'] == f_label].reset_index(drop=True)\n",
        "    train_folders = natsorted(temp['train_path'].apply(lambda x : x[:-6]).unique())\n",
        "    for train_folder in (train_folders):\n",
        "        json_path = glob(train_folder + '/*.json')[0]\n",
        "        js = json.load(open(json_path))\n",
        "        keypoints = js['annotations']\n",
        "        images_list = natsorted(glob(train_folder + '/*.png'))\n",
        "        for _, (point, image_name) in enumerate(zip(keypoints, images_list)):\n",
        "            croped_image = crop_image(image_name, point, margin=50)\n",
        "            flip_img = cv2.flip(croped_image, 1)\n",
        "            save_path = opj(data_path, save_folder, folder, f'{idx}.png')\n",
        "            idx += 1\n",
        "            cv2.imwrite(save_path, flip_img)\n",
        "            df_train.loc[len(df_train)] = [save_path] + one_sample.iloc[0][1:].values.tolist()"
      ],
      "metadata": {
        "id": "GbBWKtYZIFb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 회전 함수\n",
        "def rotation(img, angle):\n",
        "    angle = int(random.uniform(-angle, angle))\n",
        "    h, w = img.shape[:2]\n",
        "    M = cv2.getRotationMatrix2D((int(w/2), int(h/2)), angle, 1)\n",
        "    img = cv2.warpAffine(img, M, (w, h)) \n",
        "    return img"
      ],
      "metadata": {
        "id": "hiw9fKjdIK-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oslabel = [92, 188, 145]\n",
        "folder = ['651', '652', '653']\n",
        "for label, folder in tqdm(zip(oslabel, folder)):\n",
        "    idx = 0\n",
        "    one_sample = df_train[df_train['answer'] == label].reset_index(drop=True)\n",
        "    train_folders = natsorted(temp['train_path'].apply(lambda x : x[:-6]).unique())\n",
        "    for train_folder in (train_folders):\n",
        "        json_path = glob(train_folder + '/*.json')[0]\n",
        "        js = json.load(open(json_path))\n",
        "        keypoints = js['annotations']\n",
        "        images_list = natsorted(glob(train_folder + '/*.png'))\n",
        "        for _, (point, image_name) in enumerate(zip(keypoints, images_list)):\n",
        "            croped_image = crop_image(image_name, point, margin=50)\n",
        "            aug_img = rotation(croped_image, 30)\n",
        "            save_path = opj(data_path, save_folder, folder, f'{idx}.png')\n",
        "            idx += 1\n",
        "            cv2.imwrite(save_path, aug_img)\n",
        "            df_train.loc[len(df_train)] = [save_path] + one_sample.iloc[0][1:].values.tolist()\n",
        "\n",
        "df_train.to_csv('../data/df_train_add.csv', index=False) #트레인 데이터 생성"
      ],
      "metadata": {
        "id": "B_yAOD0ZIPvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train dataset에 475, 543 폴더는 의도하지 않은 나머지 손에 대해서도 Keypoint가 잡히게 됨.\n",
        "# Json의 Keypoint를 사용하기위해 475,543폴더인 경우 해당 부분 Keypoint 제거\n",
        "def remove_keypoints(folder_num, points):\n",
        "    lst = []\n",
        "    for x,y,z in points:\n",
        "        cond1 = x<250 and y>800\n",
        "        cond2 = x>1400 and y<400\n",
        "        if not (cond1 or cond2):\n",
        "           lst.append([x,y,z]) \n",
        "    # print('Finished removing {} wrong keypoints....'.format(folder_num))\n",
        "    return lst\n",
        "\n",
        "class Train_Dataset(Dataset):\n",
        "    def __init__(self, df, transform=None, df_flip_info=None, flipaug_ratio=0, label_encoder=None, margin=50, random_margin=True):\n",
        "        self.id = df['train_path'].values\n",
        "        self.target = df['answer'].values\n",
        "        self.transform = transform\n",
        "        self.margin = margin\n",
        "        self.random_margin = random_margin\n",
        "\n",
        "        # Flip Augmentation (Change target class)\n",
        "        if df_flip_info is not None:\n",
        "            self.use_flip = True\n",
        "            print('Use Flip Augmentation')\n",
        "            left = label_encoder.transform(df_flip_info['left'])\n",
        "            right = label_encoder.transform(df_flip_info['right'])\n",
        "            left_to_right = dict(zip(left, right))\n",
        "            right_to_left = dict(zip(right, left))\n",
        "            \n",
        "            self.flip_info = left_to_right.copy()\n",
        "            self.flip_info.update(right_to_left)        \n",
        "            self.flip_possible_class = list(set(np.concatenate([left, right])))\n",
        "        self.flipaug_ratio = flipaug_ratio\n",
        "\n",
        "        print(f'Dataset size:{len(self.id)}')\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = np.array(Image.open(self.id[idx]).convert('RGB'))\n",
        "        target = self.target[idx]\n",
        "\n",
        "        # Load Json File\n",
        "        try:\n",
        "            image_num = int(Path(self.id[idx]).stem)\n",
        "            dir = os.path.dirname(self.id[idx])\n",
        "            folder_num = os.path.basename(dir)\n",
        "            json_path = opj(dir, folder_num+'.json')\n",
        "            js = json.load(open(json_path))\n",
        "            keypoints = js['annotations'][image_num]['data']  # 해당 이미지에 해당하는 Keypoints\n",
        "        except:  # Augmentation으로 직접 새로 만든 Folder는 Json이 없으므로 바로 Return (미리 손 부분이 Crop된 상태로 저장하였음.)\n",
        "            image = self.transform(Image.fromarray(image))\n",
        "            return image, np.array(target)\n",
        "\n",
        "        if folder_num in ['475', '543']:\n",
        "            keypoints = remove_keypoints(folder_num, keypoints)\n",
        "\n",
        "        # Image Crop using keypoints\n",
        "        max_point = np.max(np.array(keypoints), axis=0).astype(int) + self.margin\n",
        "        min_point = np.min(np.array(keypoints), axis=0).astype(int) - self.margin\n",
        "        max_point = max_point[:-1] # remove Z order\n",
        "        min_point = min_point[:-1] # remove Z order\n",
        "\n",
        "        max_x, max_y = max_point\n",
        "        min_x, min_y = min_point\n",
        "        max_y += 100  # 손목부분까지 여유를 주기위해\n",
        "\n",
        "        # 매 에폭마다 Margin이 조금씩 다르게 들어가므로 한 폴더 내 비슷한 이미지들의 Overfitting을 방지하는 효과를 주기위해 (Only Train Phase)\n",
        "        if self.random_margin:  \n",
        "            if random.random() < 0.5:\n",
        "                max_x += self.margin\n",
        "            if random.random() < 0.5:\n",
        "                max_y += self.margin\n",
        "            if random.random() < 0.5:\n",
        "                min_x -= self.margin\n",
        "            if random.random() < 0.5:\n",
        "                min_y -= self.margin\n",
        "        else:\n",
        "            max_x += self.margin\n",
        "            max_y += self.margin\n",
        "            min_x -= self.margin\n",
        "            min_y -= self.margin\n",
        "\n",
        "        # 데이터 포인트의 크기가 원 이미지를 넘어서는 경우를 방지\n",
        "        max_x = max_x if max_x < 1920 else 1920\n",
        "        max_y = max_y if max_y < 1080 else 1080\n",
        "        min_x = min_x if min_x > 0 else 0\n",
        "        min_y = min_y if min_y > 0 else 0\n",
        "        \n",
        "        image = image[min_y:max_y, min_x:max_x]\n",
        "\n",
        "        # FlipAug\n",
        "        if (random.random() < self.flipaug_ratio) and (target in self.flip_possible_class):\n",
        "            image = np.flip(image, axis=1)  # (H, W, C)에서 width 축 flip\n",
        "            target = self.flip_info[target]\n",
        "\n",
        "        image = self.transform(Image.fromarray(image))\n",
        "        return image, np.array(target)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.id)"
      ],
      "metadata": {
        "id": "75UlDtSVeZ5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(df, batch_size, shuffle, num_workers, transform, df_flip_info=None, \n",
        "                flipaug_ratio=0, label_encoder=None, margin=50, random_margin=True):\n",
        "    dataset = Train_Dataset(df, transform, df_flip_info=df_flip_info, flipaug_ratio=flipaug_ratio, \n",
        "                            label_encoder=label_encoder, margin=margin, random_margin=random_margin)\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True,\n",
        "                                drop_last=False)\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "Z2R19BygM3EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_augmentation(img_size, ver):\n",
        "    if ver==1:\n",
        "        # For Test\n",
        "        transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225]),\n",
        "                ])\n",
        "\n",
        "\n",
        "    if ver==2:\n",
        "        # For Train\n",
        "        transform = transforms.Compose([\n",
        "                transforms.RandomAffine(20),\n",
        "                transforms.RandomPerspective(),\n",
        "                transforms.ToTensor(),\n",
        "\t            transforms.Resize((img_size, img_size)),\n",
        "    \t        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "\n",
        "\n",
        "    return transform"
      ],
      "metadata": {
        "id": "PAnPltMxecrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 동일한 포즈라도 왼손과 오른손의 class가 다름 -> Horizontal Flip 사용 불가\n",
        "- 동일한 포즈일 때\n",
        "    - 이미지: Horizontal Flip\n",
        "    - class 변경(ex. My View, 왼손, 그리고 숫자1일 때 HFlip을 수행하여 My View, 오른손, 숫자 1로 만듦)"
      ],
      "metadata": {
        "id": "34UVxFJlNZvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network"
      ],
      "metadata": {
        "id": "UDuEVApZN8sY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pytorch image models(timm) 라이브러리 활용\n",
        "- Generalization Performance에 강점을 가지는 RegNet을 Base 모델로 사용"
      ],
      "metadata": {
        "id": "84PC6X75N-v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pose_Network(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.encoder = timm.create_model(args.encoder_name, pretrained=True,\n",
        "                                    drop_path_rate=args.drop_path_rate)\n",
        "        num_head = self.encoder.head.fc.in_features\n",
        "        self.encoder.head.fc = nn.Linear(num_head, 157)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)"
      ],
      "metadata": {
        "id": "M81SCkaVeco1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils for training and Logging"
      ],
      "metadata": {
        "id": "6JhdUwslQ_lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "실험 기록 log파일로 남기기"
      ],
      "metadata": {
        "id": "vVd7tr7VRBUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Warmup Learning rate scheduler\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "class WarmUpLR(_LRScheduler):\n",
        "    \"\"\"warmup_training learning rate scheduler\n",
        "    Args:\n",
        "        optimizer: optimzier(e.g. SGD)\n",
        "        total_iters: totoal_iters of warmup phase\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
        "        \n",
        "        self.total_iters = total_iters\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        \"\"\"we will use the first m batches, and set the learning\n",
        "        rate to base_lr * m / total_iters\n",
        "        \"\"\"\n",
        "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
        "\n",
        "# Logging\n",
        "def get_root_logger(logger_name='basicsr',\n",
        "                    log_level=logging.INFO,\n",
        "                    log_file=None):\n",
        "\n",
        "    logger = logging.getLogger(logger_name)\n",
        "    # if the logger has been initialized, just return it\n",
        "    if logger.hasHandlers():\n",
        "        return logger\n",
        "\n",
        "    format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
        "    logging.basicConfig(format=format_str, level=log_level)\n",
        "\n",
        "    if log_file is not None:\n",
        "        file_handler = logging.FileHandler(log_file, 'w')\n",
        "        file_handler.setFormatter(logging.Formatter(format_str))\n",
        "        file_handler.setLevel(log_level)\n",
        "        logger.addHandler(file_handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "class AvgMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.losses = []\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        self.losses.append(val)\n"
      ],
      "metadata": {
        "id": "K1EzY1uXeclz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer"
      ],
      "metadata": {
        "id": "XqLxL8aURWKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 각 폴더별로 Group을 부여하고 해당 Group 전체가 Train or Valid로 들어가게 하는 GroupKFold를 사용\n",
        "- Class Label 분포 고려 -> Stratified KFold 적용\n",
        "\n",
        "문제점\n",
        "- 전체 데이터 중 특정 클래스가 1,2개 폴더만 존재하는 클래스 있음\n",
        "- GroupKFold로 Split시 특정 클래스는 Train에만, 또는 Valid에만 들어가는 현상 확인\n",
        "- 이는 특정 클래스에 대해 검증을 수행할 수 없음\n",
        "\n",
        "해결방안\n",
        "- 폴더가 1개인 클래스를 폴더가 2개 이상이 되도록 만듦\n",
        "- 단순히 Augmentation을 사용한게 아니라 Flip Augmentation(+Rotation) 수행"
      ],
      "metadata": {
        "id": "452pZHDKSYjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습과 검증을 위한 class\n",
        "class Trainer():\n",
        "    def __init__(self, args, save_path):\n",
        "        '''\n",
        "        args: arguments\n",
        "        save_path: Model 가중치 저장 경로\n",
        "        '''\n",
        "        super(Trainer, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Logging\n",
        "        log_file = os.path.join(save_path, 'log.log')\n",
        "        self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n",
        "        self.logger.info(args)\n",
        "        self.logger.info(args.tag)\n",
        "\n",
        "        # Train, Valid Set load\n",
        "        ############################################################################\n",
        "        # df_train = pd.read_csv(opj(args.data_path, 'df_train.csv'))\n",
        "        df_train = pd.read_csv(opj(args.data_path, 'df_train_add.csv'))\n",
        "        df_info = pd.read_csv(opj(args.data_path, 'hand_gesture_pose.csv'))\n",
        "\n",
        "        df_train = df_train.merge(df_info[['pose_id', 'gesture_type', 'hand_type']], \\\n",
        "                                how='left', left_on='answer', right_on='pose_id')\n",
        "\n",
        "        # 폴더별(Group)로 각 번호 부여\n",
        "        df_train['groups'] = df_train['train_path'].apply(lambda x:x.split('/')[3])\n",
        "        df_train.loc[:,:] = natsorted(df_train.values)\n",
        "        # 노이즈 이미지 제거: 596번은 주먹쥐기 이미지인데 갑자기 손바닥을 펴는 노이즈 이미지가 5장있음 + 0번 폴더에 9번 이미지 역시 잘못된 클래스\n",
        "        drop_idx = df_train[df_train['groups'].isin(['596'])].index.tolist()[3:8] + [9]  \n",
        "        df_train = df_train.drop(drop_idx).reset_index(drop=True)  \n",
        "        le = LabelEncoder()\n",
        "        df_train['answer'] = le.fit_transform(df_train['answer'])\n",
        "        \n",
        "        # Split Fold\n",
        "        # kf = StratifiedGroupKFold(n_splits=args.Kfold) # K-Fold\n",
        "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
        "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_train, y=df_train['answer'])):\n",
        "            df_train.loc[val_idx, 'fold'] = fold\n",
        "        df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n",
        "        df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n",
        "        \n",
        "        # Augmentation\n",
        "        self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
        "        self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
        "        \n",
        "        ######################################################################\n",
        "        # Flip Augmentation을 위한 Mapping dataframe\n",
        "        df_info = pd.read_csv('../data/hand_gesture_pose.csv')\n",
        "        df_info = df_info[df_info['hand_type'] != 'both']\n",
        "        # drop idx, 동일한 약속, gesture_type, hand_type인데 다른 클래스인 경우 존재 -> 약속 1과 2로 이름을 나누어줌.\n",
        "        df_info.loc[[105, 128], 'pose_name'] = '약속 1'  # idx: (105, 128)\n",
        "        df_info.loc[[101, 124], 'pose_name'] = '약속 2'  # idx: (101, 124)\n",
        "\n",
        "        # drop 41 idx, 동일한 약속, my hand, right class가 49와 54로 두 개있어 Mapping df만들 때 문제가 발생하여 미리 49번 클래스 처리\n",
        "        df_info = df_info.drop(41)\n",
        "\n",
        "        # Make a mapping dataframe\n",
        "        df_info = df_info.groupby(['pose_name', 'view_type', 'gesture_type', 'hand_type']).sum().unstack().reset_index().dropna(axis=0)\n",
        "        df_info['left'] = df_info.pose_id.left.apply(int)\n",
        "        df_info['right'] = df_info.pose_id.right.apply(int)\n",
        "        df_flip_info = df_info.drop('pose_id', axis=1).droplevel('hand_type', axis=1).reset_index(drop=True)\n",
        "        print('Mapping dataframe Length', df_flip_info.shape)\n",
        "        ######################################################################\n",
        "        \n",
        "        # TrainLoader\n",
        "        self.train_loader = get_loader(df_train, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, transform=self.train_transform, \n",
        "                                       df_flip_info=df_flip_info, flipaug_ratio=args.flipaug_ratio, label_encoder=le, margin=args.margin, random_margin=args.random_margin)\n",
        "        self.val_loader = get_loader(df_val, batch_size=args.batch_size, shuffle=False,\n",
        "                                       num_workers=args.num_workers, transform=self.test_transform)\n",
        "\n",
        "        # Network\n",
        "        self.model = Pose_Network(args).to(self.device)\n",
        "        macs, params = get_model_complexity_info(self.model, (3, args.img_size, args.img_size), as_strings=True,\n",
        "                                                 print_per_layer_stat=False, verbose=False)\n",
        "        self.logger.info('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
        "        self.logger.info('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
        "\n",
        "        # Loss\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "        # Optimizer & Scheduler\n",
        "        self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr, weight_decay=args.weight_decay)\n",
        "        \n",
        "        iter_per_epoch = len(self.train_loader)\n",
        "        self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n",
        "\n",
        "        if args.scheduler == 'cos':\n",
        "            tmax = args.tmax # half-cycle \n",
        "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max = tmax, eta_min=args.min_lr, verbose=True)\n",
        "        elif args.scheduler == 'cycle':\n",
        "            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
        "\n",
        "        \n",
        "        if args.multi_gpu:\n",
        "            self.model = nn.DataParallel(self.model).to(self.device)\n",
        "\n",
        "        # Train / Validate\n",
        "        best_loss = np.inf\n",
        "        best_acc = 0\n",
        "        best_epoch = 0\n",
        "        early_stopping = 0\n",
        "        start = time.time()\n",
        "        for epoch in range(1, args.epochs+1):\n",
        "            self.epoch = epoch\n",
        "\n",
        "            if args.scheduler == 'cos':\n",
        "                if epoch > args.warm_epoch:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "            # Training\n",
        "            train_loss, train_acc = self.training(args)\n",
        "\n",
        "            # Model weight in Multi_GPU or Single GPU\n",
        "            state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_acc = self.validate()\n",
        "\n",
        "            # Save models\n",
        "            if val_loss < best_loss:\n",
        "                early_stopping = 0\n",
        "                best_epoch = epoch\n",
        "                best_loss = val_loss\n",
        "                best_acc = val_acc\n",
        "\n",
        "                torch.save({'epoch':epoch,\n",
        "                            'state_dict':state_dict,\n",
        "                            'optimizer': self.optimizer.state_dict(),\n",
        "                            'scheduler': self.scheduler.state_dict(),\n",
        "                    }, os.path.join(save_path, 'best_model.pth'))\n",
        "                self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n",
        "            else:\n",
        "                early_stopping += 1\n",
        "\n",
        "            # Early Stopping\n",
        "            if early_stopping == args.patience:\n",
        "                break\n",
        "\n",
        "        self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f}')\n",
        "        end = time.time()\n",
        "        self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n",
        "\n",
        "\n",
        "    # Training\n",
        "    def training(self, args):\n",
        "        self.model.train()\n",
        "        train_loss = AvgMeter()\n",
        "        train_acc = 0\n",
        "\n",
        "        scaler = grad_scaler.GradScaler()\n",
        "        for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
        "            images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
        "            targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
        "            \n",
        "            if self.epoch <= args.warm_epoch:\n",
        "                self.warmup_scheduler.step()\n",
        "\n",
        "            self.model.zero_grad(set_to_none=True)\n",
        "            if args.amp:\n",
        "                with autocast():\n",
        "                    preds = self.model(images)\n",
        "                    loss = self.criterion(preds, targets)\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Gradient Clipping\n",
        "                if args.clipping is not None:\n",
        "                    scaler.unscale_(self.optimizer)\n",
        "                    nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
        "\n",
        "                scaler.step(self.optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "            else:\n",
        "                preds = self.model(images)\n",
        "                loss = self.criterion(preds, targets)\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
        "                self.optimizer.step()\n",
        "\n",
        "            if args.scheduler == 'cycle':\n",
        "                if self.epoch > args.warm_epoch:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "            # Metric\n",
        "            train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
        "            # log\n",
        "            train_loss.update(loss.item(), n=images.size(0))\n",
        "            \n",
        "        train_acc /= len(self.train_loader.dataset)\n",
        "\n",
        "        self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
        "        self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f}')\n",
        "        return train_loss.avg, train_acc\n",
        "            \n",
        "    # Validation or Dev\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = AvgMeter()\n",
        "            val_acc = 0\n",
        "\n",
        "            for _, (images, targets) in enumerate(self.val_loader):\n",
        "                images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
        "                targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
        "\n",
        "                preds = self.model(images)\n",
        "                loss = self.criterion(preds, targets)\n",
        "\n",
        "                # Metric\n",
        "                val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
        "                # log\n",
        "                val_loss.update(loss.item(), n=images.size(0))\n",
        "            val_acc /= len(self.val_loader.dataset)\n",
        "\n",
        "            self.logger.info(f'Valid Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f}')\n",
        "        return val_loss.avg, val_acc\n"
      ],
      "metadata": {
        "id": "B8-oT_4jecg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 와...너무 어려워요..발표 듣고 다시 뜯어봐야겠습니다"
      ],
      "metadata": {
        "id": "hycZWbCnR9ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation prediction의 분포 확인"
      ],
      "metadata": {
        "id": "cgv7vhYBTUjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델이 어떤 클래스를 못맞췄는지 확인"
      ],
      "metadata": {
        "id": "W67N1eTaTYa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Case1\n",
        "# img = Image.open('./etc/숫자1_검지흔들기.png')\n",
        "# plt.figure(figsize=(10,5))\n",
        "# plt.imshow(img)"
      ],
      "metadata": {
        "id": "K1oPfPxgemEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 나머지 클래스 예측에 대한 logloss 값은 대부분 0.1보다 작은 값을 보여주지만 숫자1과 검지 흔들기에 대해서는 매우 큰 logloss를 보여줌(5.98, 4.69)"
      ],
      "metadata": {
        "id": "DhzjWahnTicL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Case2\n",
        "# img = Image.open('./etc/주먹내밀기_주먹쥐기.png')\n",
        "# plt.figure(figsize=(10,5))\n",
        "# plt.imshow(img)"
      ],
      "metadata": {
        "id": "OC5Ko8uIemBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- logloss는 1.27, 2.19"
      ],
      "metadata": {
        "id": "AheZ_G6CTvQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 시각화를 위한 Function\n",
        "def visualize(folder_num):\n",
        "    path = f'../data/train/{folder_num}/*.png'\n",
        "    image_list = glob(path)\n",
        "    length = len(image_list)\n",
        "    \n",
        "    fig, ax = plt.subplots(1, length, figsize=(50,10))\n",
        "    for i, image in enumerate(image_list):\n",
        "        image = Image.open(image).convert('RGB')\n",
        "        ax[i].imshow(image)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "_orZH-Q5el_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(opj(args.data_path, 'df_train.csv'))\n",
        "df['groups'] = df['train_path'].apply(lambda x:x.split('/')[3])\n",
        "df = df.drop_duplicates('groups')"
      ],
      "metadata": {
        "id": "4ren_mhRel87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Case 1) 숫자1과 부정(검지 흔들기) 대해 큰 Logloss값이 발생"
      ],
      "metadata": {
        "id": "V8tbW4LoT8ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = f'../data/train/'\n",
        "number1_folder = df[df['answer_name'] == '숫자1']['groups'].tolist()\n",
        "shake_folder = df[df['answer_name'] == '부정(검지 흔들기)']['groups'].tolist()\n",
        "\n",
        "image1 = Image.open(opj(path, number1_folder[0], '1.png'))   # 352번 폴더\n",
        "image2 = Image.open(opj(path, shake_folder[11], '1.png'))    # 489번 폴더\n",
        "print(number1_folder[0], shake_folder[11])\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
        "ax[0].imshow(image1)\n",
        "ax[1].imshow(image2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g4oeIauLel6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 사람이 봐도 구분하기 어려움"
      ],
      "metadata": {
        "id": "OVDLvrwWUFfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 숫자1 & 부정(검지 흔들기) 폴더 시각화\n",
        "visualize(352), visualize(489)"
      ],
      "metadata": {
        "id": "p_zPib51UMSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  두 폴더의 이미지 각각을 놓고보면 클래스를 구별하기 어렵지만, 폴더 내 이미지 변화(손가락 움직임)를 통해 클래스를 구별할 수 있음"
      ],
      "metadata": {
        "id": "OPfjqqrbVVOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Case 2) 주먹쥐기와 경고(주먹 내밀기) 대해서도 큰 Logloss값이 발생"
      ],
      "metadata": {
        "id": "cdUSij3xVYgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = f'../data/train/'\n",
        "number1_folder = df[df['answer_name'] == '주먹쥐기']['groups'].tolist()\n",
        "shake_folder = df[df['answer_name'] == '경고(주먹 내밀기)']['groups'].tolist()\n",
        "\n",
        "image1 = Image.open(opj(path, number1_folder[1], '1.png'))   # 505번 폴더\n",
        "image2 = Image.open(opj(path, shake_folder[9], '1.png'))    # 577번 폴더\n",
        "print(number1_folder[1], shake_folder[9])\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
        "ax[0].imshow(image1)\n",
        "ax[1].imshow(image2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A6v4hAKXel3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 주먹쥐기 & 경고(주먹 내밀기) 폴더 시각화\n",
        "visualize(505), visualize(577)"
      ],
      "metadata": {
        "id": "DbtkUrFCVhqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 2가지 Case를 해결하기 위한 방법\n",
        "- Keypoint를 사용한 Rule에 기반한 접근 방법\n",
        "    - 본 대회는 Train과 Test모두 입력으로 이미지뿐만 아니라 Keypoint를 사용할 수 있음\n",
        "    - 따라서, 기존에 학습했던 것처럼 모델의 학습과 추론을 수행\n",
        "    - 다만, 추론 시에 (숫자1, 검지 흔들기), (주먹쥐기, 주먹 내밀기) 두 Case 중에 하나의 Class를 예측하면 Keypoint를 사용한 Rule로 처리\n",
        "    - 예를 들어, 모델이 숫자1(왼손, My View) 또는 검지 흔들기(왼손, My View)로 예측하였으면, Keypoint에 기반한 알고리즘으로 넘어가게되고 해당 알고리즘을 거친 예측 정답이 나오게 됨\n",
        "    - 추가적으로, 해당 Task는 같은 숫자1이라도 왼손인지 오른손인지, 또는 My View인지 Your View에 따라 Class가 다름\n",
        "    - 모델이 왼손인지 오른손인지, 또는 My View인지 Your View인지는 맞췄을거라 판단, (숫자1(왼손, My View) - 검지 흔들기(왼손, My View))과 같이 좌우, View타입은 동일하게 매핑시킴"
      ],
      "metadata": {
        "id": "_-d9cqKVViFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-Processing in detail (Method)"
      ],
      "metadata": {
        "id": "3s4iQwy2WP_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case1**\n",
        "- 숫자1과 부정(검지 흔들기)의 경우 한 폴더 내에서 숫자1은 검지의 움직임이 크지 않지만 검지 흔들기는 검지를 흔들어야하기 때문에 검지 Keypoint의 X좌표로의 움직임이 상대적으로 큼\n",
        "- 따라서, 손가락(검지)의 가장 위에 있는(0,0픽셀을 기준으로 Y값이 가장 작은) keypoint의 움직임을 통해 구별\n",
        "- 예를들어, 폴더내 이미지가 10개가 있다면 10개의 검지 Keypoint 좌표가 있게 되고 해당 x좌표들에 대해서 max-min을 계산\n",
        "- 즉, 각 폴더내에서 1개의 x좌표 변화량이 계산 됨\n",
        "- 두 클래스에 대해 계산된 검지 x좌표 변화량들에 대해서 적절한 threshold값을 설정해 클래스를 구분\n",
        "\n",
        "\n",
        "**Case2**\n",
        "- 주먹쥐기와 주먹 내밀기 같은 경우 손의 움직임을 통해 구별\n",
        "- 이는 단순하게 keypoint중 가장 오른쪽 x좌표의 변화량을 계산\n",
        "- Case1과 마찬가지로 keypoint의 x좌표 max-min을 계산\n",
        "- 두 클래스에 대해 계산된 x좌표 변화량들에 대해서 적절한 threshold값을 설정해 두 클래스를 구분"
      ],
      "metadata": {
        "id": "Lkv1xqisWSFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# threshold를 계산하기 위해 각 case에 대한 변화량을 계산하는 함수 정의\n",
        "def check_stats(find_list, ver):\n",
        "    train_path = '../data/train'\n",
        "    train_folders = natsorted(glob(train_path + '/*'))\n",
        "    stat_list = []\n",
        "    for _, train_folder in tqdm(enumerate(train_folders)):\n",
        "        try:\n",
        "            json_path = glob(train_folder + '/*.json')[0]\n",
        "            js = json.load(open(json_path))\n",
        "            cat = js.get('action')[0]\n",
        "            keypoints = js['annotations']\n",
        "            keypoints = np.array([point['data'] for point in keypoints])  # (N-이미지개수, 21 or 42(keypoints), 3(x,y,z 좌표))\n",
        "        except:\n",
        "            pass\n",
        "            # print(train_folder)\n",
        "        if cat in find_list:\n",
        "            # 숫자1과 검지흔들기 구분 # Case1\n",
        "            # 검지는 이미지내 keypoints들 중 가장 작은 y값(이미지 상 가장 높은 위치)을 갖는 point임. \n",
        "            # 해당 point의 x값을 뽑음.\n",
        "            if ver ==1 : \n",
        "                keypoints = keypoints[:, :, :2]  # keypoints : (N, 21 or 42, 2)\n",
        "                x_I_finger = [point_per_img[:,0][point_per_img[:,1].argmin()] for point_per_img in keypoints] # (N, 2) Y축으로 가장 작은 포인트 두개 추출\n",
        "                stat_list.append(np.max(x_I_finger) - np.min(x_I_finger))\n",
        "            \n",
        "            # 주먹쥐기와 주먹 내밀기(경고) # Case2\n",
        "            # keypoints들 중 가장 큰 x값(이미지 상 가장 우측 위치)을 갖는 point임.\n",
        "            # Case2같은 경우는 left손목이 없기때문에 해당 logic이 잘 작동함.\n",
        "            elif ver == 2:\n",
        "                keypoints = keypoints[:, :, 0]   \n",
        "                x_values = [point_per_img[point_per_img.argmax()] for point_per_img in keypoints]  # 가장 오른쪽\n",
        "                stat_list.append(np.max(x_values) - np.min(x_values))\n",
        "\n",
        "    print(stat_list)\n",
        "    return stat_list\n",
        "\n",
        "##label##\n",
        "############ ver1 ###############\n",
        "find_list0 = [0, 10, 100, 110] # ['숫자 1', '숫자1']  my hand, your hand 좌우\n",
        "find_list1= [42, 67, 142, 167] # ['부정(검지 흔들기)'] my hand, your hand 좌우\n",
        "\n",
        "############ ver2 ###############\n",
        "find_list2 = [146] # ['주먹쥐기']  Your hand 우\n",
        "find_list3 = [163] # ['경고(주먹 내밀기)'] Your hand 우\n",
        "\n",
        "find_list4 = [171] # ['주먹쥐기']  Your hand Both\n",
        "find_list5 = [191] # ['경고(주먹 내밀기)'] Your hand Both"
      ],
      "metadata": {
        "id": "quaeDWOSel0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case 1"
      ],
      "metadata": {
        "id": "JSnrpyrEW7Rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 숫자1 & 검지 흔들기\n",
        "li0 = check_stats(find_list0,1)  #숫자1 or 숫자 1\n",
        "li1 = check_stats(find_list1,1)  #부정(검지 흔들기)\n",
        "threshold_ver1 = max(li0) + 5    # Margin 5\n",
        "print(f'\\n{threshold_ver1:.3f}보다 크면 부정(검지 흔들기) 클래스')\n",
        "\n",
        "#  부정(검지 흔들기)클래스의 특정 폴더에서 9.58, 20.37과 같은 작은 값들도 존재 -> 움직임이 거의 없음; 노이즈"
      ],
      "metadata": {
        "id": "9eOB8t5_ezIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case 2"
      ],
      "metadata": {
        "id": "qN3-jI02W9dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(596), visualize(597), visualize(598) \n",
        "# 주먹쥐기(596번 폴더) 클래스인데 주먹을 활짝 펴버리는(첫번째 이미지) 노이즈 이미지 존재"
      ],
      "metadata": {
        "id": "0XCAIM3KezGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 주먹쥐기 vs 주먹 내밀기 Right\n",
        "li2 = check_stats(find_list2,2)   \n",
        "li3 = check_stats(find_list3,2)   \n",
        "threshold_ver2 = max(li2) + 5       # Margin 5\n",
        "print(f'\\n{threshold_ver2:.3f}보다 크면 주먹 내밀기(right) 클래스')\n",
        "\n",
        "# 주먹쥐기 vs 주먹 내밀기 Both\n",
        "li4 = check_stats(find_list4,2)   \n",
        "li4 = li4[1:]                       # 596번 폴더 변화량(218.313) Outlier -> 제외\n",
        "li5 = check_stats(find_list5,2)   \n",
        "threshold_ver2_both = max(li4) + 5  # Margin 5\n",
        "print(f'\\n{threshold_ver2_both:.3f}보다 크면 주먹 내밀기(both) 클래스')\n",
        "\n",
        "### Case2에서 Both와 Right를 구분하여 threshold를 구하고자 했지만 큰 차이가 없어서 실제 inference시에는 통합함. ###"
      ],
      "metadata": {
        "id": "y0nSForjezD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stratified KFold"
      ],
      "metadata": {
        "id": "MG5f3yxWXT8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 클래스 비율을 고려하여 랜덤하게 섞어주는 Stratified KFold 사용"
      ],
      "metadata": {
        "id": "S-l9TOQsXV83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    # Random Seed\n",
        "    seed = args.seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n",
        "    # Create model directory\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    Trainer(args, save_path)"
      ],
      "metadata": {
        "id": "sezztlCzezBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    for i in range(5):  # 5Folds Training\n",
        "        args.fold = i\n",
        "        args.exp_num = str(i)\n",
        "        main(args)"
      ],
      "metadata": {
        "id": "RR6vZnSBey-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference with Ensemble"
      ],
      "metadata": {
        "id": "uYFGttVfXd2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 위에서 구한 threshold를 이용하여 Rule Base inference 구축\n",
        "- replace_dict이라는 변수를 통해 헷갈리는 두 클래스를 매칭"
      ],
      "metadata": {
        "id": "vASTvDxwXgTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test의 keypoints(json) 변화량 구하기 위한 함수 정의\n",
        "def Refiner(keypoints, ver):\n",
        "    keypoints = np.array([point['data'] for point in keypoints]) \n",
        "    # 숫자1과 검지흔들기 구분\n",
        "    if ver == 1:  \n",
        "        keypoints = keypoints[:, :, :2]  \n",
        "        x_I_finger = [point_per_img[:,0][point_per_img[:,1].argmin()] for point_per_img in keypoints]\n",
        "        query_value = np.max(x_I_finger) - np.min(x_I_finger)\n",
        "    \n",
        "    # 주먹쥐기와 주먹 내밀기(경고) \n",
        "    elif ver == 2:\n",
        "        keypoints = keypoints[:, :, 0]  \n",
        "        x_values = [point_per_img[point_per_img.argmax()] for point_per_img in keypoints]\n",
        "        query_value = np.max(x_values) - np.min(x_values)\n",
        "    \n",
        "    return query_value"
      ],
      "metadata": {
        "id": "UND6M8H0ey7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./results/', exist_ok=True)\n",
        "!wget -i https://raw.githubusercontent.com/wooseok-shin/Egovision-1st-place-solution/main/load_pretrained.txt -P results   \n",
        "# MLP is All You Need님 Kernel을 참고 (https://dacon.io/competitions/official/235805/codeshare/3613?page=1&dtype=recent)\n"
      ],
      "metadata": {
        "id": "-IxEtabKe-zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "test_path = f'../data/test' \n",
        "test_folders = natsorted(glob(test_path + '/*'))\n",
        "\n",
        "args = easydict.EasyDict({'encoder_name':'regnety_040',\n",
        "                        'drop_path_rate':0,\n",
        "                        })\n",
        "\n",
        "load_pretrain = True        # Use Pretrained weights\n",
        "ensemble_test = True        # Ensemble or Single\n",
        "refine = True              # Use Refiner (Rule-base)\n",
        "\n",
        "\n",
        "if load_pretrain:  # Github로부터 Pretrained Weight Load\n",
        "    model_path0 = './results/0Fold_model.pth' # fold0\n",
        "    model_path1 = './results/1Fold_model.pth' # fold1\n",
        "    model_path2 = './results/2Fold_model.pth' # fold2\n",
        "    model_path3 = './results/3Fold_model.pth' # fold3\n",
        "    model_path4 = './results/4Fold_model.pth' # fold4\n",
        "\n",
        "else:  # 위에서 학습한 모델 Weight Load\n",
        "    model_path0 = './results/000/best_model.pth' # fold0\n",
        "    model_path1 = './results/001/best_model.pth' # fold1\n",
        "    model_path2 = './results/002/best_model.pth' # fold2\n",
        "    model_path3 = './results/003/best_model.pth' # fold3\n",
        "    model_path4 = './results/004/best_model.pth' # fold4\n",
        "\n",
        "\n",
        "# 5Fold Ensemble\n",
        "if ensemble_test:\n",
        "    model0 = Pose_Network(args).to(device)\n",
        "    model0.load_state_dict(torch.load(model_path0)['state_dict'])\n",
        "    model0.eval()\n",
        "\n",
        "    model1 = Pose_Network(args).to(device)\n",
        "    model1.load_state_dict(torch.load(model_path1)['state_dict'])\n",
        "    model1.eval()\n",
        "\n",
        "    model2 = Pose_Network(args).to(device)\n",
        "    model2.load_state_dict(torch.load(model_path2)['state_dict'])\n",
        "    model2.eval()\n",
        "\n",
        "    model3 = Pose_Network(args).to(device)\n",
        "    model3.load_state_dict(torch.load(model_path3)['state_dict'])\n",
        "    model3.eval()\n",
        "\n",
        "    model4 = Pose_Network(args).to(device)\n",
        "    model4.load_state_dict(torch.load(model_path4)['state_dict'])\n",
        "    model4.eval()\n",
        "\n",
        "    model_list = [model0, model1, model2, model3, model4]\n",
        "\n",
        "else:  # Single Best Model (Using the pretrained weight)\n",
        "    model_path = './results/single_best_model.pth'\n",
        "    single_best = Pose_Network(args).to(device)\n",
        "    single_best.load_state_dict(torch.load(model_path)['state_dict'])\n",
        "    single_best.eval()\n",
        "    model_list = [single_best]\n",
        "\n",
        "\n",
        "img_size = 288\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "sub = pd.read_csv('../data/sample_submission.csv')\n",
        "df_info = pd.read_csv('../data/hand_gesture_pose.csv')\n",
        "le = LabelEncoder()\n",
        "le.fit(df_info['pose_id'])\n",
        "trans = le.transform\n",
        "\n",
        "# Class Mapping dict\n",
        "ver1_list = trans([0, 42, 10, 67, 100, 142, 110, 167])   \n",
        "ver2_list = trans([146, 163, 171, 191])\n",
        "replace_dict = {146:163, 171:191, 0:42, 10:67, 100:142, 110:167}\n",
        "replace_dict = dict([trans(x) for x in list(replace_dict.items())])   # Mapping (Origin:0~195 to 0~156)\n",
        "\n",
        "total_list = np.concatenate([ver1_list, ver2_list]).tolist()\n",
        "\n",
        "\n",
        "for i, test_folder in tqdm(enumerate(test_folders)):\n",
        "    dir = os.path.dirname(test_folder)\n",
        "    folder_num = os.path.basename(test_folder)\n",
        "    json_path = opj(dir, folder_num, folder_num+'.json')\n",
        "    js = json.load(open(json_path))\n",
        "    keypoints = js['annotations']  # 해당 이미지에 해당하는 Keypoints\n",
        "    images_list = natsorted(glob(test_folder + '/*.png'))\n",
        "    images = []\n",
        "    for _, (point, image_name) in enumerate(zip(keypoints, images_list)):\n",
        "        croped_image = crop_image(image_name, point, margin=100)\n",
        "        image = transform(croped_image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images).to(device)\n",
        "    ensemble = np.zeros((157,), dtype=np.float32)\n",
        "    for model in model_list:\n",
        "        preds = model(images)\n",
        "        preds = torch.softmax(preds, dim=1)\n",
        "        preds = torch.mean(preds, dim=0).detach().cpu().numpy()    # shape:(157,)\n",
        "        ensemble += preds\n",
        "    preds = ensemble / len(model_list)\n",
        "    pred_class = preds.argmax().item()\n",
        "    if refine and (pred_class in total_list):\n",
        "        idx = list(replace_dict.keys()).index(pred_class) if pred_class in replace_dict.keys() else list(replace_dict.values()).index(pred_class)\n",
        "        cand1, cand2 = list(replace_dict.items())[idx]\n",
        "\n",
        "        if pred_class in ver1_list:\n",
        "            query_value = Refiner(keypoints, ver=1)\n",
        "            answer = cand1 if query_value < threshold_ver1 else cand2\n",
        "\n",
        "        elif pred_class in ver2_list:\n",
        "            query_value = Refiner(keypoints, ver=2)\n",
        "            answer = cand1 if query_value < threshold_ver2_both else cand2\n",
        "\n",
        "        preds[answer] = 1\n",
        "        preds = np.where(preds != 1, 0, preds)  # Refiner를 통해 나온 class를 제외한 나머지의 확률값은 모두 0으로 변환\n",
        "\n",
        "    sub.iloc[i, 1:] = preds.astype(float)\n",
        "\n",
        "sub.to_csv('./results/submission_train_add_ensemble_rule.csv',index=False)"
      ],
      "metadata": {
        "id": "8soQPVaDe-w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tw74wlHCe-um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KShnnOmte-sY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}