{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd2facdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T06:05:26.818267Z",
     "start_time": "2022-06-24T06:05:26.799317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Women's E-commerce Clothing Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabaf3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./kaggle.csv',index_col =[0])\n",
    "\n",
    "# check data values\n",
    "data.isnull().sum()/len(data)*100\n",
    "data.info()\n",
    "\n",
    "# drop unwanted columns\n",
    "data.drop(labels =['Clothing ID','Title'],axis = 1,inplace = True) #Dropping unwanted columns\n",
    "data[data['Review Text'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c33230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 항목별 data distribution 확인\n",
    "import plotly.express as px\n",
    "px.histogram(data, x = 'Age')\n",
    "px.histogram(data, x = data['Rating'])\n",
    "px.histogram(data, x = data['Class Name'])\n",
    "px.scatter(data, x=\"Age\", y=\"Positive Feedback Count\", facet_row=\"Recommended IND\", facet_col=\"Rating\",trendline=\"ols\",category_orders={\"Rating\": [1,2,3,4,5],'Recommended IND':[0,1]})\n",
    "# violin plot -> 연속형 데이터에 대한 box plot이라고 생각\n",
    "px.violin(data, x=\"Age\", y=\"Department Name\", orientation=\"h\", color=\"Recommended IND\")\n",
    "px.box(data, x=\"Age\", y=\"Division Name\", orientation=\"h\",color = 'Recommended IND')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleagning text data\n",
    "err1 = data['Review Text'].str.extractall(\"(&amp)\")\n",
    "err2 = data['Review Text'].str.extractall(\"(\\xa0)\")\n",
    "print('with &amp',len(err1[~err1.isna()]))\n",
    "print('with (\\xa0)',len(err2[~err2.isna()]))\n",
    "\n",
    "data['Review Text'] = data['Review Text'].str.replace('(&amp)','')\n",
    "data['Review Text'] = data['Review Text'].str.replace('(\\xa0)','')\n",
    "err1 = data['Review Text'].str.extractall(\"(&amp)\")\n",
    "print('with &amp',len(err1[~err1.isna()]))\n",
    "err2 = data['Review Text'].str.extractall(\"(\\xa0)\")\n",
    "print('with (\\xa0)',len(err2[~err2.isna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc31eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install TextBlob\n",
    "#from textblob import *\n",
    "'''\n",
    "TextBlob: NTTK와 더불어 많이 쓰이는 텍스트 전처리 라이브러리\n",
    "sentiment하의 polarity는 문장을 부정, 긍정 정도에 따라 -1과 1사이의 값으로 표현한다.\n",
    "'''\n",
    "#-> 문장을 빠르게 감성분석할 수 있음\n",
    "data['polarity'] = data['Review Text'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "data['polarity']\n",
    "px.histogram(data, x = 'polarity')\n",
    "\n",
    "px.box(data, y=\"polarity\", x=\"Department Name\", orientation=\"v\",color = 'Recommended IND')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 길이 기준 eda\n",
    "data['review_len'] = data['Review Text'].astype(str).apply(len)\n",
    "px.histogram(data, x = 'review_len')\n",
    "\n",
    "data['token_count'] = data['Review Text'].apply(lambda x: len(str(x).split()))\n",
    "px.histogram(data, x = 'token_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb940e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples\n",
    "\n",
    "# positive polarity\n",
    "sam = data.loc[data.polarity == 1,['Review Text']].sample(3).values\n",
    "for i in sam:\n",
    "    print(i[0])\n",
    "# neutral polarity\n",
    "sam = data.loc[data.polarity == 0.5,['Review Text']].sample(3).values\n",
    "for i in sam:\n",
    "    print(i[0])\n",
    "# negative polarity\n",
    "sam = data.loc[data.polarity < 0,['Review Text']].sample(3).values\n",
    "for i in sam:\n",
    "    print(i[0])\n",
    "\n",
    "negative = (len(data.loc[data.polarity <0,['Review Text']].values)/len(data))*100\n",
    "positive = (len(data.loc[data.polarity >0.5,['Review Text']].values)/len(data))*100\n",
    "neutral  = len(data.loc[data.polarity >0 ,['Review Text']].values) - len(data.loc[data.polarity >0.5 ,['Review Text']].values)\n",
    "neutral = neutral/len(data)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8611637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heatmap\n",
    "import seaborn as sns\n",
    "sns.heatmap(X.corr(),annot =True) # column correlation 계산\n",
    "\n",
    "# multi-colinearity\n",
    "set1 =set()\n",
    "cor = X.corr()\n",
    "for i in cor.columns:\n",
    "    for j in cor.columns:\n",
    "        if cor[i][j]>0.8 and i!=j:\n",
    "            set1.add(i)\n",
    "print(set1)\n",
    "\n",
    "X = X.drop(labels = ['token_count'],axis = 1)\n",
    "X.corr()\n",
    "\n",
    "class1 =[]\n",
    "for i in X.polarity:\n",
    "    if float(i)>=0.0:\n",
    "        class1.append(1)\n",
    "    elif float(i)<0.0:\n",
    "        class1.append(0)\n",
    "X['sentiment'] = class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3224cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "corpus =[]\n",
    "X.index = np.arange(len(X))\n",
    "\n",
    "# preprocessing, \n",
    "# re + tokenizing + stemming + corpus creation\n",
    "for i in range(len(X)):\n",
    "    review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n",
    "    review =' '.join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "# BOW 카운트 기반 언어모델\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "cv  = CV(max_features = 3000,ngram_range=(1,1))\n",
    "X_cv = cv.fit_transform(corpus).toarray()\n",
    "y = y.values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cv, y, test_size = 0.20, random_state = 0)\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "classifier = BernoulliNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TV\n",
    "tv  = TV(ngram_range =(1,1),max_features = 3000)\n",
    "X_tv = tv.fit_transform(corpus).toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tv, y, test_size = 0.20, random_state = 0)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626eabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words = 3000)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "word_index = tokenizer.word_index\n",
    "count = 0\n",
    "for i,j in word_index.items():\n",
    "    if count == 11:\n",
    "        break\n",
    "    print(i,j)\n",
    "    count = count+1\n",
    "    \n",
    " # Embedding + pooling + MLP   \n",
    "embedding_dim = 64\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(3000, embedding_dim),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary() \n",
    "num_epochs = 10\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(padded,y,epochs= num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f4d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da85cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52087be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cebb82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45950b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58cc017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72f723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5015a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383ba75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0eab55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
