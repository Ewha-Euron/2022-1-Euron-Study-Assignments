{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T09:44:44.029886Z",
     "start_time": "2022-03-10T09:44:43.048596Z"
    }
   },
   "outputs": [],
   "source": [
    "# 디렉토리 구조\n",
    "import os\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))\n",
    "list_files('../')\n",
    "\n",
    "# 현재 디렉토리 출력\n",
    "import os.path\n",
    "folder=os.getcwd()\n",
    "print ('current directory :%s' % folder)\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    ext=filename.split('.')[-1]\n",
    "    if ext == 'exe':\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T09:44:21.612818Z",
     "start_time": "2022-03-10T09:44:21.598858Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "[처음 보는 라이브러리 정리]\n",
    "\n",
    "(1) easydict\n",
    "-> key값 대신 (obj).attribute의 도트 표기법으로 접근할 수 있는 딕셔너리를 제공하는 라이브러리\n",
    "-> ipynb 에서 사용이 불가한 argparse 대신 easydict을 사용해 인자를 삽입/수정할 수 있다. \n",
    "\n",
    "(2) natsort\n",
    "-> 텍스트로 된 숫자(파일명)을 정렬하기 위한 라이브러리. \n",
    "-> 지정된 경로의 파일명을 정렬해서 리스트 형식으로 반환.\n",
    "\n",
    "(3) ptflops\n",
    "-> (pytorch로 생성된?) 딥러닝 모델의 연산량(flops)를 계산해주기 위한 라이브러리 \n",
    "\n",
    "(4) StratifiedKFold /  StratifiedGroupKFold\n",
    "-> k-fold cross validation(교차검증): \n",
    "교차 검증법은 overfitting을 방지하기 위한 기법 중 하나로 test 데이터와 별개의 validation set, k개의 학습 데이터 fold를 만들어 바꾸어 가면서 학습을 진행하는 것이다.\n",
    "-> stratified k-fold는 y 값(label 데이터)의 분포에 따라 train/validation 데이터를 나눈다. 각 fold에 클래스 비율을 동일하게 갖도록 설정함으로써 데이터가 몰리는 것을 방지한다. \n",
    "\n",
    "(5) timm\n",
    "-> py'T'orch 'IM'age 'M'odels 이다.\n",
    "-> 컴퓨터 비전 분야의 SOTA모델(pretrained on ImageNet data), 레이어, optim, dataloader, augmentation등의 유용한 툴을 제공하는 라이브러리이다. \n",
    "\n",
    "(6) glob\n",
    "-> 파이썬 프로그램을 작성할 때, 특정한 패턴이나 확장자를 가진 파일들의 경로나 이름이 필요할 때가 있다.\n",
    "glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환한다. \n",
    "-> 단, 조건에 정규식을 사용할 수 없으며 엑셀 등에서도 사용할 수 있는 '*'와 '?'같은 와일드카드만을 지원한다.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T14:32:55.365280Z",
     "start_time": "2022-03-10T14:32:44.661715Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraray import\n",
    "import os\n",
    "import cv2 # openCV\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import easydict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "from os.path import join as opj\n",
    "from ptflops import get_model_complexity_info\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "from PIL import Image\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, grad_scaler\n",
    "from torchvision import transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T09:44:50.492194Z",
     "start_time": "2022-03-10T09:44:50.477240Z"
    }
   },
   "outputs": [],
   "source": [
    "# argparse 가 아닌 easydict을 이용해서 input arguments(hyper parameters)작성\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "    {'exp_num':'0',\n",
    "     'experiment':'Base',\n",
    "     'tag':'Default',\n",
    "\n",
    "     # Path settings\n",
    "     'data_path':'../data',\n",
    "     'fold':4,\n",
    "     'Kfold':5,\n",
    "     'model_path':'results/',\n",
    "\n",
    "     # Model parameter settings\n",
    "     'encoder_name':'regnety_040',\n",
    "     'drop_path_rate':0.2,\n",
    "     \n",
    "     # Training parameter settings\n",
    "     ## Base Parameter\n",
    "     'img_size':288,\n",
    "     'batch_size':16,\n",
    "     'epochs':60,\n",
    "     'optimizer':'Lamb',\n",
    "     'initial_lr':5e-6,\n",
    "     'weight_decay':1e-3,\n",
    "\n",
    "     ## Augmentation\n",
    "     'aug_ver':2,\n",
    "     'flipaug_ratio':0.3,\n",
    "     'margin':50,\n",
    "     'random_margin':True,\n",
    "\n",
    "     ## Scheduler\n",
    "     'scheduler':'cycle',\n",
    "     'warm_epoch':5,\n",
    "     ### Cosine Annealing\n",
    "     'min_lr':5e-6,\n",
    "     'tmax':145,\n",
    "     ### OnecycleLR\n",
    "     'max_lr':1e-3,\n",
    "\n",
    "     ## etc.\n",
    "     'patience':50,\n",
    "     'clipping':None,\n",
    "\n",
    "     # Hardware settings\n",
    "     'amp':True,\n",
    "     'multi_gpu':False,\n",
    "     'logging':False,\n",
    "     'num_workers':4,\n",
    "     'seed':42\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T09:44:21.815259Z",
     "start_time": "2022-03-10T09:44:21.800300Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "[Image data augmentation]\n",
    "\"A survey on Image Data Augmentation for Deep Learning\" \n",
    "(https://journalofbigdata.springeropen.com/track/pdf/10.1186/s40537-019-0197-0.pdf)\n",
    "-> data augmentation은 overfitting 상황에서 variance를 낮추기 위한 Regularization 기법의 일종으로 원본 train data에 여러 가지 변환을 처리해 데이터의 양을 늘리는 작업이다.   \n",
    "\n",
    "(1) image random crop\n",
    "-> 아래 crop_image 함수가 구현하고 있는 random crop data augmentation은 이미지 데이터의 일부 영역에서 추출한 feature만을 가지고도 라벨 판별이 가능하도록 하기 위해 사용한다. \n",
    "-> 좌표를 가리키는 array point를 기준으로 각 x,y좌표보다 margin만큼 더 크고, 작은 영역을 crop한다. \n",
    "\n",
    "(2) image filp\n",
    "-> 이미지를 filp해 training data를 늘리는 방법(좌우)(상하 반전은 잘 사용 X)\n",
    "\n",
    "(3) image rotation\n",
    "-> 이미지를 랜덤한 방향으로 랜덤한 각도만큼 회전시켜 training set으로 사용. task에 따라 bounding box 재정의가 필요하다.(object detection)\n",
    "-> 더 많은 라벨을 가진 데이터를 수집할 수 없을 때 사용하는 증강 방법\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T09:44:53.296516Z",
     "start_time": "2022-03-10T09:44:53.287540Z"
    }
   },
   "outputs": [],
   "source": [
    "# keypoint를 기준으로 이미지를 crop하기 위한 함수 정의\n",
    "# train과 test시 해당 함수가 적용된 crop이미지가 inputs으로 들어가게 됩니다.\n",
    "def crop_image(imges, point, margin=100):\n",
    "    image = np.array(Image.open(imges).convert('RGB'))\n",
    "    point = point['data'] \n",
    "    max_point = np.max(np.array(point), axis=0).astype(int) + margin\n",
    "    min_point = np.min(np.array(point), axis=0).astype(int) - margin\n",
    "    max_point = max_point[:-1] # remove Z order\n",
    "    min_point = min_point[:-1] # remove Z order\n",
    "\n",
    "    max_x, max_y = max_point\n",
    "    min_x, min_y = min_point\n",
    "    max_y += margin  # 손목까지\n",
    "    \n",
    "    # 데이터 포인트의 크기가 원 이미지를 넘어서는 경우를 방지\n",
    "    max_x = max_x if max_x < 1920 else 1920\n",
    "    max_y = max_y if max_y < 1080 else 1080\n",
    "    min_x = min_x if min_x > 0 else 0\n",
    "    min_y = min_y if min_y > 0 else 0\n",
    "    \n",
    "    crop_image = image[min_y:max_y, min_x:max_x]\n",
    "\n",
    "    return crop_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[데이터 형식]\n",
    " # 다운받은 데이터는 train, test 폴더가 있다.\n",
    "train data 폴더 안에는 정수 명의 폴더목록이 있으며 그 안에는 같은 손동작 사진이 여러장과 json파일이 들어있음.\n",
    "jason 파일에는 \"action\", \"actor\",\"id\",\"annotations\" 속성이 들어있다. \n",
    "\"action\"은 len 2 짜리 리스트 (eg.[175, \"\\uc190 \\uc548\\uacbd\"]) -> 무슨 의미? 손동작의 label\n",
    "\"actor\"는 배우에 대한 정보\n",
    "\"annotations\"은 \"image_id\"와 \"data\"키를 가지는 딕셔너리, data는 각 png이미지 파일의 poision-> [이미지의 id, label, key_point 정보.]\n",
    "data/\n",
    "        df_train.csv\n",
    "        df_train_add.csv\n",
    "        hand_gesture_pose.csv\n",
    "        sample_submission.csv\n",
    "        test/\n",
    "            649/ \n",
    "                0.png, 1.png, 2.png, 3.png, 4.png, 5.png ,6.png, 649.json\n",
    "                . \n",
    "                . \n",
    "                .\n",
    "            865/\n",
    "                0.png, 1.png, 2.png, 3.png, 4.png, 5.png ,6.png, 649.json\n",
    "            \n",
    "        train/\n",
    "            0/ \n",
    "                0.png, 1.png, 2.png, 3.png, 4.png, 5.png ,6.png, 649.json\n",
    "                . \n",
    "                . \n",
    "                .\n",
    "            653/\n",
    "                0.png, 1.png, 2.png, 3.png, 4.png, 5.png ,6.png, 649.json\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T09:45:00.514754Z",
     "start_time": "2022-03-10T09:44:59.775779Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataloader에서 사용할 dataframe 만들기\n",
    "\n",
    "train_path = '../data/train'\n",
    "train_folders = natsorted(glob(train_path + '/*')) # train 폴더 안의 데이터 폴더명(정수) 정렬\n",
    "\n",
    "answers = []\n",
    "for train_folder in train_folders:\n",
    "    json_path = glob(train_folder + '/*.json')[0]\n",
    "    js = json.load(open(json_path))\n",
    "    cat = js.get('action')[0]\n",
    "    cat_name = js.get('action')[1]\n",
    "    \n",
    "    images_list = glob(train_folder + '/*.png') # 폴더 안의 png 이미지, 갯수는 다 다름\n",
    "    for image_name in images_list:\n",
    "        answers.append([image_name, cat, cat_name]) # 이미지 번호와 정답 label(cat, cat_name)을 answer로 묶가\n",
    "\n",
    "answers = pd.DataFrame(answers, columns = ['train_path','answer', 'answer_name'])\n",
    "answers.to_csv('../data/df_train.csv', index=False) # 라벨들을 따로 묶은 것 csv로 저장\n",
    "\n",
    "# 클래스가 1개뿐인 폴더들 Augmentation해서 이미지 생성후 dataframe재정의\n",
    "# 새롭게 정의한 dataframe을 학습에 이용시 약간의 성능향상을 확인할 수 있었음.\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# data_path = '../data'\n",
    "data_path = 'C:/Users/cse/Desktop/Jupyter/eruron/data' # -> 하위 폴더 아니라 상위 폴더에 저장되서 직접 경로 바꿈 (왜지?)\n",
    "\n",
    "df_train = pd.read_csv(opj(data_path, 'df_train.csv'))\n",
    "df_info = pd.read_csv(opj(data_path, 'hand_gesture_pose.csv'))\n",
    "# 키값 기준으로 두 csv파일의 열 병합 \n",
    "df_train = df_train.merge(df_info[['pose_id', 'gesture_type', 'hand_type']],\n",
    "                        how='left', left_on='answer', right_on='pose_id') # how='left' -> df2에 값이 없는 경우 NaN\n",
    "\n",
    "save_folder = 'train' \n",
    "for i in range(649, 649+5): \n",
    "    if not os.path.exists(opj(data_path, save_folder, str(i))):  # 645~654 폴더 추가 -> 아래 aug data\n",
    "        os.makedirs(opj(data_path, save_folder, str(i)))\n",
    "\n",
    "# flip aug가능한 label : 131, 47 (one sample)\n",
    "oslabel_fliplabel = [(131,156), (47, 22)] # one sample label, flip label  # -> 한개 아닌데...?\n",
    "folders = ['649', '650'] # Train 648번 folder에 이은 number 생성\n",
    "for label, folder in tqdm(zip(oslabel_fliplabel, folders)):\n",
    "    idx = 0\n",
    "    os_label, f_label  = label[0], label[1]\n",
    "    one_sample = df_train[df_train['answer'] == os_label].reset_index(drop=True)\n",
    "    temp = df_train[df_train['answer'] == f_label].reset_index(drop=True)\n",
    "    train_folders = natsorted(temp['train_path'].apply(lambda x : x[:-6]).unique())\n",
    "    for train_folder in (train_folders):\n",
    "        json_path = glob(train_folder + '/*.json')[0]\n",
    "        js = json.load(open(json_path))\n",
    "        keypoints = js['annotations']\n",
    "        images_list = natsorted(glob(train_folder + '/*.png'))\n",
    "        for _, (point, image_name) in enumerate(zip(keypoints, images_list)):\n",
    "            print(\"point: \",point)\n",
    "            croped_image = crop_image(image_name, point, margin=50)\n",
    "            flip_img = cv2.flip(croped_image, 1)\n",
    "            save_path = opj(data_path, save_folder, folder, f'{idx}.png')\n",
    "            idx += 1\n",
    "            cv2.imwrite(save_path, flip_img)\n",
    "            df_train.loc[len(df_train)] = [save_path] + one_sample.iloc[0][1:].values.tolist()\n",
    "\n",
    "def rotation(img, angle):\n",
    "    angle = int(random.uniform(-angle, angle))\n",
    "    h, w = img.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((int(w/2), int(h/2)), angle, 1)\n",
    "    img = cv2.warpAffine(img, M, (w, h)) \n",
    "    return img\n",
    "\n",
    "oslabel = [92, 188, 145]\n",
    "folder = ['651', '652', '653']\n",
    "for label, folder in tqdm(zip(oslabel, folder)):\n",
    "    idx = 0\n",
    "    one_sample = df_train[df_train['answer'] == label].reset_index(drop=True)\n",
    "    train_folders = natsorted(temp['train_path'].apply(lambda x : x[:-6]).unique())\n",
    "    for train_folder in (train_folders):\n",
    "        json_path = glob(train_folder + '/*.json')[0]\n",
    "        js = json.load(open(json_path))\n",
    "        keypoints = js['annotations']\n",
    "        images_list = natsorted(glob(train_folder + '/*.png'))\n",
    "        for _, (point, image_name) in enumerate(zip(keypoints, images_list)):\n",
    "            croped_image = crop_image(image_name, point, margin=50)\n",
    "            aug_img = rotation(croped_image, 30)\n",
    "            save_path = opj(data_path, save_folder, folder, f'{idx}.png')\n",
    "            idx += 1\n",
    "            cv2.imwrite(save_path, aug_img)\n",
    "            df_train.loc[len(df_train)] = [save_path] + one_sample.iloc[0][1:].values.tolist()\n",
    "\n",
    "df_train.to_csv('../data/df_train_add.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset에 475, 543 폴더는 의도하지 않은 나머지 손에 대해서도 Keypoint가 잡히게 됨.\n",
    "# Json의 Keypoint를 사용하기위해 475,543폴더인 경우 해당 부분 Keypoint 제거\n",
    "def remove_keypoints(folder_num, points):\n",
    "    lst = []\n",
    "    for x,y,z in points:\n",
    "        cond1 = x<250 and y>800\n",
    "        cond2 = x>1400 and y<400\n",
    "        if not (cond1 or cond2):\n",
    "            lst.append([x,y,z]) \n",
    "    # print('Finished removing {} wrong keypoints....'.format(folder_num))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T08:04:52.980558Z",
     "start_time": "2022-03-10T08:04:52.905758Z"
    }
   },
   "outputs": [],
   "source": [
    "# train dataframe 에 augmentation 적용\n",
    "class Train_Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None, df_flip_info=None, flipaug_ratio=0, label_encoder=None, margin=50, random_margin=True):\n",
    "        self.id = df['train_path'].values\n",
    "        self.target = df['answer'].values\n",
    "        self.transform = transform\n",
    "        self.margin = margin\n",
    "        self.random_margin = random_margin\n",
    "\n",
    "        # Flip Augmentation (Change target class)\n",
    "        if df_flip_info is not None:\n",
    "            self.use_flip = True\n",
    "            print('Use Flip Augmentation')\n",
    "            left = label_encoder.transform(df_flip_info['left'])\n",
    "            right = label_encoder.transform(df_flip_info['right'])\n",
    "            left_to_right = dict(zip(left, right))\n",
    "            right_to_left = dict(zip(right, left))\n",
    "            \n",
    "            self.flip_info = left_to_right.copy()\n",
    "            self.flip_info.update(right_to_left)        \n",
    "            self.flip_possible_class = list(set(np.concatenate([left, right])))\n",
    "        self.flipaug_ratio = flipaug_ratio\n",
    "\n",
    "        print(f'Dataset size:{len(self.id)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(Image.open(self.id[idx]).convert('RGB'))\n",
    "        target = self.target[idx]\n",
    "\n",
    "        # Load Json File\n",
    "        try:\n",
    "            image_num = int(Path(self.id[idx]).stem)\n",
    "            dir = os.path.dirname(self.id[idx])\n",
    "            folder_num = os.path.basename(dir)\n",
    "            json_path = opj(dir, folder_num+'.json')\n",
    "            js = json.load(open(json_path))\n",
    "            keypoints = js['annotations'][image_num]['data']  # 해당 이미지에 해당하는 Keypoints\n",
    "        except:  # Augmentation으로 직접 새로 만든 Folder는 Json이 없으므로 바로 Return (미리 손 부분이 Crop된 상태로 저장하였음.)\n",
    "            image = self.transform(Image.fromarray(image))\n",
    "            return image, np.array(target)\n",
    "\n",
    "        if folder_num in ['475', '543']:\n",
    "            keypoints = remove_keypoints(folder_num, keypoints)\n",
    "\n",
    "        # Image Crop using keypoints  -> crop_image 왜 안써?\n",
    "        max_point = np.max(np.array(keypoints), axis=0).astype(int) + self.margin\n",
    "        min_point = np.min(np.array(keypoints), axis=0).astype(int) - self.margin\n",
    "        max_point = max_point[:-1] # remove Z order\n",
    "        min_point = min_point[:-1] # remove Z order\n",
    "\n",
    "        max_x, max_y = max_point\n",
    "        min_x, min_y = min_point\n",
    "        max_y += 100  # 손목부분까지 여유를 주기위해\n",
    "\n",
    "        # 매 에폭마다 Margin이 조금씩 다르게 들어가므로 한 폴더 내 비슷한 이미지들의 Overfitting을 방지하는 효과를 주기위해 (Only Train Phase)\n",
    "        if self.random_margin:  \n",
    "            if random.random() < 0.5:\n",
    "                max_x += self.margin\n",
    "            if random.random() < 0.5:\n",
    "                max_y += self.margin\n",
    "            if random.random() < 0.5:\n",
    "                min_x -= self.margin\n",
    "            if random.random() < 0.5:\n",
    "                min_y -= self.margin\n",
    "        else:\n",
    "            max_x += self.margin\n",
    "            max_y += self.margin\n",
    "            min_x -= self.margin\n",
    "            min_y -= self.margin\n",
    "\n",
    "        # 데이터 포인트의 크기가 원 이미지를 넘어서는 경우를 방지\n",
    "        max_x = max_x if max_x < 1920 else 1920\n",
    "        max_y = max_y if max_y < 1080 else 1080\n",
    "        min_x = min_x if min_x > 0 else 0\n",
    "        min_y = min_y if min_y > 0 else 0\n",
    "        \n",
    "        image = image[min_y:max_y, min_x:max_x]\n",
    "\n",
    "        # FlipAug\n",
    "        if (random.random() < self.flipaug_ratio) and (target in self.flip_possible_class):\n",
    "            image = np.flip(image, axis=1)  # (H, W, C)에서 width 축 flip\n",
    "            target = self.flip_info[target]\n",
    "\n",
    "        image = self.transform(Image.fromarray(image))\n",
    "        return image, np.array(target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id)\n",
    "\n",
    "def get_loader(df, batch_size, shuffle, num_workers, transform, df_flip_info=None, \n",
    "                flipaug_ratio=0, label_encoder=None, margin=50, random_margin=True):\n",
    "    dataset = Train_Dataset(df, transform, df_flip_info=df_flip_info, flipaug_ratio=flipaug_ratio, \n",
    "                            label_encoder=label_encoder, margin=margin, random_margin=random_margin)\n",
    "    # torch.utils.data.DataLoader (iterable generator) 반환\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True,\n",
    "                                drop_last=False)\n",
    "    return data_loader\n",
    "\n",
    "# transform 전처리\n",
    "def get_train_augmentation(img_size, ver): \n",
    "    if ver==1:\n",
    "        # For Test\n",
    "        transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)), # PIL 사이즈 재조정\n",
    "                transforms.ToTensor(), # np tensor 형변환\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], # 정규화\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "                ])\n",
    "\n",
    "\n",
    "    if ver==2:\n",
    "        # For Train\n",
    "        transform = transforms.Compose([\n",
    "                transforms.RandomAffine(20), \n",
    "                transforms.RandomPerspective(),\n",
    "                transforms.ToTensor(),\n",
    "\t            transforms.Resize((img_size, img_size)),\n",
    "    \t        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[전처리 기법]\n",
    "perspective transformation(= Homography Matrix = Projective Transformation)\n",
    "Homograhpy\n",
    "한 평면을 다른 평면에 투영(Projection) 시켰을 때, 투영된 대응점들 사이에서는 일정한 변환 관계가 성립한다. 이 변환 관계를 Homography라고 한다.\n",
    "점 매핑을 통해 projection으로 인한 변화를 보정해주는 변환 (-> SIFT같은 기법?)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "'''\n",
    "[regnet] \n",
    "\"Designing Network Design Spaces\"\n",
    "최근 NAS가 많이 사용되고 있지만 이런 방식은 특정 세팅에만 적잡한 single network instance를 만들기 때문에 general하지 않다는 단점이 있다.\n",
    "->  manual design과 NAS 방식의 각각의 장점을 조합한 새로운 디자인 패러다임\n",
    "-> manual desig와 같이 네트워크 구조를 설명이 가능하며 심플하고 모든 세팅을 아울러 잘 작동하는 네트워크를 만들 수 있는 general한 design principle을 발견하는 것을 목표로 한다.\n",
    "NAS와 같이 이러한 목표를 달성하도록 도와주는 semi-automated procedure의 장점을 활용한다.\n",
    "-> 결국, 전체적으로 하나의 특정 network instance를 만드는 것이 목표가 아닌 network의 population들을 파라미터화 시킨 design space를 디자인 하는 것이 목표이다.\n",
    "-> stem-body-cell\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T08:05:21.736633Z",
     "start_time": "2022-03-10T08:05:21.729652Z"
    }
   },
   "outputs": [],
   "source": [
    "class Pose_Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model(args.encoder_name, pretrained=True,\n",
    "                                    drop_path_rate=args.drop_path_rate,\n",
    "                                    ) # timm으로 pretrained 모델 불러오기\n",
    "        num_head = self.encoder.head.fc.in_features\n",
    "        self.encoder.head.fc = nn.Linear(num_head, 157)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[learning rate scheduler]\n",
    "Lr Scheduler는 미리 학습 일정을 정해두고, 그 일정에 따라 학습률을 조정하는 방법이다. \n",
    "일반적으로는 warmup이라는 파라미터를 정하고 현재 step이 warmup보다 낮을 경우는 learning rate를 linear하게 증가 시키고, \n",
    "warmup 이후에는 각 Lr Scheduler에서 정한 방법대로 learning rate를 update한다.\n",
    "\n",
    "Constant Lr Scheduler, Exponential Lr Scheduler, King Lr Scheduler, Cosine Lr Scheduler 등이 있다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T08:05:24.119260Z",
     "start_time": "2022-03-10T08:05:24.105297Z"
    }
   },
   "outputs": [],
   "source": [
    "# Warmup Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
    "\n",
    "# Logging - INFO log 기록\n",
    "def get_root_logger(logger_name='basicsr',\n",
    "                    log_level=logging.INFO,\n",
    "                    log_file=None):\n",
    "\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    # if the logger has been initialized, just return it\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(format=format_str, level=log_level)\n",
    "\n",
    "    if log_file is not None:\n",
    "        file_handler = logging.FileHandler(log_file, 'w')\n",
    "        file_handler.setFormatter(logging.Formatter(format_str))\n",
    "        file_handler.setLevel(log_level)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class AvgMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.losses.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T08:05:27.286786Z",
     "start_time": "2022-03-10T08:05:27.241907Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, args, save_path):\n",
    "        '''\n",
    "        args: arguments\n",
    "        save_path: Model 가중치 저장 경로\n",
    "        '''\n",
    "        super(Trainer, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Logging\n",
    "        log_file = os.path.join(save_path, 'log.log')\n",
    "        self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n",
    "        self.logger.info(args)\n",
    "        self.logger.info(args.tag)\n",
    "\n",
    "        # Train, Valid Set load\n",
    "        ############################################################################\n",
    "        # df_train = pd.read_csv(opj(args.data_path, 'df_train.csv'))\n",
    "        df_train = pd.read_csv(opj(args.data_path, 'df_train_add.csv'))\n",
    "        df_info = pd.read_csv(opj(args.data_path, 'hand_gesture_pose.csv'))\n",
    "\n",
    "        df_train = df_train.merge(df_info[['pose_id', 'gesture_type', 'hand_type']], \\\n",
    "                                how='left', left_on='answer', right_on='pose_id')\n",
    "\n",
    "        # 폴더별(Group)로 각 번호 부여\n",
    "        df_train['groups'] = df_train['train_path'].apply(lambda x:x.split('/')[3])\n",
    "        df_train.loc[:,:] = natsorted(df_train.values)\n",
    "        # 노이즈 이미지 제거: 596번은 주먹쥐기 이미지인데 갑자기 손바닥을 펴는 노이즈 이미지가 5장있음 + 0번 폴더에 9번 이미지 역시 잘못된 클래스\n",
    "        drop_idx = df_train[df_train['groups'].isin(['596'])].index.tolist()[3:8] + [9]  \n",
    "        df_train = df_train.drop(drop_idx).reset_index(drop=True)  \n",
    "        le = LabelEncoder()\n",
    "        df_train['answer'] = le.fit_transform(df_train['answer'])\n",
    "        \n",
    "        # Split Fold\n",
    "        # kf = StratifiedGroupKFold(n_splits=args.Kfold)\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df_train, y=df_train['answer'])):\n",
    "            df_train.loc[val_idx, 'fold'] = fold\n",
    "        df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n",
    "        df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n",
    "        \n",
    "        # Augmentation\n",
    "        self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
    "        self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
    "        \n",
    "        ######################################################################\n",
    "        # Flip Augmentation을 위한 Mapping dataframe\n",
    "        df_info = pd.read_csv('../data/hand_gesture_pose.csv')\n",
    "        df_info = df_info[df_info['hand_type'] != 'both']\n",
    "        # drop idx, 동일한 약속, gesture_type, hand_type인데 다른 클래스인 경우 존재 -> 약속 1과 2로 이름을 나누어줌.\n",
    "        df_info.loc[[105, 128], 'pose_name'] = '약속 1'  # idx: (105, 128)\n",
    "        df_info.loc[[101, 124], 'pose_name'] = '약속 2'  # idx: (101, 124)\n",
    "\n",
    "        # drop 41 idx, 동일한 약속, my hand, right class가 49와 54로 두 개있어 Mapping df만들 때 문제가 발생하여 미리 49번 클래스 처리\n",
    "        df_info = df_info.drop(41)\n",
    "\n",
    "        # Make a mapping dataframe\n",
    "        df_info = df_info.groupby(['pose_name', 'view_type', 'gesture_type', 'hand_type']).sum().unstack().reset_index().dropna(axis=0)\n",
    "        df_info['left'] = df_info.pose_id.left.apply(int)\n",
    "        df_info['right'] = df_info.pose_id.right.apply(int)\n",
    "        df_flip_info = df_info.drop('pose_id', axis=1).droplevel('hand_type', axis=1).reset_index(drop=True)\n",
    "        print('Mapping dataframe Length', df_flip_info.shape)\n",
    "        ######################################################################\n",
    "        \n",
    "        # 이 위까지 데이터 읽고 전처리 적용\n",
    "        \n",
    "        # TrainLoader\n",
    "        self.train_loader = get_loader(df_train, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, transform=self.train_transform, \n",
    "                                       df_flip_info=df_flip_info, flipaug_ratio=args.flipaug_ratio, label_encoder=le, margin=args.margin, random_margin=args.random_margin)\n",
    "        self.val_loader = get_loader(df_val, batch_size=args.batch_size, shuffle=False,\n",
    "                                       num_workers=args.num_workers, transform=self.test_transform)\n",
    "\n",
    "        # Network\n",
    "        self.model = Pose_Network(args).to(self.device)\n",
    "        macs, params = get_model_complexity_info(self.model, (3, args.img_size, args.img_size), as_strings=True,\n",
    "                                                 print_per_layer_stat=False, verbose=False)\n",
    "        self.logger.info('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "        self.logger.info('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n",
    "        # Loss\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Optimizer & Scheduler\n",
    "        self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "        iter_per_epoch = len(self.train_loader)\n",
    "        self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n",
    "\n",
    "        if args.scheduler == 'cos':\n",
    "            tmax = args.tmax # half-cycle \n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max = tmax, eta_min=args.min_lr, verbose=True)\n",
    "        elif args.scheduler == 'cycle':\n",
    "            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
    "\n",
    "        \n",
    "        if args.multi_gpu:\n",
    "            self.model = nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "        # Train / Validate\n",
    "        best_loss = np.inf\n",
    "        best_acc = 0\n",
    "        best_epoch = 0\n",
    "        early_stopping = 0\n",
    "        start = time.time()\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            if args.scheduler == 'cos':\n",
    "                if epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_acc = self.training(args)\n",
    "\n",
    "            # Model weight in Multi_GPU or Single GPU\n",
    "            state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate()\n",
    "\n",
    "            # Save models\n",
    "            if val_loss < best_loss:\n",
    "                early_stopping = 0\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "\n",
    "                torch.save({'epoch':epoch,\n",
    "                            'state_dict':state_dict,\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'scheduler': self.scheduler.state_dict(),\n",
    "                    }, os.path.join(save_path, 'best_model.pth'))\n",
    "                self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if early_stopping == args.patience:\n",
    "                break\n",
    "\n",
    "        self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f}')\n",
    "        end = time.time()\n",
    "        self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n",
    "\n",
    "\n",
    "    # Training\n",
    "    def training(self, args):\n",
    "        self.model.train()\n",
    "        train_loss = AvgMeter()\n",
    "        train_acc = 0\n",
    "\n",
    "        scaler = grad_scaler.GradScaler()\n",
    "        for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
    "            images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "            targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            if self.epoch <= args.warm_epoch:\n",
    "                self.warmup_scheduler.step()\n",
    "\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    preds = self.model(images)\n",
    "                    loss = self.criterion(preds, targets)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Gradient Clipping\n",
    "                if args.clipping is not None:\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if args.scheduler == 'cycle':\n",
    "                if self.epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Metric\n",
    "            train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "            # log\n",
    "            train_loss.update(loss.item(), n=images.size(0))\n",
    "            \n",
    "        train_acc /= len(self.train_loader.dataset)\n",
    "\n",
    "        self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
    "        self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f}')\n",
    "        return train_loss.avg, train_acc\n",
    "            \n",
    "    # Validation or Dev\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = AvgMeter()\n",
    "            val_acc = 0\n",
    "\n",
    "            for _, (images, targets) in enumerate(self.val_loader):\n",
    "                images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "                targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "\n",
    "                # Metric\n",
    "                val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "                # log\n",
    "                val_loss.update(loss.item(), n=images.size(0))\n",
    "            val_acc /= len(self.val_loader.dataset)\n",
    "\n",
    "            self.logger.info(f'Valid Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f}')\n",
    "        return val_loss.avg, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T08:11:12.880332Z",
     "start_time": "2022-03-10T08:11:12.849416Z"
    }
   },
   "outputs": [],
   "source": [
    "## Case1\n",
    "img = Image.open('./etc/숫자1_검지흔들기.png')\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T08:05:33.911066Z",
     "start_time": "2022-03-10T08:05:33.897104Z"
    }
   },
   "outputs": [],
   "source": [
    "# 시각화를 위한 Function\n",
    "def visualize(folder_num):\n",
    "    path = f'../data/train/{folder_num}/*.png'\n",
    "    image_list = glob(path)\n",
    "    length = len(image_list)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, length, figsize=(50,10))\n",
    "    for i, image in enumerate(image_list):\n",
    "        image = Image.open(image).convert('RGB')\n",
    "        ax[i].imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T08:05:36.627799Z",
     "start_time": "2022-03-10T08:05:36.608850Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(opj(args.data_path, 'df_train.csv'))\n",
    "df['groups'] = df['train_path'].apply(lambda x:x.split('/')[3])\n",
    "df = df.drop_duplicates('groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T08:09:03.655499Z",
     "start_time": "2022-03-10T08:09:03.625579Z"
    }
   },
   "outputs": [],
   "source": [
    "path = f'../data/train/'\n",
    "number1_folder = df[df['answer_name'] == '숫자1']['groups'].tolist()\n",
    "shake_folder = df[df['answer_name'] == '부정(검지 흔들기)']['groups'].tolist()\n",
    "\n",
    "image1 = Image.open(opj(path, number1_folder[0], '1.png'))   # 352번 폴더\n",
    "image2 = Image.open(opj(path, shake_folder[11], '1.png'))    # 489번 폴더\n",
    "print(number1_folder[0], shake_folder[11])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "ax[0].imshow(image1)\n",
    "ax[1].imshow(image2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
