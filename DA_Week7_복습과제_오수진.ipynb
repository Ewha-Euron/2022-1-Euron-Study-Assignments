{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-18T06:42:19.380847Z","iopub.execute_input":"2022-04-18T06:42:19.381752Z","iopub.status.idle":"2022-04-18T06:42:19.420332Z","shell.execute_reply.started":"2022-04-18T06:42:19.381605Z","shell.execute_reply":"2022-04-18T06:42:19.419699Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#Importing the basic librarires\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\n\n#from brokenaxes import brokenaxes\nfrom statsmodels.formula import api\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [10,6]\n\nimport warnings \nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:42:35.607992Z","iopub.execute_input":"2022-04-18T06:42:35.608530Z","iopub.status.idle":"2022-04-18T06:42:37.546822Z","shell.execute_reply.started":"2022-04-18T06:42:35.608494Z","shell.execute_reply":"2022-04-18T06:42:37.545652Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Importing the dataset\n\ndf = pd.read_csv('/kaggle/input/song-popularity-dataset/song_data.csv')\n\ndf.drop(['song_name'], axis=1, inplace=True)\ndisplay(df.head())\n\ntarget = 'song_popularity'\nfeatures = [i for i in df.columns if i not in [target]]\n\noriginal_df = df.copy(deep=True)\n\nprint('\\n\\033[1mInference:\\033[0m The Datset consists of {} features & {} samples.'.format(df.shape[1], df.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:42:59.822799Z","iopub.execute_input":"2022-04-18T06:42:59.823132Z","iopub.status.idle":"2022-04-18T06:42:59.987190Z","shell.execute_reply.started":"2022-04-18T06:42:59.823100Z","shell.execute_reply":"2022-04-18T06:42:59.986088Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Checking the dtypes of all the columns\n\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:43:12.109565Z","iopub.execute_input":"2022-04-18T06:43:12.110337Z","iopub.status.idle":"2022-04-18T06:43:12.134392Z","shell.execute_reply.started":"2022-04-18T06:43:12.110272Z","shell.execute_reply":"2022-04-18T06:43:12.133506Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Checking number of unique rows in each feature\n\ndf.nunique().sort_values()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:43:16.556362Z","iopub.execute_input":"2022-04-18T06:43:16.556669Z","iopub.status.idle":"2022-04-18T06:43:16.573286Z","shell.execute_reply.started":"2022-04-18T06:43:16.556637Z","shell.execute_reply":"2022-04-18T06:43:16.572490Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Checking number of unique rows in each feature\n\nnu = df[features].nunique().sort_values()\nnf = []; cf = []; nnf = 0; ncf = 0; #numerical & categorical features\n\nfor i in range(df[features].shape[1]):\n    if nu.values[i]<=16:cf.append(nu.index[i])\n    else: nf.append(nu.index[i])\n\nprint('\\n\\033[1mInference:\\033[0m The Datset has {} numerical & {} categorical features.'.format(len(nf),len(cf)))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:43:21.734587Z","iopub.execute_input":"2022-04-18T06:43:21.734851Z","iopub.status.idle":"2022-04-18T06:43:21.755590Z","shell.execute_reply.started":"2022-04-18T06:43:21.734825Z","shell.execute_reply":"2022-04-18T06:43:21.754396Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Checking the stats of all the columns\n\ndisplay(df.describe())","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:43:27.638406Z","iopub.execute_input":"2022-04-18T06:43:27.638716Z","iopub.status.idle":"2022-04-18T06:43:27.707189Z","shell.execute_reply.started":"2022-04-18T06:43:27.638687Z","shell.execute_reply":"2022-04-18T06:43:27.706500Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Let us first analyze the distribution of the target variable\n\nplt.figure(figsize=[8,4])\nsns.distplot(df[target], color='g',hist_kws=dict(edgecolor=\"black\", linewidth=2), bins=30)\nplt.title('Target Variable Distribution - Median Value of Homes ($1Ms)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:46:24.826229Z","iopub.execute_input":"2022-04-18T06:46:24.826572Z","iopub.status.idle":"2022-04-18T06:46:25.241617Z","shell.execute_reply.started":"2022-04-18T06:46:24.826541Z","shell.execute_reply":"2022-04-18T06:46:25.240578Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#Visualising the categorical features \n\nprint('\\033[1mVisualising Categorical Features:'.center(100))\n\nn=2\nplt.figure(figsize=[15,3*math.ceil(len(cf)/n)])\n\nfor i in range(len(cf)):\n    if df[cf[i]].nunique()<=8:\n        plt.subplot(math.ceil(len(cf)/n),n,i+1)\n        sns.countplot(df[cf[i]])\n    else:\n        plt.subplot(2,1,2)\n        sns.countplot(df[cf[i]])\n        \nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:46:29.865958Z","iopub.execute_input":"2022-04-18T06:46:29.866282Z","iopub.status.idle":"2022-04-18T06:46:30.363949Z","shell.execute_reply.started":"2022-04-18T06:46:29.866252Z","shell.execute_reply":"2022-04-18T06:46:30.362875Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Visualising the numeric features \n\nprint('\\033[1mNumeric Features Distribution'.center(100))\n\nn=5\n\nclr=['r','g','b','g','b','r']\n\nplt.figure(figsize=[15,4*math.ceil(len(nf)/n)])\nfor i in range(len(nf)):\n    plt.subplot(math.ceil(len(nf)/3),n,i+1)\n    sns.distplot(df[nf[i]],hist_kws=dict(edgecolor=\"black\", linewidth=2), bins=10, color=list(np.random.randint([255,255,255])/255))\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=[15,4*math.ceil(len(nf)/n)])\nfor i in range(len(nf)):\n    plt.subplot(math.ceil(len(nf)/3),n,i+1)\n    df.boxplot(nf[i])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:46:37.520733Z","iopub.execute_input":"2022-04-18T06:46:37.521209Z","iopub.status.idle":"2022-04-18T06:46:41.797170Z","shell.execute_reply.started":"2022-04-18T06:46:37.521164Z","shell.execute_reply":"2022-04-18T06:46:41.796188Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Understanding the relationship between all the features\n\ng = sns.pairplot(df)\nplt.title('Pairplots for all the Feature')\ng.map_upper(sns.kdeplot, levels=4, color=\".2\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:46:42.905097Z","iopub.execute_input":"2022-04-18T06:46:42.906033Z","iopub.status.idle":"2022-04-18T07:06:50.385855Z","shell.execute_reply.started":"2022-04-18T06:46:42.905973Z","shell.execute_reply":"2022-04-18T07:06:50.384892Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#Removal of any Duplicate rows (if any)\n\ncounter = 0\nrs,cs = original_df.shape\n\ndf.drop_duplicates(inplace=True)\n\nif df.shape==(rs,cs):\n    print('\\n\\033[1mInference:\\033[0m The dataset doesn\\'t have any duplicates')\nelse:\n    print(f'\\n\\033[1mInference:\\033[0m Number of duplicates dropped/fixed ---> {rs-df.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:06:50.387366Z","iopub.execute_input":"2022-04-18T07:06:50.388278Z","iopub.status.idle":"2022-04-18T07:06:50.410991Z","shell.execute_reply.started":"2022-04-18T07:06:50.388236Z","shell.execute_reply":"2022-04-18T07:06:50.409929Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Check for empty elements\n\nnvc = pd.DataFrame(df.isnull().sum().sort_values(), columns=['Total Null Values'])\nnvc['Percentage'] = round(nvc['Total Null Values']/df.shape[0],3)*100\nprint(nvc)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:06:50.412358Z","iopub.execute_input":"2022-04-18T07:06:50.412615Z","iopub.status.idle":"2022-04-18T07:06:50.427345Z","shell.execute_reply.started":"2022-04-18T07:06:50.412586Z","shell.execute_reply":"2022-04-18T07:06:50.426364Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Converting categorical Columns to Numeric\n\ndf3 = df.copy()\n\necc = nvc[nvc['Percentage']!=0].index.values\nfcc = [i for i in cf if i not in ecc]\n#One-Hot Binay Encoding\noh=True\ndm=True\nfor i in fcc:\n    #print(i)\n    if df3[i].nunique()==2:\n        if oh==True: print(\"\\033[1mOne-Hot Encoding on features:\\033[0m\")\n        print(i);oh=False\n        df3[i]=pd.get_dummies(df3[i], drop_first=True, prefix=str(i))\n    if (df3[i].nunique()>2 and df3[i].nunique()<17):\n        if dm==True: print(\"\\n\\033[1mDummy Encoding on features:\\033[0m\")\n        print(i);dm=False\n        df3 = pd.concat([df3.drop([i], axis=1), pd.DataFrame(pd.get_dummies(df3[i], drop_first=True, prefix=str(i)))],axis=1)\n        \ndf3.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:06:50.429854Z","iopub.execute_input":"2022-04-18T07:06:50.430804Z","iopub.status.idle":"2022-04-18T07:06:50.468128Z","shell.execute_reply.started":"2022-04-18T07:06:50.430761Z","shell.execute_reply":"2022-04-18T07:06:50.467156Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#Removal of outlier:\n\ndf1 = df3.copy()\n\n#features1 = [i for i in features if i not in ['CHAS','RAD']]\nfeatures1 = nf\n\nfor i in features1:\n    Q1 = df1[i].quantile(0.25)\n    Q3 = df1[i].quantile(0.75)\n    IQR = Q3 - Q1\n    df1 = df1[df1[i] <= (Q3+(1.5*IQR))]\n    df1 = df1[df1[i] >= (Q1-(1.5*IQR))]\n    df1 = df1.reset_index(drop=True)\ndisplay(df1.head())\nprint('\\n\\033[1mInference:\\033[0m\\nBefore removal of outliers, The dataset had {} samples.'.format(df3.shape[0]))\nprint('After removal of outliers, The dataset now has {} samples.'.format(df1.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:06:50.469529Z","iopub.execute_input":"2022-04-18T07:06:50.469765Z","iopub.status.idle":"2022-04-18T07:06:50.572212Z","shell.execute_reply.started":"2022-04-18T07:06:50.469733Z","shell.execute_reply":"2022-04-18T07:06:50.571364Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#Final Dataset size after performing Preprocessing\n\ndf = df1.copy()\ndf.columns=[i.replace('-','_') for i in df.columns]\n\nplt.title('Final Dataset')\nplt.pie([df.shape[0], original_df.shape[0]-df.shape[0]], radius = 1, labels=['Retained','Dropped'], counterclock=False, \n        autopct='%1.1f%%', pctdistance=0.9, explode=[0,0], shadow=True)\nplt.pie([df.shape[0]], labels=['100%'], labeldistance=-0, radius=0.78)\nplt.show()\n\nprint(f'\\n\\033[1mInference:\\033[0m After the cleanup process, {original_df.shape[0]-df.shape[0]} samples were dropped, \\\nwhile retaining {round(100 - (df.shape[0]*100/(original_df.shape[0])),2)}% of the data.')","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:06:50.573577Z","iopub.execute_input":"2022-04-18T07:06:50.573924Z","iopub.status.idle":"2022-04-18T07:06:50.729355Z","shell.execute_reply.started":"2022-04-18T07:06:50.573878Z","shell.execute_reply":"2022-04-18T07:06:50.728443Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Splitting the data intro training & testing sets\n\nm=[]\nfor i in df.columns.values:\n    m.append(i.replace(' ','_'))\n    \ndf.columns = m\nX = df.drop([target],axis=1)\nY = df[target]\nTrain_X, Test_X, Train_Y, Test_Y = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=100)\nTrain_X.reset_index(drop=True,inplace=True)\n\nprint('Original set  ---> ',X.shape,Y.shape,'\\nTraining set  ---> ',Train_X.shape,Train_Y.shape,'\\nTesting set   ---> ', Test_X.shape,'', Test_Y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:06:50.731033Z","iopub.execute_input":"2022-04-18T07:06:50.732297Z","iopub.status.idle":"2022-04-18T07:06:50.761718Z","shell.execute_reply.started":"2022-04-18T07:06:50.732239Z","shell.execute_reply":"2022-04-18T07:06:50.760670Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Feature Scaling (Standardization)\n\nstd = StandardScaler()\n\nprint('\\033[1mStandardardization on Training set'.center(120))\nTrain_X_std = std.fit_transform(Train_X)\nTrain_X_std = pd.DataFrame(Train_X_std, columns=X.columns)\ndisplay(Train_X_std.describe())\n\nprint('\\n','\\033[1mStandardardization on Testing set'.center(120))\nTest_X_std = std.transform(Test_X)\nTest_X_std = pd.DataFrame(Test_X_std, columns=X.columns)\ndisplay(Test_X_std.describe())","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:06:50.763856Z","iopub.execute_input":"2022-04-18T07:06:50.765738Z","iopub.status.idle":"2022-04-18T07:06:50.961206Z","shell.execute_reply.started":"2022-04-18T07:06:50.765646Z","shell.execute_reply":"2022-04-18T07:06:50.959984Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#Checking the correlation\n\nprint('\\033[1mCorrelation Matrix'.center(100))\nplt.figure(figsize=[25,20])\nsns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, center=0) #cmap='BuGn'\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:06:50.962760Z","iopub.execute_input":"2022-04-18T07:06:50.963232Z","iopub.status.idle":"2022-04-18T07:06:54.228784Z","shell.execute_reply.started":"2022-04-18T07:06:50.963186Z","shell.execute_reply":"2022-04-18T07:06:54.227809Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#Testing a Linear Regression model with statsmodels\n\nTrain_xy = pd.concat([Train_X_std,Train_Y.reset_index(drop=True)],axis=1)\na = Train_xy.columns.values\n\nAPI = api.ols(formula='{} ~ {}'.format(target,' + '.join(i for i in Train_X.columns)), data=Train_xy).fit()\n#print(API.conf_int())\n#print(API.pvalues)\nAPI.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:06:54.231683Z","iopub.execute_input":"2022-04-18T07:06:54.232749Z","iopub.status.idle":"2022-04-18T07:06:54.364793Z","shell.execute_reply.started":"2022-04-18T07:06:54.232705Z","shell.execute_reply":"2022-04-18T07:06:54.363524Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nTrr=[]; Tss=[]; n=3\norder=['ord-'+str(i) for i in range(2,n)]\n#Trd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\n#Tsd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\n\nDROP=[];b=[]\n\nfor i in range(len(Train_X_std.columns)):\n    vif = pd.DataFrame()\n    X = Train_X_std.drop(DROP,axis=1)\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    vif.reset_index(drop=True, inplace=True)\n    if vif.loc[0][1]>1:\n        DROP.append(vif.loc[0][0])\n        LR = LinearRegression()\n        LR.fit(Train_X_std.drop(DROP,axis=1), Train_Y)\n\n        pred1 = LR.predict(Train_X_std.drop(DROP,axis=1))\n        pred2 = LR.predict(Test_X_std.drop(DROP,axis=1))\n        \n        Trr.append(np.sqrt(mean_squared_error(Train_Y, pred1)))\n        Tss.append(np.sqrt(mean_squared_error(Test_Y, pred2)))\n\n        #Trd.loc[i,'ord-'+str(k)] = round(np.sqrt(mean_squared_error(Train_Y, pred1)),2)\n        #Tsd.loc[i,'ord-'+str(k)] = round(np.sqrt(mean_squared_error(Test_Y, pred2)),2)\n        \nprint('Dropped Features --> ',DROP)\n#plt.plot(b)\n#plt.show()\n#print(API.summary())\n\n# plt.figure(figsize=[20,4])\n# plt.subplot(1,3,1)\n# sns.heatmap(Trd.loc[:6], cmap='BuGn', annot=True, vmin=0, vmax=Trd.max().max())\n# plt.title('Train RMSE')\n# plt.subplot(1,3,2)\n# sns.heatmap(Tsd.loc[:6], cmap='BuGn', annot=True, vmin=0, vmax=Trd.max().max()+10)\n# plt.title('Test RMSE')\n# plt.subplot(1,3,3)\n# sns.heatmap((Trd+Tsd).loc[:6], cmap='BuGn', annot=True, vmin=0, vmax=Trd.max().max()+25)\n# plt.title('Total RMSE')\n# plt.show()\n\nplt.plot(Trr, label='Train RMSE')\nplt.plot(Tss, label='Test RMSE')\n#plt.ylim([19.75,20.75])\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:06:54.366851Z","iopub.execute_input":"2022-04-18T07:06:54.367199Z","iopub.status.idle":"2022-04-18T07:07:01.850442Z","shell.execute_reply.started":"2022-04-18T07:06:54.367155Z","shell.execute_reply":"2022-04-18T07:07:01.849336Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nTrr=[]; Tss=[]; n=3\norder=['ord-'+str(i) for i in range(2,n)]\nTrd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\nTsd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\n\nm=df.shape[1]-2\nfor i in range(m):\n    lm = LinearRegression()\n    rfe = RFE(lm,n_features_to_select=Train_X_std.shape[1]-i)             # running RFE\n    rfe = rfe.fit(Train_X_std, Train_Y)\n\n    LR = LinearRegression()\n    LR.fit(Train_X_std.loc[:,rfe.support_], Train_Y)\n\n    pred1 = LR.predict(Train_X_std.loc[:,rfe.support_])\n    pred2 = LR.predict(Test_X_std.loc[:,rfe.support_])\n\n    Trr.append(np.sqrt(mean_squared_error(Train_Y, pred1)))\n    Tss.append(np.sqrt(mean_squared_error(Test_Y, pred2)))\n\n# plt.figure(figsize=[20,4])\n# plt.subplot(1,3,1)\n# sns.heatmap(Trd.loc[:6], cmap='BuGn', annot=True, vmin=0, vmax=Trd.max().max())\n# plt.title('Train RMSE')\n# plt.subplot(1,3,2)\n# sns.heatmap(Tsd.loc[:6], cmap='BuGn', annot=True, vmin=0, vmax=Trd.max().max()+10)\n# plt.title('Test RMSE')\n# plt.subplot(1,3,3)\n# sns.heatmap((Trd+Tsd).loc[:6], cmap='BuGn', annot=True, vmin=0, vmax=Trd.max().max()+25)\n# plt.title('Total RMSE')\n# plt.show()\n\nplt.plot(Trr, label='Train RMSE')\nplt.plot(Tss, label='Test RMSE')\n#plt.ylim([19.75,20.75])\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:07:01.852000Z","iopub.execute_input":"2022-04-18T07:07:01.852248Z","iopub.status.idle":"2022-04-18T07:07:05.265575Z","shell.execute_reply.started":"2022-04-18T07:07:01.852219Z","shell.execute_reply":"2022-04-18T07:07:05.264423Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"plt.plot(Trr, label='Train RMSE')\nplt.plot(Tss, label='Test RMSE')\nplt.ylim([20,20.75])\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:07:05.266854Z","iopub.execute_input":"2022-04-18T07:07:05.267077Z","iopub.status.idle":"2022-04-18T07:07:05.445953Z","shell.execute_reply.started":"2022-04-18T07:07:05.267050Z","shell.execute_reply":"2022-04-18T07:07:05.445255Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA().fit(Train_X_std)\n\nfig, ax = plt.subplots(figsize=(8,6))\nx_values = range(1, pca.n_components_+1)\nax.bar(x_values, pca.explained_variance_ratio_, lw=2, label='Explained Variance')\nax.plot(x_values, np.cumsum(pca.explained_variance_ratio_), lw=2, label='Cumulative Explained Variance', color='red')\nplt.plot([0,pca.n_components_+1],[0.9,0.9],'g--')\nax.set_title('Explained variance of components')\nax.set_xlabel('Principal Component')\nax.set_ylabel('Explained Variance')\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:07:05.447017Z","iopub.execute_input":"2022-04-18T07:07:05.448296Z","iopub.status.idle":"2022-04-18T07:07:05.761071Z","shell.execute_reply.started":"2022-04-18T07:07:05.448253Z","shell.execute_reply":"2022-04-18T07:07:05.759842Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nTrr=[]; Tss=[]; n=3\norder=['ord-'+str(i) for i in range(2,n)]\nTrd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\nTsd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\nm=df.shape[1]-1\n\nfor i in range(m):\n    pca = PCA(n_components=Train_X_std.shape[1]-i)\n    Train_X_std_pca = pca.fit_transform(Train_X_std)\n    Test_X_std_pca = pca.fit_transform(Test_X_std)\n    \n    LR = LinearRegression()\n    LR.fit(Train_X_std_pca, Train_Y)\n\n    pred1 = LR.predict(Train_X_std_pca)\n    pred2 = LR.predict(Test_X_std_pca)\n\n    Trr.append(round(np.sqrt(mean_squared_error(Train_Y, pred1)),2))\n    Tss.append(round(np.sqrt(mean_squared_error(Test_Y, pred2)),2))\n\n# plt.figure(figsize=[20,4.5])\n# plt.subplot(1,3,1)\n# sns.heatmap(Trd.loc[:6], cmap='BuGn', annot=True, vmin=0, vmax=Trd.max().max())\n# plt.title('Train RMSE')\n# plt.subplot(1,3,2)\n# sns.heatmap(Tsd.loc[:6], cmap='BuGn', annot=True, vmin=0, vmax=Trd.max().max()+10)\n# plt.title('Test RMSE')\n# plt.subplot(1,3,3)\n# sns.heatmap((Trd+Tsd).loc[:6], cmap='BuGn', annot=True, vmin=0, vmax=Trd.max().max()+25)\n# plt.title('Total RMSE')\n# plt.show()\n\nplt.plot(Trr, label='Train RMSE')\nplt.plot(Tss, label='Test RMSE')\n#plt.ylim([19.5,20.75])\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:07:05.762595Z","iopub.execute_input":"2022-04-18T07:07:05.762838Z","iopub.status.idle":"2022-04-18T07:07:08.296645Z","shell.execute_reply.started":"2022-04-18T07:07:05.762809Z","shell.execute_reply":"2022-04-18T07:07:08.295482Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# #Shortlisting the selected Features (with RFE)\n\nlm = LinearRegression()\nrfe = RFE(lm,n_features_to_select=Train_X_std.shape[1]-df.shape[1]+10)             # running RFE\nrfe = rfe.fit(Train_X_std, Train_Y)\n\nLR = LinearRegression()\nLR.fit(Train_X_std.loc[:,rfe.support_], Train_Y)\n\n#print(Train_X_std.loc[:,rfe.support_].columns)\n\npred1 = LR.predict(Train_X_std.loc[:,rfe.support_])\npred2 = LR.predict(Test_X_std.loc[:,rfe.support_])\n\nprint(np.sqrt(mean_squared_error(Train_Y, pred1)))\nprint(np.sqrt(mean_squared_error(Test_Y, pred2)))\n\n# Train_X_std = Train_X_std.loc[:,rfe.support_]\n# Test_X_std = Test_X_std.loc[:,rfe.support_]","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:07:08.298556Z","iopub.execute_input":"2022-04-18T07:07:08.298919Z","iopub.status.idle":"2022-04-18T07:07:08.452232Z","shell.execute_reply.started":"2022-04-18T07:07:08.298873Z","shell.execute_reply":"2022-04-18T07:07:08.450954Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#Let us first define a function to evaluate our models\n\nModel_Evaluation_Comparison_Matrix = pd.DataFrame(np.zeros([5,8]), columns=['Train-R2','Test-R2','Train-RSS','Test-RSS',\n                                                                            'Train-MSE','Test-MSE','Train-RMSE','Test-RMSE'])\nrc=np.random.choice(Train_X_std.loc[:,Train_X_std.nunique()>=50].columns.values,3,replace=False)\ndef Evaluate(n, pred1,pred2):\n    #Plotting predicted predicteds alongside the actual datapoints \n    plt.figure(figsize=[15,6])\n    for e,i in enumerate(rc):\n        plt.subplot(2,3,e+1)\n        plt.scatter(y=Train_Y, x=Train_X_std[i], label='Actual')\n        plt.scatter(y=pred1, x=Train_X_std[i], label='Prediction')\n        plt.legend()\n    plt.show()\n\n    #Evaluating the Multiple Linear Regression Model\n\n    print('\\n\\n{}Training Set Metrics{}'.format('-'*20, '-'*20))\n    print('\\nR2-Score on Training set --->',round(r2_score(Train_Y, pred1),20))\n    print('Residual Sum of Squares (RSS) on Training set  --->',round(np.sum(np.square(Train_Y-pred1)),20))\n    print('Mean Squared Error (MSE) on Training set       --->',round(mean_squared_error(Train_Y, pred1),20))\n    print('Root Mean Squared Error (RMSE) on Training set --->',round(np.sqrt(mean_squared_error(Train_Y, pred1)),20))\n\n    print('\\n{}Testing Set Metrics{}'.format('-'*20, '-'*20))\n    print('\\nR2-Score on Testing set --->',round(r2_score(Test_Y, pred2),20))\n    print('Residual Sum of Squares (RSS) on Training set  --->',round(np.sum(np.square(Test_Y-pred2)),20))\n    print('Mean Squared Error (MSE) on Training set       --->',round(mean_squared_error(Test_Y, pred2),20))\n    print('Root Mean Squared Error (RMSE) on Training set --->',round(np.sqrt(mean_squared_error(Test_Y, pred2)),20))\n    print('\\n{}Residual Plots{}'.format('-'*20, '-'*20))\n    \n    Model_Evaluation_Comparison_Matrix.loc[n,'Train-R2']  = round(r2_score(Train_Y, pred1),20)\n    Model_Evaluation_Comparison_Matrix.loc[n,'Test-R2']   = round(r2_score(Test_Y, pred2),20)\n    Model_Evaluation_Comparison_Matrix.loc[n,'Train-RSS'] = round(np.sum(np.square(Train_Y-pred1)),20)\n    Model_Evaluation_Comparison_Matrix.loc[n,'Test-RSS']  = round(np.sum(np.square(Test_Y-pred2)),20)\n    Model_Evaluation_Comparison_Matrix.loc[n,'Train-MSE'] = round(mean_squared_error(Train_Y, pred1),20)\n    Model_Evaluation_Comparison_Matrix.loc[n,'Test-MSE']  = round(mean_squared_error(Test_Y, pred2),20)\n    Model_Evaluation_Comparison_Matrix.loc[n,'Train-RMSE']= round(np.sqrt(mean_squared_error(Train_Y, pred1)),20)\n    Model_Evaluation_Comparison_Matrix.loc[n,'Test-RMSE'] = round(np.sqrt(mean_squared_error(Test_Y, pred2)),20)\n\n    # Plotting y_test and y_pred to understand the spread.\n    plt.figure(figsize=[15,4])\n\n    plt.subplot(1,2,1)\n    sns.distplot((Train_Y - pred1))\n    plt.title('Error Terms')          \n    plt.xlabel('Errors') \n\n    plt.subplot(1,2,2)\n    plt.scatter(Train_Y,pred1)\n    plt.plot([Train_Y.min(),Train_Y.max()],[Train_Y.min(),Train_Y.max()], 'r--')\n    plt.title('Test vs Prediction')         \n    plt.xlabel('y_test')                       \n    plt.ylabel('y_pred')                       \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:07:08.457742Z","iopub.execute_input":"2022-04-18T07:07:08.459926Z","iopub.status.idle":"2022-04-18T07:07:08.544978Z","shell.execute_reply.started":"2022-04-18T07:07:08.459861Z","shell.execute_reply":"2022-04-18T07:07:08.543848Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#Linear Regression\n\nMLR = LinearRegression().fit(Train_X_std,Train_Y)\npred1 = MLR.predict(Train_X_std)\npred2 = MLR.predict(Test_X_std)\n\nprint('{}{}\\033[1m Evaluating Multiple Linear Regression Model \\033[0m{}{}\\n'.format('<'*3,'-'*35 ,'-'*35,'>'*3))\nprint('The Coeffecient of the Regresion Model was found to be ',MLR.coef_)\nprint('The Intercept of the Regresion Model was found to be ',MLR.intercept_)\n\nEvaluate(0, pred1, pred2)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:07:08.547155Z","iopub.execute_input":"2022-04-18T07:07:08.547959Z","iopub.status.idle":"2022-04-18T07:07:11.464200Z","shell.execute_reply.started":"2022-04-18T07:07:08.547902Z","shell.execute_reply":"2022-04-18T07:07:11.463033Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#Creating a Ridge Regression model\n\nRLR = Ridge().fit(Train_X_std,Train_Y)\npred1 = RLR.predict(Train_X_std)\npred2 = RLR.predict(Test_X_std)\n\nprint('{}{}\\033[1m Evaluating Ridge Regression Model \\033[0m{}{}\\n'.format('<'*3,'-'*35 ,'-'*35,'>'*3))\nprint('The Coeffecient of the Regresion Model was found to be ',MLR.coef_)\nprint('The Intercept of the Regresion Model was found to be ',MLR.intercept_)\n\nEvaluate(1, pred1, pred2)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:07:11.465772Z","iopub.execute_input":"2022-04-18T07:07:11.466042Z","iopub.status.idle":"2022-04-18T07:07:14.261909Z","shell.execute_reply.started":"2022-04-18T07:07:11.466012Z","shell.execute_reply":"2022-04-18T07:07:14.260144Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#Creating a Ridge Regression model\n\nLLR = Lasso().fit(Train_X_std,Train_Y)\npred1 = LLR.predict(Train_X_std)\npred2 = LLR.predict(Test_X_std)\n\nprint('{}{}\\033[1m Evaluating Lasso Regression Model \\033[0m{}{}\\n'.format('<'*3,'-'*35 ,'-'*35,'>'*3))\nprint('The Coeffecient of the Regresion Model was found to be ',MLR.coef_)\nprint('The Intercept of the Regresion Model was found to be ',MLR.intercept_)\n\nEvaluate(2, pred1, pred2)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:07:14.263572Z","iopub.execute_input":"2022-04-18T07:07:14.264665Z","iopub.status.idle":"2022-04-18T07:07:17.064341Z","shell.execute_reply.started":"2022-04-18T07:07:14.264615Z","shell.execute_reply":"2022-04-18T07:07:17.063333Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#Creating a ElasticNet Regression model\n\nENR = ElasticNet().fit(Train_X_std,Train_Y)\npred1 = ENR.predict(Train_X_std)\npred2 = ENR.predict(Test_X_std)\n\nprint('{}{}\\033[1m Evaluating Elastic-Net Regression Model \\033[0m{}{}\\n'.format('<'*3,'-'*35 ,'-'*35,'>'*3))\nprint('The Coeffecient of the Regresion Model was found to be ',MLR.coef_)\nprint('The Intercept of the Regresion Model was found to be ',MLR.intercept_)\n\nEvaluate(3, pred1, pred2)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:07:17.065711Z","iopub.execute_input":"2022-04-18T07:07:17.067722Z","iopub.status.idle":"2022-04-18T07:07:19.955608Z","shell.execute_reply.started":"2022-04-18T07:07:17.067655Z","shell.execute_reply":"2022-04-18T07:07:19.954904Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#Checking polynomial regression performance on various degrees\n\nTrr=[]; Tss=[]\nn_degree=5\n\nfor i in range(2,n_degree):\n    #print(f'{i} Degree')\n    poly_reg = PolynomialFeatures(degree=i)\n    X_poly = poly_reg.fit_transform(Train_X_std)\n    X_poly1 = poly_reg.fit_transform(Test_X_std)\n    LR = LinearRegression()\n    LR.fit(X_poly, Train_Y)\n    \n    pred1 = LR.predict(X_poly)\n    Trr.append(np.sqrt(mean_squared_error(Train_Y, pred1)))\n    \n    pred2 = LR.predict(X_poly1)\n    Tss.append(np.sqrt(mean_squared_error(Test_Y, pred2)))\n\nplt.figure(figsize=[15,6])\nplt.subplot(1,2,1)\nplt.plot(range(2,n_degree),Trr, label='Training')\nplt.plot(range(2,n_degree),Tss, label='Testing')\n#plt.plot([1,4],[1,4],'b--')\nplt.title('Polynomial Regression Fit')\n#plt.ylim([0,5])\nplt.xlabel('Degree')\nplt.ylabel('RMSE')\nplt.grid()\nplt.legend()\n#plt.xticks()\n\nplt.subplot(1,2,2)\nplt.plot(range(2,n_degree),Trr, label='Training')\nplt.plot(range(2,n_degree),Tss, label='Testing')\nplt.title('Polynomial Regression Fit')\nplt.ylim([0,5e-12])\nplt.xlabel('Degree')\nplt.ylabel('RMSE')\nplt.grid()\nplt.legend()\n#plt.xticks()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:07:19.956964Z","iopub.execute_input":"2022-04-18T07:07:19.957297Z","iopub.status.idle":"2022-04-18T07:12:01.099245Z","shell.execute_reply.started":"2022-04-18T07:07:19.957267Z","shell.execute_reply":"2022-04-18T07:12:01.097814Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"#Using the 2nd Order Polynomial Regression model (degree=2)\n\npoly_reg = PolynomialFeatures(degree=2)\nX_poly = poly_reg.fit_transform(Train_X_std)\nX_poly1 = poly_reg.fit_transform(Test_X_std)\nPR = LinearRegression()\nPR.fit(X_poly, Train_Y)\n\npred1 = PR.predict(X_poly)\npred2 = PR.predict(X_poly1)\n\nprint('{}{}\\033[1m Evaluating Polynomial Regression Model \\033[0m{}{}\\n'.format('<'*3,'-'*35 ,'-'*35,'>'*3))\nprint('The Coeffecient of the Regresion Model was found to be ',MLR.coef_)\nprint('The Intercept of the Regresion Model was found to be ',MLR.intercept_)\n\nEvaluate(4, pred1, pred2)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:12:01.101098Z","iopub.execute_input":"2022-04-18T07:12:01.101374Z","iopub.status.idle":"2022-04-18T07:12:04.146171Z","shell.execute_reply.started":"2022-04-18T07:12:01.101342Z","shell.execute_reply":"2022-04-18T07:12:04.144941Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Regression Models Results Evaluation\n\nEMC = Model_Evaluation_Comparison_Matrix.copy()\nEMC.index = ['Multiple Linear Regression (MLR)','Ridge Linear Regression (RLR)','Lasso Linear Regression (LLR)','Elastic-Net Regression (ENR)','Polynomial Regression (PNR)']\nEMC","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:12:04.148255Z","iopub.execute_input":"2022-04-18T07:12:04.148906Z","iopub.status.idle":"2022-04-18T07:12:04.173206Z","shell.execute_reply.started":"2022-04-18T07:12:04.148589Z","shell.execute_reply":"2022-04-18T07:12:04.171931Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# R2-Scores Comparison for different Regression Models\n\nR2 = round(EMC['Train-R2'].sort_values(ascending=True),4)\nplt.hlines(y=R2.index, xmin=0, xmax=R2.values)\nplt.plot(R2.values, R2.index,'o')\nplt.title('R2-Scores Comparison for various Regression Models')\nplt.xlabel('R2-Score')\n#plt.ylabel('Regression Models')\nfor i, v in enumerate(R2):\n    plt.text(v+0.002, i-0.05, str(v*100), color='blue')\nplt.xlim([0,0.1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:12:04.174873Z","iopub.execute_input":"2022-04-18T07:12:04.175483Z","iopub.status.idle":"2022-04-18T07:12:04.355233Z","shell.execute_reply.started":"2022-04-18T07:12:04.175390Z","shell.execute_reply":"2022-04-18T07:12:04.354505Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Root Mean SquaredError Comparison for different Regression Models\n\ncc = Model_Evaluation_Comparison_Matrix.columns.values\ns=5\nplt.bar(np.arange(5), Model_Evaluation_Comparison_Matrix[cc[6]].values, width=0.3, label='RMSE (Training)')\nplt.bar(np.arange(5)+0.3, Model_Evaluation_Comparison_Matrix[cc[7]].values, width=0.3, label='RMSE (Testing)')\nplt.xticks(np.arange(5),EMC.index, rotation =35)\nplt.legend()\nplt.ylim([0,25])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T07:12:04.356427Z","iopub.execute_input":"2022-04-18T07:12:04.357160Z","iopub.status.idle":"2022-04-18T07:12:04.542521Z","shell.execute_reply.started":"2022-04-18T07:12:04.357120Z","shell.execute_reply":"2022-04-18T07:12:04.541796Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}