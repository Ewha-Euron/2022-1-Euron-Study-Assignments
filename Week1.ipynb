{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Prerequisites"
      ],
      "metadata": {
        "id": "Y7-r5yRv511V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0-1. Requirements\n",
        "\n",
        "* Ubuntu 18.04, Cuda 11\n",
        "* opencv-python\n",
        "* numpy\n",
        "* pandas\n",
        "* timm\n",
        "* torch==1.8.0 torchvision 0.9.0 with cuda 11.1\n",
        "* natsort\n",
        "* scikit-learn==1.0.0\n",
        "* pillow\n",
        "* torch_optimizer\n",
        "* tqdm\n",
        "* ptflops\n",
        "* easydict\n",
        "* matplotlib"
      ],
      "metadata": {
        "id": "FHEdTITG59OY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0-2 Directory 구조\n"
      ],
      "metadata": {
        "id": "xiJwxVNX6U2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q69rr5_E33d5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def list_files(startpath):\n",
        "  for root, dirs, files in os.walk(startpath):\n",
        "    level = root.replace(startpath, '').count(os.sep)\n",
        "    indent = ' ' * 4 * (level)\n",
        "    print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "    subindent = ' ' * 4 * (level + 1)\n",
        "    for f in files:\n",
        "      print('{}{}'.format(subindent, f))\n",
        "list_files('../')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0-3. Import Library"
      ],
      "metadata": {
        "id": "pjdjjLXH7Bb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ptflops\n",
        "!pip install timm\n",
        "!pip install torch_optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0xN34X9BRMb",
        "outputId": "4f8c7d83-0812-4a6e-bcc8-6221db2a211b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ptflops in /usr/local/lib/python3.7/dist-packages (0.6.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from ptflops) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->ptflops) (3.10.0.2)\n",
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 12.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.5)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n",
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 452 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer) (1.10.0+cu111)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->torch_optimizer) (3.10.0.2)\n",
            "Installing collected packages: pytorch-ranger, torch-optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import logging\n",
        "import easydict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from glob import glob # 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환\n",
        "from pathlib import Path\n",
        "from natsort import natsorted # 텍스트로 된 숫자(파일명) 정렬\n",
        "from os.path import join as opj\n",
        "from ptflops import get_model_complexity_info\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
        "from PIL import Image\n",
        "\n",
        "import timm # PyTorch Image Models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, grad_scaler\n",
        "from torchvision import transforms\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "9h3MiUks65sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0-4. Config\n"
      ],
      "metadata": {
        "id": "H9KsghxEA98k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 및 학습의 Hyper-parameter 정의 (사용자에 의해 세팅되는 값)"
      ],
      "metadata": {
        "id": "k83zbUegCv3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EasyDict는 자바스크립트와 같이 dict의 value에 속성으로 접근할 수 있다. \n",
        "args = easydict.EasyDict(\n",
        "    {'exp_num':'0',\n",
        "     'experiment':'Base',\n",
        "     'tag':'Default',\n",
        "\n",
        "     # Path settings\n",
        "     'data_path': '../data',\n",
        "     'fold':4, \n",
        "     'Kfold':5,\n",
        "     \"model_path\":'results/',\n",
        "\n",
        "     # Model parameter settings\n",
        "     \"encoder_name\":'regnety_040',\n",
        "     \"drop_path_rate\":0.2,\n",
        "\n",
        "     # Training parameter settings\n",
        "     ## Base Parameter\n",
        "     \"img_size\":288, \n",
        "     \"batch_size\":16, \n",
        "     \"epochs\":60,\n",
        "     \"optimizer\":\"Lamb\",\n",
        "     \"initial_lr\":5e-6,\n",
        "     \"weight_decay\":1e-3,\n",
        "\n",
        "     ## Augmentation\n",
        "     \"aug_ver\":2, \n",
        "     \"flipaug_ratio\":0.3, \n",
        "     \"margin\":50, \n",
        "     \"random_margin\":True, \n",
        "\n",
        "     ## Scheduler\n",
        "     \"scheduler\":\"cycle\",\n",
        "     \"warm_epoch\":5, \n",
        "     ### Cosine Annealing\n",
        "     \"min_lr\":5e-6,\n",
        "     \"tmax\":145, \n",
        "     ### OneycleLR\n",
        "     'max_lr':1e-3, \n",
        "\n",
        "     ## etc\n",
        "     'patience':50, \n",
        "     'clipping':None,\n",
        "\n",
        "     # Hardware settings\n",
        "     'amp':True, \n",
        "     'multi_gpu':False, \n",
        "     'logging':False, \n",
        "     'num_workers':4, \n",
        "     'seed':42\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "Avo7t6jMBAXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Make dataset\n"
      ],
      "metadata": {
        "id": "RsNpYp0fWMSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keypoint를 기준으로 이미지를 crop하기 위한 함수 정의 \n",
        "# train과 test 시 해당 함수가 적용된 crop 이미지가 Inputs으로 들어가게 된다. \n",
        "\n",
        "def crop_image(images, point, margin=100):\n",
        "  image = np.array(Image.open(images).convert('RGB')) # 이미지 객체를 RGB로 변환하기\n",
        "  point = point['data']\n",
        "  max_point = np.max(np.array(point), axis=0).astype(int)+margin \n",
        "  min_point = np.min(np.array(point), axis=1).astype(int)-margin\n",
        "  max_point = max_point[:-1] # remove Z order\n",
        "  min_point = min_point[:-1] # remove Z order\n",
        "\n",
        "  max_x, max_y = max_point\n",
        "  min_x, min_y = min_point\n",
        "  max_y += margin # 손목까지\n",
        "\n",
        "  # 데이터 포인트의 크기가 원 이미지를 넘어서는 경우를 방지\n",
        "  max_x = max_x if max_x < 1920 else 1920\n",
        "  max_y = max_y if max_y < 1080 else 1080\n",
        "  min_x = min_x if min_x > 0 else 0\n",
        "  min_y = min_y if min_y > 0 else 0\n",
        "\n",
        "  crop_image = image[min_y:max_y, min_x:max_x]\n",
        "\n",
        "  return crop_image"
      ],
      "metadata": {
        "id": "IOQ8SnHyFjBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader에서 사용할 dataframe 만들기\n",
        "train_path = '../data/train'\n",
        "train_folders = natsorted(glob(train_path + \"/*\"))\n",
        "\n",
        "answers=[]\n",
        "for train_folder in train_folders:\n",
        "  json_path = glob(train_folder+\"/*.json\")[0]\n",
        "  js = json.load(open(json_path))\n",
        "  cat = js.get(\"action\")[0]\n",
        "  cat_name = js.get('action')[1]\n",
        "\n",
        "  images_list= glob(train_folder +\"/*.png\")\n",
        "  for image_name in images_list:\n",
        "    answers.append([image_name, cat, cat_name])\n",
        "\n",
        "answers = pd.DataFrame(answers, columns = ['train_path','answer','answer_name'])\n",
        "answers.to_csv(\"../data/df_train.csv\", index=False)\n",
        "\n",
        "# 클래스가 1개뿐인 폴더들 Augmentation해서 이미지 생성 후, dataframe 재정의.\n",
        "# 새롭게 정의한 dataframe을 학습에 이용시 약간의 성능향상을 확인할 수 있음. \n",
        "seed = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "data_path = \"../data\"\n",
        "df_train = pd.read_csv(opj(data_path, 'df_train.csv'))\n",
        "df_info = pd.read_csv(opj(data_path, \"hand_geture_pose.csv\"))\n",
        "df_train =pd_train.merge(df_info[['pose_id', \"gesture_type\",\"hand_type\"]], \n",
        "                         how=\"left\", left_on=\"answer\", right_on =\"pose_id\")\n",
        "save_folder = \"train\"\n",
        "for i in range(649, 649+5):\n",
        "  if not os.path.exists(opj(data_path, save_folder, str(i))):\n",
        "    os.makedirs(opj(data_path, save_folder, str(i)))\n",
        "\n",
        "# flip aug 가능한 lable: 131, 47(one sample)\n",
        "oslabel_fliplabel = [(131,156), (47, 22)] # one sample label, flip label\n",
        "folders = ['649', '650'] # Train 648번 folder에 이은 number 생성\n",
        "\n",
        "# tqdm: 진행률 프로세스 바\n",
        "# zip은 순회가능 한 객체를 결합해 줌 ex. zip([1,2,3],['A','B','C']) => [(1,'A'),(2,'B'),(3,'C')]\n",
        "for label, folder in tqdm(zip(oslabel_fliplabel, folders)):\n",
        "  idx=0\n",
        "  os_label, f_label = label[0],label[1]\n",
        "  one_sample = df_train[df_train['answer'] == os_label].reset_index(drop=True)\n",
        "  temp = df_train[df_train['answer']== f_label].reset_index(drop=True)\n",
        "  train_folders = natsorted(temp['train_path'].apply(lambda x:x[:-6]).unique())\n",
        "\n",
        "  for train_folder in (train_folders):\n",
        "    json_path = glob(train_folder + \"/*.json\")[0]\n",
        "    js = json.load(open(json_path))\n",
        "    keypoints = js['annotations']\n",
        "    images_list = natsorted(glob(train_folder + \"/*.png\"))\n",
        "    for _, (point, image_name) in enumerate(zip(keypoints, images_list)):\n",
        "      croped_image = crop_image(image_name, point, margin= 50)\n",
        "      flip_img = cv2.flip(croped_image, 1)\n",
        "      save_path = opj(data_path, save_folder, folder, f'{idx}.png')\n",
        "      idx += 1\n",
        "      cv2.imwrite(save_path, flip_img)\n",
        "      df_train.loc[len(df_train)] = [save_path] + one_sample.iloc[0][1:].values.tolist()\n",
        "\n",
        "def rotation(img, angle):\n",
        "  angle = int(random.uniform(-angle, angle))\n",
        "  h, w = img.shape[:2]\n",
        "  M = cv2.getRotationMatrix2D((int(w/2), int(h/2)), angle, 1)\n",
        "  img = cv2.warpAffine(img, M, (w,h))\n",
        "  return img\n",
        "\n",
        "oslabel=[92, 188, 145]\n",
        "folder = ['651','652','653']\n",
        "for label, folder in tqdm(zip(oslabel, folder)):\n",
        "  idx=0\n",
        "  one_sample = df_train[df_train['answer']==label].reset_index(drop=True)\n",
        "  train_folders = natsorted(temp['train_path'].apply(lambda x:x[:-6]).unique())\n",
        "  for train_folder in (train_folders):\n",
        "    json_path = glob(train_folder+\"/*.json\")[0]\n",
        "    js =json.load(opne(json_path))\n",
        "    keypoints = js['annotations']\n",
        "    images_list = natsorted(glob(train_folder + \"/*.png\"))\n",
        "    for _, (point, image_name) in enumerate(zip(keypoints, images_list)):\n",
        "      croped_image = crop_image(image_names, point, margin =50)\n",
        "      aug_img = rotation(croped_image, 30)\n",
        "      save_path = opj(data_path, save_folder, folder, f'{idx}.png')\n",
        "      idx +=1\n",
        "      cv2.imwrite(save_path, aug_img)\n",
        "      df_train.loc[len(df_train)] = [save_path] + one_sample.iloc[0][1:].values.tolist()\n",
        "\n",
        "df_train.to_csv(\"../data/df_train_add.csv\", index=False)"
      ],
      "metadata": {
        "id": "GNOQOmKbaAnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset & Loader (Flip Augmentation & Crop using the keypoints & Remove noise keypoints)"
      ],
      "metadata": {
        "id": "uiq-3F6ClSl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "original image를 불러 다양한 Margin 값에 대해 crop 하는 방식으로 pytorch의 dataset을 구성함."
      ],
      "metadata": {
        "id": "Wto2y3DPlSjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train dataset에 475, 543 폴더는 의도하지 않은 나머지 손에 대해서도 Keypoint가 잡하게 됨. \n",
        "# Json의 Keypoint를 사용하기 위해 475, 543 폴더인 경우 해당 부분 Keypoint 제거\n",
        "\n",
        "def remove_keypoints(folder_num, points):\n",
        "  lst = []\n",
        "  for x,y,z in points:\n",
        "    cond1 = x < 250 and y > 800\n",
        "    cond2 = x > 1400 and y < 400\n",
        "    if not (cond1 or cond2):\n",
        "      lst.append([x,y,z])\n",
        "  return lst\n",
        "\n",
        "class Train_Dataset(Dataset):\n",
        "  def __init__(self, df, transform = None, df_flip_info=None, flipaug_ratio=0, label_encoder=None, margin=50, random_margin =True):\n",
        "    self.id = df['train_path'].values\n",
        "    self.target= df['answer'].values\n",
        "    self.transform = transform\n",
        "    self.margin = margin\n",
        "    self.random_margin = random_margin\n",
        "\n",
        "    # Flip Augmentation (Change target class)\n",
        "    if df_flip_info is not None:\n",
        "      self.use_flip = True\n",
        "      print(\"Use Flip Augmentation\")\n",
        "      left = label_encoder.transform(df_flip_info['left'])\n",
        "      right = label_encoder.transform(df_flip_info['right'])\n",
        "      left_to_right = dict(zip(left, right))\n",
        "      right_to_left = dict(zip(right, left))\n",
        "\n",
        "      self.flip_info = left_to_right.copy()\n",
        "      self.flip_info.update(right_to_left)\n",
        "      self.flip_possible_class  = list(set(np.concatenate([left, right])))\n",
        "    self.flipaug_ratio = flipaug_ratio\n",
        "\n",
        "    print(f'Dataset size:{len(self.id)}')\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = np.array(Image.open(self.id[idx]).convert(\"RGB\"))\n",
        "    target = self.targe[idx]\n",
        "\n",
        "    # Load Json File\n",
        "    try:\n",
        "      image_num = int(Path(self.id[idx]).stem)\n",
        "      dir = os.path.dirname(self.id[idx])\n",
        "      folder_num = os.path.basename(dir)\n",
        "      json_path = opj(dir, folder_num + \".json\")\n",
        "      js = json.load(open(json_path))\n",
        "      keypoints = js['annotations'][image_num]['data'] # 해당 이미지에 해당하는 Keypoints\n",
        "    except: # Augmentation으로 직접 새로 만든 foldersms Json이 없으므로 바로 return \n",
        "      image = self.transform(Image.fromarray(image))\n",
        "      return image, np.array(target)\n",
        "\n",
        "    if folder_num in ['475', '543']:\n",
        "      keypoints = remove_keypoints(folder_num, keypoints)\n",
        "\n",
        "    # Image Crop using keypoints\n",
        "    max_point = np.max(np.array(keypoints), axis=0).astype(int) + self.margin\n",
        "    min_point = np.min(np.array(keypoints), axis=0).astype(int) - self.margin\n",
        "    max_point = max_point[:-1] #remove Z order\n",
        "    min_point = min_point[:-1] #remove Z order\n",
        "\n",
        "    max_x, max_y = max_point\n",
        "    min_x, min_y = min_point\n",
        "    max_y += 100 # 손목부분까지 여유\n",
        "\n",
        "    # 매 epoch마다 margin이 조금씩 다르게 들어가므로 한 폴더 내 비슷한 이미지들의 overfitting을 방지 (only train phase)\n",
        "    if self.random_margin:\n",
        "      if random.random() < 0.5:\n",
        "        max_x += self.margin\n",
        "      if rnadom.random() < 0.5:\n",
        "        max_y += self.margin\n",
        "      if random.random() < 0.5:\n",
        "        min_x -= self.margin\n",
        "      if random.random() < 0.5:\n",
        "        min_y -= self.margin\n",
        "    else:\n",
        "      max_x += self.margin\n",
        "      max_y += self.margin\n",
        "      min_x -= self.margin\n",
        "      min_y -= self.margin\n",
        "\n",
        "    #데이터 포인트의 크기가 원 이미지를 넘어서는 경우 방지\n",
        "    max_x = max_x if max_x < 1920 else 1920\n",
        "    max_y = max_y if max_y < 1080 else 1080\n",
        "    min_x = min_x if min_x > 0 else 0\n",
        "    min_y = min_y if min_y > 0 else 0\n",
        "\n",
        "    image = image[min_y:max_y, min_x:max_x]\n",
        "\n",
        "    # FlipAug\n",
        "    if(random.random() < self.flipaug_ratio) and (target in self.flip_possible_class):\n",
        "      image = np.flip(image, axis=1) # (H, W, C)에서 width 축 flip\n",
        "      target = self.flip_info[target]\n",
        "    image = self.transform(Image.fromarray(image))\n",
        "    return image, np.array(target)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.id)\n",
        "  \n",
        "  \n",
        "def get_loader(df, batch_size, shuffle, num_workers, transform , df_flip_info = None,\n",
        "               flipaug_ratio=0, label_encoder = None, margin = 50, random_margin =True):\n",
        "  dataset = Train_Dataset(df, transform, df_flip_info=df_flip_info, flipaug_ratio=flipaug_ratio, \n",
        "                            label_encoder=label_encoder, margin=margin, random_margin=random_margin)\n",
        "  data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True,\n",
        "                                drop_last=False)\n",
        "  return data_loader\n",
        "\n",
        "\n",
        "def get_train_augmentation(img_size, ver):\n",
        "  if ver==1:\n",
        "    # For Test\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "  if ver==2:\n",
        "    # For Train\n",
        "    transform = transforms.Compose([\n",
        "            transforms.RandomAffine(20),\n",
        "            transforms.RandomPerspective(),\n",
        "            transforms.ToTensor(),\n",
        "          transforms.Resize((img_size, img_size)),\n",
        "          transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "return transform"
      ],
      "metadata": {
        "id": "LxNQILI6k0CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flip Augmentation\n",
        "\n",
        "* 중복된 데이터가 많아 오버피팅이 빠르게 발생하여 적절한 Augmentation 수행 필요\n",
        "* 평소 이미지 Task에서 흔하게 사용되는 Augmentationdls Horizontal Flip은 사용할 수 없었음. (동일한 포즈라도 왼손, 오른손 Class가 다르기 때문)\n",
        "* 동일한 포즈일 때, 이미지는 Horizontal Flip을 시키고 Class 도 바꾸어주면 충분히 학습에 사용할 수 있을 것이라고 판단.\n",
        "* 예로, My View, 왼손, 숫자 1일 때 HFlip을 수행하여 My View, 오른손, 숫자 1 클래스를 생성\n",
        "* 이를 위해 미리 아래와 같으 mapping dataFrame 을 만들어 주었고 pytorch dataset안에 이식하였음\n",
        "* Flip augmentation을 수행하는 비율을 0.1~0.5로 다양하게 해보았을 때 0.3이 가장 적절하였음.\n"
      ],
      "metadata": {
        "id": "OpibSojHTdwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network\n"
      ],
      "metadata": {
        "id": "Vz31cFllUnSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pytorch image model(timm) 라이브러리를 활용하여 generalization performance에 강점을 가지는 RegNet 을 Base 모델로 사용"
      ],
      "metadata": {
        "id": "gdZJ4tqpUsRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pose_Network(nn.Module):\n",
        "  def __init__(self, args):\n",
        "    super().__init__()\n",
        "    self.encoder = timm.create_model(args.encoder_name, pretrained=True,\n",
        "                                     drop_path_rate = args.drop_path_rate)\n",
        "    num_head = self.encoder.head.fc.in_features\n",
        "    self.encoder.head.fc = nn.Linear(num_head, 157)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.encoder(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "0Ba8VKCMUmOR",
        "outputId": "34bd89f5-0bbb-4ce7-a52d-14feb5c46efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b80dc8ceedeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPose_Network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     self.encoder = timm.create_model(args.encoder_name, pretrained=True,\n\u001b[1;32m      5\u001b[0m                                      drop_path_rate = args.drop_path_rate)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils for training and Logging"
      ],
      "metadata": {
        "id": "rhv2eQmbVQfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "logging과 avgmeter를 이용해 실험 기록을 log파일로 남도록 저장한다. "
      ],
      "metadata": {
        "id": "5YtVpVP9VVVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* leargning rate scheduler\n",
        "* 학습과정에서 lr을 조정함."
      ],
      "metadata": {
        "id": "bucJzXhFV4BM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# warmup learning rate scheduler\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "class WarnUpLR(_LRScheduler):\n",
        "  \"\"\"warmup_training learning rate scheduler\n",
        "  Args:\n",
        "    optimizer: optimizer(e.g.SGD)\n",
        "    total_iters: totla_iters of warmup phase\n",
        "  \"\"\"\n",
        "  def __init__(self, optimizer,total_iters, last_epoch=-1):\n",
        "    self.total_iters = total_iters\n",
        "    super().__init__(optimizer, last_epoch)\n",
        "\n",
        "  def get_lr(self):\n",
        "    \"\"\" we will use the first m batches, and set the learning rate to base_lr * m/ total_iters\"\"\"\n",
        "    return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
        "\n",
        "#Logging\n",
        "def get_root_logger(logger_name=\"basicsr\", log_level= logging.INFO, log_file =None):\n",
        "  logger = logging.getLogger(logger_name)\n",
        "    # if the logger has been initialized, just return it\n",
        "  if logger.hasHandlers():\n",
        "      return logger\n",
        "\n",
        "  format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
        "  logging.basicConfig(format=format_str, level=log_level)\n",
        "\n",
        "  if log_file is not None:\n",
        "      file_handler = logging.FileHandler(log_file, 'w')\n",
        "      file_handler.setFormatter(logging.Formatter(format_str))\n",
        "      file_handler.setLevel(log_level)\n",
        "      logger.addHandler(file_handler)\n",
        "\n",
        "  return logger\n",
        "\n",
        "class AvgMeter(object):\n",
        "  def __init__(self):\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.val = 0\n",
        "    self.avg = 0\n",
        "    self.sum = 0\n",
        "    self.count = 0\n",
        "    self.losses = []\n",
        "\n",
        "  def update(self, val, n=1):\n",
        "    self.val = val\n",
        "    self.sum += val * n\n",
        "    self.count += n\n",
        "    self.avg = self.sum / self.count\n",
        "    self.losses.append(val)"
      ],
      "metadata": {
        "id": "MJtG8__tVUBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer\n",
        "모델의 학습(training function)과 검증(ValidatioN)을 위한 Class이다. "
      ],
      "metadata": {
        "id": "N13yF8vfXenH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "  def __init__(self, args, save_path):\n",
        "    \"\"\"\n",
        "    args: arguments\n",
        "    save_path: Model 가중치 저장 경로 \n",
        "    \"\"\"\n",
        "    super(Trainer, self).__init__()\n",
        "    self.device = torch.device('cuda'if torch.cuda.is_available()else\"cpu\")\n",
        "\n",
        "    #logging\n",
        "    log_file = os.path.join(save_path, \"log.log\")\n",
        "    self.logger = get_root_logger(logger_name=\"IR\", log_level=loggin.INFO, log_file=log_file)\n",
        "    self.logger.info(args)\n",
        "    self.logger.info(args.tag)\n",
        "\n",
        "    # Train, Valid Set load\n",
        "    ################\n",
        "    df_train = pd.read_csv(opj(args.data_path, 'df_tain_add.csv'))\n",
        "    df_info = pd.read_csv(opj(args.data_path, 'hadn_gesture_pose.csv'))\n",
        "    df_train = df_train.merge(df_info[['pose_id', 'gesture_type', 'hand_type']], \\\n",
        "                                how='left', left_on='answer', right_on='pose_id')\n",
        "\n",
        "    # 폴더별(Group)로 각 번호 부여\n",
        "    df_train['groups'] = df_train['train_path'].apply(lambda x:x.split('/')[3])\n",
        "    df_train.loc[:,:] = natsorted(df_train.values)\n",
        "    # 노이즈 이미지 제거: 596번은 주먹쥐기 이미지인데 갑자기 손바닥을 펴는 노이즈 이미지가 5장있음 + 0번 폴더에 9번 이미지 역시 잘못된 클래스\n",
        "    drop_idx = df_train[df_train['groups'].isin(['596'])].index.tolist()[3:8] + [9]  \n",
        "    df_train = df_train.drop(drop_idx).reset_index(drop=True)  \n",
        "    le = LabelEncoder()\n",
        "    df_train['answer'] = le.fit_transform(df_train['answer'])\n",
        "    \n",
        "    # Split Fold\n",
        "    # kf = StratifiedGroupKFold(n_splits=args.Kfold)\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(df_train, y=df_train['answer'])):\n",
        "        df_train.loc[val_idx, 'fold'] = fold\n",
        "    df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n",
        "    df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n",
        "    \n",
        "    # Augmentation\n",
        "    self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
        "    self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
        "    \n",
        "    ######################################################################\n",
        "    # Flip Augmentation을 위한 Mapping dataframe\n",
        "    df_info = pd.read_csv('../data/hand_gesture_pose.csv')\n",
        "    df_info = df_info[df_info['hand_type'] != 'both']\n",
        "    # drop idx, 동일한 약속, gesture_type, hand_type인데 다른 클래스인 경우 존재 -> 약속 1과 2로 이름을 나누어줌.\n",
        "    df_info.loc[[105, 128], 'pose_name'] = '약속 1'  # idx: (105, 128)\n",
        "    df_info.loc[[101, 124], 'pose_name'] = '약속 2'  # idx: (101, 124)\n",
        "\n",
        "    # drop 41 idx, 동일한 약속, my hand, right class가 49와 54로 두 개있어 Mapping df만들 때 문제가 발생하여 미리 49번 클래스 처리\n",
        "    df_info = df_info.drop(41)\n",
        "\n",
        "    # Make a mapping dataframe\n",
        "    df_info = df_info.groupby(['pose_name', 'view_type', 'gesture_type', 'hand_type']).sum().unstack().reset_index().dropna(axis=0)\n",
        "    df_info['left'] = df_info.pose_id.left.apply(int)\n",
        "    df_info['right'] = df_info.pose_id.right.apply(int)\n",
        "    df_flip_info = df_info.drop('pose_id', axis=1).droplevel('hand_type', axis=1).reset_index(drop=True)\n",
        "    print('Mapping dataframe Length', df_flip_info.shape)\n",
        "    ######################################################################\n",
        "    \n",
        "    # TrainLoader\n",
        "    self.train_loader = get_loader(df_train, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, transform=self.train_transform, \n",
        "                                    df_flip_info=df_flip_info, flipaug_ratio=args.flipaug_ratio, label_encoder=le, margin=args.margin, random_margin=args.random_margin)\n",
        "    self.val_loader = get_loader(df_val, batch_size=args.batch_size, shuffle=False,\n",
        "                                    num_workers=args.num_workers, transform=self.test_transform)\n",
        "\n",
        "    # Network\n",
        "    self.model = Pose_Network(args).to(self.device)\n",
        "    macs, params = get_model_complexity_info(self.model, (3, args.img_size, args.img_size), as_strings=True,\n",
        "                                              print_per_layer_stat=False, verbose=False)\n",
        "    self.logger.info('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
        "    self.logger.info('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
        "\n",
        "    # Loss\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Optimizer & Scheduler\n",
        "    self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr, weight_decay=args.weight_decay)\n",
        "    \n",
        "    iter_per_epoch = len(self.train_loader)\n",
        "    self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n",
        "\n",
        "    if args.scheduler == 'cos':\n",
        "        tmax = args.tmax # half-cycle \n",
        "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max = tmax, eta_min=args.min_lr, verbose=True)\n",
        "    elif args.scheduler == 'cycle':\n",
        "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
        "\n",
        "    \n",
        "    if args.multi_gpu:\n",
        "        self.model = nn.DataParallel(self.model).to(self.device)\n",
        "\n",
        "    # Train / Validate\n",
        "    best_loss = np.inf\n",
        "    best_acc = 0\n",
        "    best_epoch = 0\n",
        "    early_stopping = 0\n",
        "    start = time.time()\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "      self.epoch = epoch\n",
        "\n",
        "      if args.scheduler == 'cos':\n",
        "        if epoch > args.warm_epoch:\n",
        "          self.scheduler.step()\n",
        "\n",
        "      # Training\n",
        "      train_loss, train_acc = self.training(args)\n",
        "\n",
        "      # Model weight in Multi_GPU or Single GPU\n",
        "      state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
        "\n",
        "      # Validation\n",
        "      val_loss, val_acc = self.validate()\n",
        "\n",
        "      # Save models\n",
        "      if val_loss < best_loss:\n",
        "          early_stopping = 0\n",
        "          best_epoch = epoch\n",
        "          best_loss = val_loss\n",
        "          best_acc = val_acc\n",
        "\n",
        "          torch.save({'epoch':epoch,\n",
        "                      'state_dict':state_dict,\n",
        "                      'optimizer': self.optimizer.state_dict(),\n",
        "                      'scheduler': self.scheduler.state_dict(),\n",
        "              }, os.path.join(save_path, 'best_model.pth'))\n",
        "          self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n",
        "      else:\n",
        "          early_stopping += 1\n",
        "\n",
        "      # Early Stopping\n",
        "      if early_stopping == args.patience:\n",
        "          break\n",
        "\n",
        "    self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f}')\n",
        "    end = time.time()\n",
        "    self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n",
        "\n",
        "\n",
        "  # Training\n",
        "  def training(self, args):\n",
        "    self.model.train()\n",
        "    train_loss = AvgMeter()\n",
        "    train_acc = 0\n",
        "\n",
        "    scaler = grad_scaler.GradScaler()\n",
        "    for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
        "      images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
        "      targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
        "      \n",
        "      if self.epoch <= args.warm_epoch:\n",
        "          self.warmup_scheduler.step()\n",
        "\n",
        "      self.model.zero_grad(set_to_none=True)\n",
        "      if args.amp:\n",
        "          with autocast():\n",
        "              preds = self.model(images)\n",
        "              loss = self.criterion(preds, targets)\n",
        "          scaler.scale(loss).backward()\n",
        "\n",
        "          # Gradient Clipping\n",
        "          if args.clipping is not None:\n",
        "              scaler.unscale_(self.optimizer)\n",
        "              nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
        "\n",
        "          scaler.step(self.optimizer)\n",
        "          scaler.update()\n",
        "\n",
        "      else:\n",
        "        preds = self.model(images)\n",
        "        loss = self.criterion(preds, targets)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
        "        self.optimizer.step()\n",
        "\n",
        "      if args.scheduler == 'cycle':\n",
        "        if self.epoch > args.warm_epoch:\n",
        "          self.scheduler.step()\n",
        "\n",
        "      # Metric\n",
        "      train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
        "      # log\n",
        "      train_loss.update(loss.item(), n=images.size(0))\n",
        "        \n",
        "    train_acc /= len(self.train_loader.dataset)\n",
        "\n",
        "    self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
        "    self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f}')\n",
        "    return train_loss.avg, train_acc\n",
        "        \n",
        "  # Validation or Dev\n",
        "  def validate(self):\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "      val_loss = AvgMeter()\n",
        "      val_acc = 0\n",
        "\n",
        "      for _, (images, targets) in enumerate(self.val_loader):\n",
        "        images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
        "        targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
        "\n",
        "        preds = self.model(images)\n",
        "        loss = self.criterion(preds, targets)\n",
        "\n",
        "        # Metric\n",
        "        val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
        "        # log\n",
        "        val_loss.update(loss.item(), n=images.size(0))\n",
        "      val_acc /= len(self.val_loader.dataset)\n",
        "      self.logger.info(f'Valid Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f}')\n",
        "    return val_loss.avg, val_acc"
      ],
      "metadata": {
        "id": "kKtV2cQJXXVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stratified GroupKFold Split\n",
        "* 데이터셋을 보면 총 이미지는 5888개 이지만, 한 폴더 내의 이미지가 대부분 비슷한 것을 확인할 수 있다. \n",
        "\n",
        "* 이러한 데이터를 일반적인 split방법으로 나눌 경우 validation accuracy가 거의 1에 가깝게 나오게 되어, 어떤 class에 대한 적중률이 떨어지는 지에 대한 분석을 진행할 수 없다. \n",
        "\n",
        "* 따라서, 각 폴더별로 Group을 부여하고 해당 Group 전체가 Train or Valid로 들어가게 하는 GroupKFold를 사용하고, 추가적으로 Class Label 분포를 고려하기 위해 Stratified KFold를 함께 적용한다.\n",
        "\n",
        "* 단일 모델로 public 기준 0.024의 스코어를 얻음\n",
        "\n",
        "* 그 이후 Fold를 다르게 학습한 경우 Public score가 0.15~0.20 정도로 매우 극심한 차이를 보임\n",
        "\n",
        "* 확인해보니 전체 데이터 중 특정 클래스가 1,2개 폴더만 존재하는 클래스가 있었음. groupkfold로 split시에 특정 클래스는 train에만, 또는 Valid에만 들어가는 것을 확인. 이는 특정 클래스에 대해 검증을 수행할 수 없게 되는 문제가 있음. \n",
        "\n",
        "* 이를 방지하기 위해 make dataset section에서 볼 수 있듯, 폴더가 1개인 클래스를 폴더가 2개 이상이 되도록 만들어 줌. 이 때 단순히 augmentation을 사용한 것이 아니라 flip augmentation(+rotation)을 수행하였음. "
      ],
      "metadata": {
        "id": "dYn4INBDMEpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation prediction의 분포 확인\n",
        "* 위 방법을 통해 적절한 train/ valid split을 수행한 후에도 여전히 fold별 편차가 심한 것을 확인. \n",
        "* -> 모델이 어떤 클래스를 못 맞추었는지에 대한 분석 진행\n",
        "* 분석 결과 특정 몇 개 클래스에서 logloss 값이 매우 크게 발생하는 것을 확인하였고, insight를 얻기 위해 해당 클래스들의 이미지를 살펴봄."
      ],
      "metadata": {
        "id": "gwb4ERb_Nk8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Case1\n",
        "# img = Image.open(\"./etc/숫자1_검지흔들기.png\")\n",
        "# plt.figure(figsize=(10,5))\n",
        "# plt.imshow(img)"
      ],
      "metadata": {
        "id": "0dQtX11cN-P5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Case2\n",
        "# img = Image.open(\"./etc/주먹내밀기_주먹쥐기.png\")\n",
        "# plt.figrue(figsize=(10,5))\n",
        "# plt.imshow(img)"
      ],
      "metadata": {
        "id": "TGAdMDtBOJe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시각화를 위한 function\n",
        "def visualize(folder_num):\n",
        "  path = f'../data/train/{folder_num}/*.png'\n",
        "  image_list = glob(path)\n",
        "  length = len(image_list)\n",
        "\n",
        "  fig, ax = plt.subplots(1,length, figsize=(50,10))\n",
        "  for i, image in enumerate(image_list):\n",
        "    image = Image.open(image).convert('RGB')\n",
        "    ax[i].imshow(image)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "TjdylTokOaeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df =pd.read_csv(opj(args.data_path, \"df_train.csv\"))\n",
        "df['groups'] = df['train_path'].apply(lamda x:x.split(\"/\")[3])\n",
        "df = df.drop_duplicates(\"groups\")"
      ],
      "metadata": {
        "id": "4tUykr92OacL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Case 1) 첫 번째는 숫자1과 부정(검지 흔들기) 대해 큰 Logloss값이 발생.\n",
        "두 이미지는 각각 숫자1과 부정(검지 흔들기) Class임.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iqR5HqZNPjL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = f'../data/train/'\n",
        "number1_folder = df[df['answer_name'] == '숫자1']['groups'].tolist()\n",
        "shake_folder = df[df['answer_name'] == '부정(검지 흔들기)']['groups'].tolist()\n",
        "\n",
        "image1 = Image.open(opj(path, number1_folder[0], '1.png'))   # 352번 폴더\n",
        "image2 = Image.open(opj(path, shake_folder[11], '1.png'))    # 489번 폴더\n",
        "print(number1_folder[0], shake_folder[11])\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
        "ax[0].imshow(image1)\n",
        "ax[1].imshow(image2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U5lDWbWFOaaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "당 두 개의 클래스는 사람이 봐도 구분이 힘듦.\n",
        "마찬가지로 사람도 구분 할 수 없는 이미지를 모델이 각각 다른 Class로 학습을 하게되어, 모델이 해당 클래스 샘플들에 대해 매우 헷갈려하여 Logloss 값이 크게 나오는 것을 확인.\n"
      ],
      "metadata": {
        "id": "ZX3yZ7aBPyDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Case 2) 두 번째는 주먹쥐기와 경고(주먹 내밀기) 대해서도 큰 Logloss값이 발생. \n",
        "\n",
        "주먹쥐기와 경고(주먹 내밀기) Class.\n"
      ],
      "metadata": {
        "id": "7Ojh2mUWQhCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = f'../data/train/'\n",
        "number1_folder = df[df['answer_name'] == '주먹쥐기']['groups'].tolist()\n",
        "shake_folder = df[df['answer_name'] == '경고(주먹 내밀기)']['groups'].tolist()\n",
        "\n",
        "image1 = Image.open(opj(path, number1_folder[1], '1.png'))   \n",
        "image2 = Image.open(opj(path, shake_folder[9], '1.png'))    \n",
        "print(number1_folder[1], shake_folder[9])\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
        "ax[0].imshow(image1)\n",
        "ax[1].imshow(image2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xWI111leQAbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* (숫자1, 검지흔들기)pair와 마찬가지로 단순히 두 이미지만 봤을때는 어떤 클래스인지 판단하기가 어려움.\n",
        "* 주먹쥐기의 경우에는 계속 같은 위치에서 크게 움직임이 없지만 주먹 내밀기의 경우에는 주먹이 카메라에 가까워졌다 멀어졌다 하는식으로 촬영된 것을 확인.\n",
        "* 위의 2가지 Case를 해결하기 위한 2가지 방법.\n",
        "  1.  폴더 내 이미지들의 Sequence를 살려서, 한 번의 입력에 여러 장의 이미지가 들어가 모델이 학습하도록 하는 방법\n",
        "    *   위 방법은 (a)시퀀스를 어느 정도의 길이로 만들어주느냐, (b)각 폴더 내에서 시퀀스 Pair를 얼마나 잘 만들어주느냐(변화가 있는 Class의 경우) 같은 Pre-processing에 영향을 크게 받을 것으로 추측.\n",
        "  2. Keypoint를 사용한 Rule에 기반한 접근 방법\n",
        "    * 본 대회는 Train과 Test모두 입력으로 이미지뿐만 아니라 Keypoint를 사용할 수 있음. 따라서, 기존에 학습했던 것처럼 모델의 학습과 추론을 수행함.\n",
        "    * 다만, 추론 시에 (숫자1, 검지 흔들기), (주먹쥐기, 주먹 내밀기) 두 Case 중에 하나의 Class를 예측하면 Keypoint를 사용한 Rule로 처리하는 방법임.\n",
        "    * 예를 들어, 모델이 숫자1(왼손, My View) 또는 검지 흔들기(왼손, My View)로 예측하였으면, Keypoint에 기반한 알고리즘으로 넘어가게되고 해당 알고리즘을 거친 예측 정답이 나오게 됨.\n",
        "    * 추가적으로, 해당 Task는 같은 숫자1이라도 왼손인지 오른손인지, 또는 My View인지 Your View에 따라 Class가 다름.\n",
        "    * 모델이 왼손인지 오른손인지, 또는 My View인지 Your View인지는 맞추었을 것이라 판단하였고, (숫자1(왼손, My View) - 검지 흔들기(왼손, My View))과 같이 좌우, View타입은 동일하게 매핑시킴.\n",
        "\n"
      ],
      "metadata": {
        "id": "thvy7XGrQ1ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-Processing in detail (Method)\n"
      ],
      "metadata": {
        "id": "BDgYFFaQSo34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case1\n",
        "숫자1과 부정(검지 흔들기)의 경우, 한 폴더 내에서 숫자1은 검지의 움직임이 크지 않지만 검지 흔들기는 검지를 흔들어야하기 때문에 검지 Keypoint의 X좌표로의 움직임이 상대적으로 클 수 밖에 없음.\n",
        "따라서, 손가락(검지)의 가장 위에 있는(0,0픽셀을 기준으로 Y값이 가장 작은) keypoint의 움직임을 통해 구별하고자 함.\n",
        "예를들어, 폴더내 이미지가 10개가 있다면 10개의 검지 Keypoint 좌표가 있게 되고 해당 x좌표들에 대해서 max-min을 계산하게 된다. 즉, 각 폴더내에서 1개의 x좌표 변화량이 계산 됨.\n",
        "두 클래스에 대해 계산된 검지 x좌표 변화량들에 대해서 적절한 threshold값을 설정해 클래스를 구분짓게 함.\n",
        "\n",
        "Case2\n",
        "주먹 쥐기와 주먹 내밀기 같은 경우 손의 움직임을 통해 구별하고자 \u001d함.\n",
        "이는 단순하게 keypoint중 가장 오른쪽 x좌표의 변화량으로 계산함.\n",
        "Case1과 마찬가지로 keypoint의 x좌표 max-min을 계산.\n",
        "두 클래스에 대해 계산된 x좌표 변화량들에 대해서 적절한 threshold값을 설정해 두 클래스를 구분짓게 함."
      ],
      "metadata": {
        "id": "h4QV-k4-VyVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# threshold를 계산하기 위해 각 case에 대한 변화량을 계산하는 함수 정의\n",
        "def check_stats(find_list, ver):\n",
        "  train_path = '../data/train'\n",
        "  train_folders = natsorted(glob(train_path + '/*'))\n",
        "  stat_list = []\n",
        "  for _, train_folder in tqdm(enumerate(train_folders)):\n",
        "    try:\n",
        "      json_path = glob(train_folder + '/*.json')[0]\n",
        "      js = json.load(open(json_path))\n",
        "      cat = js.get('action')[0]\n",
        "      keypoints = js['annotations']\n",
        "      keypoints = np.array([point['data'] for point in keypoints])  # (N-이미지개수, 21 or 42(keypoints), 3(x,y,z 좌표))\n",
        "    except:\n",
        "      pass\n",
        "    if cat in find_list:\n",
        "      # 숫자1과 검지흔들기 구분 # case1\n",
        "      # 검지는 이미지내 keypoints들 중 가장 작은 y값(이미지 상 가장 높은 위치)을 갖는 point임. \n",
        "      # 해당 point의 x값을 뽑음.\n",
        "      if ver ==1 : \n",
        "        keypoints = keypoints[:, :, :2]  # keypoints : (N, 21 or 42, 2)\n",
        "        x_I_finger = [point_per_img[:,0][point_per_img[:,1].argmin()] for point_per_img in keypoints] # (N, 2) Y축으로 가장 작은 포인트 두개 추출\n",
        "        stat_list.append(np.max(x_I_finger) - np.min(x_I_finger))\n",
        "      \n",
        "      # 주먹쥐기와 주먹 내밀기(경고) # Case2\n",
        "      # keypoints들 중 가장 큰 x값(이미지 상 가장 우측 위치)을 갖는 point임.\n",
        "      # Case2같은 경우는 left 손목이 없기때문에 해당 logic이 잘 작동함.\n",
        "      elif ver == 2:\n",
        "        keypoints = keypoints[:, :, 0]   \n",
        "        x_values = [point_per_img[point_per_img.argmax()] for point_per_img in keypoints]  # 가장 오른쪽\n",
        "        stat_list.append(np.max(x_values) - np.min(x_values))\n",
        "\n",
        "  print(stat_list)\n",
        "  return stat_list\n",
        "\n",
        "##label##\n",
        "######### ver1 ############\n",
        "find_list0 = [0, 10, 100, 110] # ['숫자 1', '숫자1']  my hand, your hand 좌우\n",
        "find_list1= [42, 67, 142, 167] # ['부정(검지 흔들기)'] my hand, your hand 좌우\n",
        "\n",
        "##########ver2 ############\n",
        "find_list2 = [146] # ['주먹쥐기']  Your hand 우\n",
        "find_list3 = [163] # ['경고(주먹 내밀기)'] Your hand 우\n",
        "\n",
        "find_list4 = [171] # ['주먹쥐기']  Your hand Both\n",
        "find_list5 = [191] # ['경고(주먹 내밀기)'] Your hand Both"
      ],
      "metadata": {
        "id": "YzdyXIxpRMLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case1\n",
        "* Train dataset 내 두 클래스간 검지 위치 변화량 차이가 분명함을 확인.\n",
        "* 부정(검지 흔들기)클래스의 특정 폴더에서 9.58, 20.37과 같은 작은 값들도 존재하였는데, 해당 폴더를 살펴보면 손가락의 움직임이 거의 없어 숫자 1이라고 판단해도 될만한 노이즈인 경우가 있었음.\n",
        "* 따라서, 해당 threshold로는 완벽한 구분이 불가능 할 수 있지만, 두 Outlier를 제외하면 움직임 값이 매우 크기 때문에 숫자1의 움직임들 중 Max값인 26.85에  Margin(5)을 \b적용시킴."
      ],
      "metadata": {
        "id": "3AI3bxTrVsUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 숫자1 & 검지 흔들기\n",
        "li0 = check_stats(find_list0,1) #숫자1 or 숫자 1\n",
        "li1 = check_stats(find_list1,1) #부정(검지 흔들기)\n",
        "threshold_ver1 = max(li0) + 5   # Margin 5\n",
        "print(f'\\n{threshold_ver1:.3f}보다 크면 부정(검지 흔들기) 클래스')"
      ],
      "metadata": {
        "id": "RXfp-U-QRMJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case2\n",
        "* 두 클래스간 가장 큰 x좌표의 위치 변화량 차이가 분명함을 확인할 수 있음.\n",
        "* 주먹쥐기(596번 폴더) 클래스이지만 주먹을 활짝 펴버리는(첫번째 이미지) 노이즈 이미지가 존재함. 해당 이미지 때문에 596번 폴더의 x좌표 변화량이 매우 크게 나타남을 확인.\n",
        "* 596번 폴더는 제외하고 threshold를 구함.\n",
        "\n"
      ],
      "metadata": {
        "id": "UtOwcuS5VUWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 주먹쥐기 vs 주먹 내밀기 Right\n",
        "li2 = check_stats(find_list2,2)   \n",
        "li3 = check_stats(find_list3,2)   \n",
        "threshold_ver2 = max(li2) + 5   # Margin 5\n",
        "print(f'\\n{threshold_ver2:.3f}보다 크면 주먹 내밀기(right) 클래스')\n",
        "\n",
        "# 주먹쥐기 vs 주먹 내밀기 Both\n",
        "li4 = check_stats(find_list4,2)   \n",
        "li4 = li4[1:]  # 596번 폴더 변화량(218.313) Outlier -> 제외\n",
        "li5 = check_stats(find_list5,2)   \n",
        "threshold_ver2_both = max(li4) + 5  # Margin 5\n",
        "print(f'\\n{threshold_ver2_both:.3f}보다 크면 주먹 내밀기(both) 클래스')\n",
        "\n",
        "## Case2에서 Both와 Right를 구분하여 threshold를 구하고자 했지만 큰 차이가 없어 실제 inference시에는 통합함. ###"
      ],
      "metadata": {
        "id": "C11kLF_cVFyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stratified KFold\n",
        "* 앞서 Stratified GroupKFold로 데이터를 Split하여 오류가 심한 클래스에 대한 분석을 수행함.\n",
        "* 다만, Stratified GroupKFold 방법은 여전히 각 클래스당 폴더 개수가 너무 적기 때문인지 Validation Loss 기준으로 더 좋은 성능을 얻지는 못하였음.\n",
        "* 그래서 처음에 StratifiedGroupKFold를 사용하고자 했던 선택을 바꾸어, 클래스 비율을 고려하여 랜덤하게 섞어주는 Stratified KFold를 사용."
      ],
      "metadata": {
        "id": "nyAkfdKVTKfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Function"
      ],
      "metadata": {
        "id": "hCNf5GGBTe2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "  # Random Seed\n",
        "  seed = args.seed\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "\n",
        "  save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n",
        "  # Create model directory\n",
        "  os.makedirs(save_path, exist_ok=True)\n",
        "  Trainer(args, save_path)"
      ],
      "metadata": {
        "id": "fRXDJhjWRMHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  for i in range(5): # 5Folds Training\n",
        "    args.fold = i\n",
        "    args.exp_num = str(i)\n",
        "    main(args)"
      ],
      "metadata": {
        "id": "JChUvGkDRMFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference with Ensemble\n",
        "* 위에서 구한 threshold를 이용하여 Rule Base inference를 구축.\n",
        "* 변수 replace_dict를 통해 헷갈리는 두 클래스를 매칭함.\n"
      ],
      "metadata": {
        "id": "kko2ZWO7T2JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test의 keypoints(json) 변화량을 구하기 위한 함수 정의\n",
        "def Refiner(keypoints, ver):\n",
        "  keypoints = np.array([point['data']for point in keypoints])\n",
        "  # 숫자 1과 검지 흔들기 구분\n",
        "  if ver == 1:  \n",
        "    keypoints = keypoints[:, :, :2]  \n",
        "    x_I_finger = [point_per_img[:,0][point_per_img[:,1].argmin()] for point_per_img in keypoints]\n",
        "    query_value = np.max(x_I_finger) - np.min(x_I_finger)\n",
        "    \n",
        "  # 주먹쥐기와 주먹 내밀기(경고) \n",
        "  elif ver == 2:\n",
        "    keypoints = keypoints[:, :, 0]  \n",
        "    x_values = [point_per_img[point_per_img.argmax()] for point_per_img in keypoints]\n",
        "    query_value = np.max(x_values) - np.min(x_values)\n",
        "\n",
        "  return query_value"
      ],
      "metadata": {
        "id": "yGoTBXnbRMCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Pretrained Weight (./results/ 경로)\n",
        "os.makedirs('./results/', exist_ok=True)\n",
        "!wget -i https://raw.githubusercontent.com/wooseok-shin/Egovision-1st-place-solution/main/load_pretrained.txt -P results   "
      ],
      "metadata": {
        "id": "jQOKg374RL_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "test_path = f'../data/test' \n",
        "test_folders = natsorted(glob(test_path + '/*'))\n",
        "\n",
        "args = easydict.EasyDict({'encoder_name':'regnety_040',\n",
        "                        'drop_path_rate':0,\n",
        "                        })\n",
        "\n",
        "load_pretrain = True  # Use Pretrained weights\n",
        "ensemble_test = True  # Ensemble or Single\n",
        "refine = True         # Use Refiner (Rule-base)\n",
        "\n",
        "\n",
        "if load_pretrain:  # Github로부터 Pretrained Weight Load\n",
        "  model_path0 = './results/0Fold_model.pth' # fold0\n",
        "  model_path1 = './results/1Fold_model.pth' # fold1\n",
        "  model_path2 = './results/2Fold_model.pth' # fold2\n",
        "  model_path3 = './results/3Fold_model.pth' # fold3\n",
        "  model_path4 = './results/4Fold_model.pth' # fold4\n",
        "\n",
        "else:  # 위에서 학습한 모델 Weight Load\n",
        "  model_path0 = './results/000/best_model.pth' # fold0\n",
        "  model_path1 = './results/001/best_model.pth' # fold1\n",
        "  model_path2 = './results/002/best_model.pth' # fold2\n",
        "  model_path3 = './results/003/best_model.pth' # fold3\n",
        "  model_path4 = './results/004/best_model.pth' # fold4\n",
        "\n",
        "\n",
        "# 5Fold Ensemble\n",
        "if ensemble_test:\n",
        "  model0 = Pose_Network(args).to(device)\n",
        "  model0.load_state_dict(torch.load(model_path0)['state_dict'])\n",
        "  model0.eval()\n",
        "\n",
        "  model1 = Pose_Network(args).to(device)\n",
        "  model1.load_state_dict(torch.load(model_path1)['state_dict'])\n",
        "  model1.eval()\n",
        "\n",
        "  model2 = Pose_Network(args).to(device)\n",
        "  model2.load_state_dict(torch.load(model_path2)['state_dict'])\n",
        "  model2.eval()\n",
        "\n",
        "  model3 = Pose_Network(args).to(device)\n",
        "  model3.load_state_dict(torch.load(model_path3)['state_dict'])\n",
        "  model3.eval()\n",
        "\n",
        "  model4 = Pose_Network(args).to(device)\n",
        "  model4.load_state_dict(torch.load(model_path4)['state_dict'])\n",
        "  model4.eval()\n",
        "\n",
        "  model_list = [model0, model1, model2, model3, model4]\n",
        "\n",
        "else:  # Single Best Model (Using the pretrained weight)\n",
        "  model_path = './results/single_best_model.pth'\n",
        "  single_best = Pose_Network(args).to(device)\n",
        "  single_best.load_state_dict(torch.load(model_path)['state_dict'])\n",
        "  single_best.eval()\n",
        "  model_list = [single_best]\n",
        "\n",
        "\n",
        "img_size = 288\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "sub = pd.read_csv('../data/sample_submission.csv')\n",
        "df_info = pd.read_csv('../data/hand_gesture_pose.csv')\n",
        "le = LabelEncoder()\n",
        "le.fit(df_info['pose_id'])\n",
        "trans = le.transform\n",
        "\n",
        "# Class Mapping dict\n",
        "ver1_list = trans([0, 42, 10, 67, 100, 142, 110, 167])   \n",
        "ver2_list = trans([146, 163, 171, 191])\n",
        "replace_dict = {146:163, 171:191, 0:42, 10:67, 100:142, 110:167}\n",
        "replace_dict = dict([trans(x) for x in list(replace_dict.items())])   # Mapping (Origin:0~195 to 0~156)\n",
        "\n",
        "total_list = np.concatenate([ver1_list, ver2_list]).tolist()\n",
        "\n",
        "\n",
        "for i, test_folder in tqdm(enumerate(test_folders)):\n",
        "  dir = os.path.dirname(test_folder)\n",
        "  folder_num = os.path.basename(test_folder)\n",
        "  json_path = opj(dir, folder_num, folder_num+'.json')\n",
        "  js = json.load(open(json_path))\n",
        "  keypoints = js['annotations']  # 해당 이미지에 해당하는 Keypoints\n",
        "  images_list = natsorted(glob(test_folder + '/*.png'))\n",
        "  images = []\n",
        "  for _, (point, image_name) in enumerate(zip(keypoints, images_list)):\n",
        "    croped_image = crop_image(image_name, point, margin=100)\n",
        "    image = transform(croped_image)\n",
        "    images.append(image)\n",
        "\n",
        "  images = torch.stack(images).to(device)\n",
        "  ensemble = np.zeros((157,), dtype=np.float32)\n",
        "  for model in model_list:\n",
        "    preds = model(images)\n",
        "    preds = torch.softmax(preds, dim=1)\n",
        "    preds = torch.mean(preds, dim=0).detach().cpu().numpy()    # shape:(157,)\n",
        "    ensemble += preds\n",
        "  preds = ensemble / len(model_list)\n",
        "  pred_class = preds.argmax().item()\n",
        "  if refine and (pred_class in total_list):\n",
        "    idx = list(replace_dict.keys()).index(pred_class) if pred_class in replace_dict.keys() else list(replace_dict.values()).index(pred_class)\n",
        "    cand1, cand2 = list(replace_dict.items())[idx]\n",
        "\n",
        "      if pred_class in ver1_list:\n",
        "        query_value = Refiner(keypoints, ver=1)\n",
        "        answer = cand1 if query_value < threshold_ver1 else cand2\n",
        "\n",
        "      elif pred_class in ver2_list:\n",
        "        query_value = Refiner(keypoints, ver=2)\n",
        "        answer = cand1 if query_value < threshold_ver2_both else cand2\n",
        "\n",
        "      preds[answer] = 1\n",
        "      preds = np.where(preds != 1, 0, preds)  # Refiner를 통해 나온 class를 제외한 나머지의 확률값은 모두 0으로 변환\n",
        "\n",
        "  sub.iloc[i, 1:] = preds.astype(float)\n",
        "\n",
        "sub.to_csv('./results/submission_train_add_ensemble_rule.csv',index=False)"
      ],
      "metadata": {
        "id": "8T32DRyAUfTC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}