{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week_18.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Code (LSTM)"
      ],
      "metadata": {
        "id": "iJNMbd0qYEVn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIQvldSD9S_s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss, accuracy_score,f1_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from transformers import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.read_csv('train.csv')\n",
        "test=pd.read_csv('test.csv')\n",
        "sample_submission=pd.read_csv('sample_submission.csv')"
      ],
      "metadata": {
        "id": "Dig7N8Gf9y6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.label.value_counts(sort=False)/len(train)\n",
        "# -> 불균형이 심한 데이터"
      ],
      "metadata": {
        "id": "r-LL_nBA935D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "length = train['과제명'].astype(str).apply(len)\n",
        "plt.hist(length, bins = 50, alpha=0.5, color='r', label='word')\n",
        "plt.boxplot(length, labels=['counts'], showmeans=True)"
      ],
      "metadata": {
        "id": "AqpumT4i-AZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "length=train['요약문_연구목표'].astype(str).apply(len)\n",
        "plt.hist(length, bins=50, alpha=0.5, color='r', label='word')\n",
        "plt.title('histogram of length of summary_object')\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.boxplot(length, labels=['counts'], showmeans=True)"
      ],
      "metadata": {
        "id": "WkwspryS-ARm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 전처리\n",
        "train = train[['과제명', 'label']]\n",
        "test = test[['과제명']]"
      ],
      "metadata": {
        "id": "2kf9-PdSWiJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.re.sub 한글 및 공백을 제외한 문자 제거\n",
        "# 2.okt 객체를 활용해 형태소 단위로 나누기\n",
        "# 3.remove_stopwords로 불용어 제거\n",
        "def preprocessing(text, remove_stopwords = False, stop_words=[]):\n",
        "  text=re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ]\",\"\", text)\n",
        "  word_text = okt.morphs(text, stem = True)\n",
        "  if remove_stopwords:\n",
        "    word_review = [token for token in word_text if not token in stop_words]\n",
        "  return word_review"
      ],
      "metadata": {
        "id": "sfFOB5cWW-Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=['은','는','이','가', '하','아','것','들','의','있','되','수','보','주','등','한']\n",
        "okt = Okt()\n",
        "clean_train_text = []\n",
        "clean_test_text = []"
      ],
      "metadata": {
        "id": "pKfNC0naW-Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in tqdm.tqdm(train['과제명']):\n",
        "  try: \n",
        "    clean_train_text.append(preprocessing(text, okt, remove_stopwords=True, stop_words = stop_words))\n",
        "  except:\n",
        "    clean_train_text.append([])"
      ],
      "metadata": {
        "id": "LPOG9v3NXrFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in tqdm.tqdm(test['과제명']):\n",
        "    if type(text) == str:\n",
        "        clean_test_text.append(preprocessing(text, okt, remove_stopwords=True, stop_words=stop_words))\n",
        "    else:\n",
        "        clean_test_text.append([])"
      ],
      "metadata": {
        "id": "cc9O9rl1X_0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텐서플로의 전처리 모듈을 활용해 토크나이징 객체를 만든 후 인덱스 벡터로 전환\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_train_text)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(clean_train_text)\n",
        "test_sequences = tokenizer.texts_to_sequences(clean_test_text)\n",
        "word_vocab = tokenizer.word_index\n",
        "\n",
        "# 패딩 처리\n",
        "train_inputs = pad_sequences(train_sequences, maxlen = 40, padding = 'post')\n",
        "test_intpus = pad_sequences(test_sequences, maxlen = 40, padding = 'post')"
      ],
      "metadata": {
        "id": "Xzr7J61UYCcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels - np.array(train['label']))"
      ],
      "metadata": {
        "id": "inpvGXuIZgA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파라미터 설정\n",
        "vocab_size = data_configs['vocab_size']\n",
        "embedding_dim = 32\n",
        "max_length = 40\n",
        "oov_tok = '<OOV>'"
      ],
      "metadata": {
        "id": "e23plyw0ZjJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가벼운 NLP 모델 생성\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length)\n",
        "  tf.keras.layers.GlobalAveragePooling1D(), \n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(46, activation = 'softmax')                    \n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "V04AVWksZvH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit model\n",
        "num_epochs = 30\n",
        "history = model.fit(train_inputs, labels, \n",
        "                    epochs = num_epochs, verbose = 2, \n",
        "                    validation_split=0,2)"
      ],
      "metadata": {
        "id": "R8FWFrEdaY4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가지표가 Macro F1 -> 확률값으로 결과를 내면 안된다\n",
        "pred = model.predict(test_inputs)\n",
        "pred = tf.argmax(pred, axis = 1)"
      ],
      "metadata": {
        "id": "4eGJDqDIajGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission['label'] = pred"
      ],
      "metadata": {
        "id": "g1qOSdgBazPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission.to_csv('baseline.csv', index = False)"
      ],
      "metadata": {
        "id": "3g2ngaxqa4oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PORORO"
      ],
      "metadata": {
        "id": "c4e7_JyDbpae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from proro import Pororo\n",
        "import html\n",
        "from sentence_transformers import SentenceTransforer, util\n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "gt6H0ZUJbsJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pororo sentence embedding task 사용\n",
        "sembed = Pororo(task = 'sentence_embedding', lang='ko')"
      ],
      "metadata": {
        "id": "qlrH40vTcBsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_corpus(self, courpus):\n",
        "  \"text data embedding\"\n",
        "  corpus_embeddings = self._model.encode(courpus, convert_to_tensor=True)\n",
        "  return corpus_embeddings\n",
        "\n",
        "def embedding_to_embeddings(self, embedding, embeddings, cands):\n",
        "  # embedding한 corpus를 비교해 유사도 추출\"\n",
        "  total_result_list = [] \n",
        "  for embed in embedding :\n",
        "    cos_scores = util.pytorch_cos_sim(embed, embeddings)[0]\n",
        "    cos_scores = cos_scores.cpu()\n",
        "    k = min(len(cos_scores), 10)\n",
        "    top_results = np.argpartition(-cos_scores, range(k))[0:k]\n",
        "    top_results = top_results.tolist()\n",
        "    result = list()\n",
        "    for idx in top_results:\n",
        "      result.append(\n",
        "      (idx, cands[idx].strip(), round(cos_scores[idx].item(), 2)))\n",
        "    total_result_list.append(result)\n",
        "  return total_result_list"
      ],
      "metadata": {
        "id": "w4y11ZhzcIfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리\n",
        "def data_preprocessing(data):\n",
        "    data = data.fillna('NONE')\n",
        "    data['요약문_연구목표'] = data.apply(lambda x : x['과제명'] if x['요약문_연구목표'] == 'NONE' else x['요약문_연구목표'], axis=1)\n",
        "    okt = Okt()\n",
        "    data['요약문_한글키워드'] = data.apply(lambda x : ','.join(okt.nouns(x['과제명'])) if x['요약문_한글키워드'] == 'NONE' else x['요약문_한글키워드'], axis = 1)\n",
        "    return data"
      ],
      "metadata": {
        "id": "efGGCPn2c55l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = data_preprocessing(train_data)"
      ],
      "metadata": {
        "id": "Ls-k4DYtdEUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 중분류\n",
        "change_label_dict = {\n",
        "    0:0,1:1, 2:1, 3:1,4:2,5:2,6:2,7:2,8:2,9:2,10:2,11:2,12:3,13:3,14:4,15:4,16:5,17:5,18:6,19:6,20:6,21:7,22:7,23:8,24:8,25:8,26:8,27:9,28:9,29:9,30:9,31:10,32:10,33:11,34:11,35:11,36:12,37:12,38:13,39:13,40:13,41:14,42:14,43:14,44:14,45:14}\n",
        "train_data['middle_label'] = [change_label_dict[label] for label in train_data['label']]"
      ],
      "metadata": {
        "id": "euMf0cimdFvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid set 생성\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "train_data, valid_ata = train_test_split(train_data, test_size=0.1, random_state = 42)\n",
        "y_true = valid_data['label']\n",
        "y_pred = list()"
      ],
      "metadata": {
        "id": "BN1r1Q54dPqx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}