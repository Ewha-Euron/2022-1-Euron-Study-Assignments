{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Euron_week4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TabNet Baseline  \n",
        "refer: https://www.kaggle.com/code/chumajin/optiver-realized-tabnet-baseline#1.-TabNet"
      ],
      "metadata": {
        "id": "S_-IJX4atIe0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lfg_NfztEMv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from joblib import Parallel, delayed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sc\n",
        "from sklearn.model_selection import KFold\n",
        "import lightgbm as lgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('max_columns', 300)\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn.functional as F\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_pickle(\"../input/optiverlgbbase/train.pkl\")\n",
        "print(train)"
      ],
      "metadata": {
        "id": "4mDveMUdtYvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in train.columns.to_list()[4:]:\n",
        "    train[col] = train[col].fillna(train[col].mean())"
      ],
      "metadata": {
        "id": "aYklErl0thD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Encoder"
      ],
      "metadata": {
        "id": "snIgh5JBtpa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder \n",
        "import pickle\n",
        "\n",
        "le=LabelEncoder()\n",
        "le.fit(train[\"stock_id\"])\n",
        "train[\"stock_id\"] = le.transform(train[\"stock_id\"])\n",
        "\n",
        "with open( 'stock_id_encoder.txt', 'wb') as f:\n",
        "    pickle.dump(le, f)\n",
        "\n",
        "print(train[\"stock_id\"])"
      ],
      "metadata": {
        "id": "aNB4LnxltjO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kfold"
      ],
      "metadata": {
        "id": "EwcYaDT5tvn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn import model_selection\n",
        "\n",
        "def create_folds(data, num_splits,target):\n",
        "    # we create a new column called kfold and fill it with -1\n",
        "    data[\"kfold\"] = -1\n",
        "    \n",
        "    # the next step is to randomize the rows of the data\n",
        "    data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    # calculate number of bins by Sturge's rule\n",
        "    # I take the floor of the value, you can also\n",
        "    # just round it\n",
        "    num_bins = int(np.floor(1 + np.log2(len(data))))\n",
        "    \n",
        "    # bin targets\n",
        "    data.loc[:, \"bins\"] = pd.cut(\n",
        "        data[target], bins=num_bins, labels=False\n",
        "    )\n",
        "    \n",
        "    # initiate the kfold class from model_selection module\n",
        "    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n",
        "    \n",
        "    # fill the new kfold column\n",
        "    # note that, instead of targets, we use bins!\n",
        "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
        "        data.loc[v_, 'kfold'] = f\n",
        "    \n",
        "    # drop the bins column\n",
        "    data = data.drop(\"bins\", axis=1)\n",
        "\n",
        "    # return dataframe with folds\n",
        "    return data\n",
        "\n",
        "train = create_folds(train, 5,\"target\")"
      ],
      "metadata": {
        "id": "X-cdsw16txg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TabNet"
      ],
      "metadata": {
        "id": "g40WrtPht0kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "from pytorch_tabnet.metrics import Metric\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "import os\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "3TSQmgBht2UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmspe(y_true, y_pred):\n",
        "    '''\n",
        "    Compute Root Mean Square Percentage Error between two arrays.\n",
        "    '''\n",
        "    \n",
        "    if (y_true == 0).any():\n",
        "        raise ValueError(\"Root Mean Square Percentage Error cannot be used when \"\n",
        "                         \"targets contain zero values.\")\n",
        "        \n",
        "    loss = np.sqrt(np.mean(np.square(((y_true - y_pred) / y_true)), axis=0)).item()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "class RMSPE(Metric):\n",
        "    def __init__(self):\n",
        "        self._name = \"rmspe\"\n",
        "        self._maximize = False\n",
        "\n",
        "    def __call__(self, y_true, y_score):\n",
        "        return rmspe(y_true, y_score)\n",
        "\n",
        "\n",
        "tabnet_params = dict(\n",
        "    n_d = 32,\n",
        "    n_a = 32,\n",
        "    n_steps = 3,\n",
        "    gamma = 1.3,\n",
        "    lambda_sparse = 0,\n",
        "    optimizer_fn = optim.Adam,\n",
        "    optimizer_params = dict(lr = 1e-2, weight_decay = 1e-5),\n",
        "    mask_type = \"entmax\",\n",
        "    scheduler_params = dict(\n",
        "        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n",
        "    scheduler_fn = ReduceLROnPlateau,\n",
        "    seed = 42,\n",
        "    #verbose = 5,\n",
        "    cat_dims=[len(le.classes_)], cat_emb_dim=[10], cat_idxs=[-1] # define categorical features\n",
        ")"
      ],
      "metadata": {
        "id": "7w9nWSGGt6Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "id": "RVJnxMmet-IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 50\n",
        "\n",
        "bestscores=[]\n",
        "\n",
        "for fold in range(5):\n",
        "    \n",
        "   \n",
        "\n",
        "    traindf = train[train[\"kfold\"]!=fold].reset_index(drop=True)\n",
        "    validdf = train[train[\"kfold\"]==fold].reset_index(drop=True)\n",
        "\n",
        "    ## Normalization except stock id ; stock id is used as categoral features\n",
        "\n",
        "    X_train = traindf.drop(['row_id', 'target', 'time_id',\"kfold\",\"stock_id\"], axis = 1).values\n",
        "    \n",
        "    X_train = scaler.transform(X_train)\n",
        "    X_traindf = pd.DataFrame(X_train)\n",
        "\n",
        "    X_traindf[\"stock_id\"]=traindf[\"stock_id\"]\n",
        "\n",
        "    X_train = X_traindf.values\n",
        "    y_train = traindf['target'].values.reshape(-1, 1)\n",
        "\n",
        "    # validation is same\n",
        "    X_valid = validdf.drop(['row_id', 'target', 'time_id',\"kfold\",\"stock_id\"], axis = 1).values\n",
        "    X_valid = scaler.transform(X_valid)\n",
        "\n",
        "    X_validdf = pd.DataFrame(X_valid)\n",
        "\n",
        "    X_validdf[\"stock_id\"]=validdf[\"stock_id\"]\n",
        "\n",
        "    X_valid = X_validdf.values\n",
        "    y_valid = validdf['target'].values.reshape(-1, 1)\n",
        "    \n",
        "    # calculate weight\n",
        "    \n",
        "    y_weight = 1/np.square(traindf[\"target\"])\n",
        "    \n",
        "   \n",
        "    print(\"----Fold:{}--------start----\".format(str(fold)))\n",
        "\n",
        "    # initialize random seed\n",
        "\n",
        "    random_seed(SEED)\n",
        "\n",
        "\n",
        "    # tabnet model\n",
        "\n",
        "    clf = TabNetRegressor(**tabnet_params)\n",
        "\n",
        "    # tabnet training\n",
        "\n",
        "    clf.fit(\n",
        "        X_train=X_train, y_train=y_train,\n",
        "        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "        eval_name=['train', 'valid'],\n",
        "        eval_metric=[RMSPE],\n",
        "        max_epochs=max_epochs,\n",
        "        patience=10,\n",
        "        batch_size=1024*2, virtual_batch_size=128*2,\n",
        "        num_workers=4,\n",
        "        drop_last=False,\n",
        "        weights = y_weight,\n",
        "        loss_fn=nn.L1Loss()\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # save tabnet model\n",
        "    saving_path_name = \"tabnet_model_test_\" + str(fold)\n",
        "    saved_filepath = clf.save_model(saving_path_name)\n",
        "\n",
        "            \n",
        "            \n",
        "    bestscores.append(clf.best_cost)\n",
        "    "
      ],
      "metadata": {
        "id": "4btGnVKIt_xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bestscores)"
      ],
      "metadata": {
        "id": "0Bg8WGwtuGKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"cv average is \",str(np.mean(bestscores)))"
      ],
      "metadata": {
        "id": "skl2KtlZuIDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature importance of last model in #1\n",
        "\n"
      ],
      "metadata": {
        "id": "KfV1JZSHuN36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Fe = pd.DataFrame()\n",
        "\n",
        "featurecols = traindf.drop(['row_id', 'target', 'time_id',\"kfold\",\"stock_id\"], axis = 1).columns.to_list()\n",
        "featurecols.append(\"stock_id\")\n",
        "\n",
        "Fe[\"features\"] = featurecols\n",
        "Fe[\"Importance\"] = clf.feature_importances_\n",
        "print(Fe)"
      ],
      "metadata": {
        "id": "cudz67QduKzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Fe2 = Fe.sort_values(\"Importance\",ascending=False)\n",
        "print(Fe2)"
      ],
      "metadata": {
        "id": "kOFkBHNLuTnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Z-u0eMC3uWkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"font.size\"] = 18\n",
        "sns.barplot(y=Fe2[\"features\"][:10],x=Fe2[\"Importance\"][:10])"
      ],
      "metadata": {
        "id": "Rb7gNrJQuYfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(y=Fe2[\"features\"][-10:],x=Fe2[\"Importance\"][-10:])"
      ],
      "metadata": {
        "id": "B0r9y-iDuakR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masks"
      ],
      "metadata": {
        "id": "Mxgo4rx4ubyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explain_matrix, masks = clf.explain(X_valid)\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(20,20))\n",
        "\n",
        "for i in range(3):\n",
        "    axs[i].imshow(masks[i][:50])\n",
        "    axs[i].set_title(f\"mask {i}\")"
      ],
      "metadata": {
        "id": "QfATZDJBucpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masksum = masks[0]+masks[1]+masks[2]\n",
        "\n",
        "masksumdf = pd.DataFrame(masksum)\n",
        "masksumdf.columns = featurecols\n",
        "print(masksumdf)"
      ],
      "metadata": {
        "id": "4bjoolxBug-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"font.size\"] = 14\n",
        "plt.figure(figsize=(25,15))\n",
        "sns.heatmap(masksumdf,cbar=False)\n",
        "plt.savefig(\"result.jpg\")"
      ],
      "metadata": {
        "id": "BnKpqXhtukli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "pKSt_Rjbu2NP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data directory\n",
        "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
        "\n",
        "# Function to calculate first WAP\n",
        "def calc_wap1(df):\n",
        "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
        "    return wap\n",
        "\n",
        "# Function to calculate second WAP\n",
        "def calc_wap2(df):\n",
        "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
        "    return wap\n",
        "\n",
        "# Function to calculate the log of the return\n",
        "# Remember that logb(x / y) = logb(x) - logb(y)\n",
        "def log_return(series):\n",
        "    return np.log(series).diff()\n",
        "\n",
        "# Calculate the realized volatility\n",
        "def realized_volatility(series):\n",
        "    return np.sqrt(np.sum(series**2))\n",
        "\n",
        "# Function to count unique elements of a series\n",
        "def count_unique(series):\n",
        "    return len(np.unique(series))\n",
        "\n",
        "# Function to read our base train and test set\n",
        "def read_train_test():\n",
        "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
        "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
        "    # Create a key to merge with book and trade data\n",
        "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
        "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
        "    print(f'Our training set has {train.shape[0]} rows')\n",
        "    return train, test\n",
        "\n",
        "# Function to preprocess book data (for each stock id)\n",
        "def book_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    # Calculate Wap\n",
        "    df['wap1'] = calc_wap1(df)\n",
        "    df['wap2'] = calc_wap2(df)\n",
        "    # Calculate log returns\n",
        "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
        "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
        "    # Calculate wap balance\n",
        "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
        "    # Calculate spread\n",
        "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
        "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
        "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
        "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
        "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
        "    \n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'wap1': [np.sum, np.mean, np.std],\n",
        "        'wap2': [np.sum, np.mean, np.std],\n",
        "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'wap_balance': [np.sum, np.mean, np.std],\n",
        "        'price_spread':[np.sum, np.mean, np.std],\n",
        "        'bid_spread':[np.sum, np.mean, np.std],\n",
        "        'ask_spread':[np.sum, np.mean, np.std],\n",
        "        'total_volume':[np.sum, np.mean, np.std],\n",
        "        'volume_imbalance':[np.sum, np.mean, np.std]\n",
        "    }\n",
        "    \n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "    \n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
        "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n",
        "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
        "    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n",
        "    \n",
        "    # Merge all\n",
        "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n",
        "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n",
        "    # Drop unnecesary time_ids\n",
        "    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n",
        "    \n",
        "    # Create row_id so we can merge\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
        "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
        "    return df_feature\n",
        "\n",
        "# Function to preprocess trade data (for each stock id)\n",
        "def trade_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
        "    \n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'log_return':[realized_volatility],\n",
        "        'seconds_in_bucket':[count_unique],\n",
        "        'size':[np.sum],\n",
        "        'order_count':[np.mean],\n",
        "    }\n",
        "    \n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "    \n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
        "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n",
        "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
        "    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n",
        "\n",
        "    # Merge all\n",
        "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n",
        "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n",
        "    # Drop unnecesary time_ids\n",
        "    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n",
        "    \n",
        "    df_feature = df_feature.add_prefix('trade_')\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
        "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
        "    return df_feature\n",
        "\n",
        "# Function to get group stats for the stock_id and time_id\n",
        "def get_time_stock(df):\n",
        "    # Get realized volatility columns\n",
        "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n",
        "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n",
        "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n",
        "\n",
        "    # Group by the stock id\n",
        "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
        "    # Rename columns joining suffix\n",
        "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
        "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
        "\n",
        "    # Group by the stock id\n",
        "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
        "    # Rename columns joining suffix\n",
        "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
        "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
        "    \n",
        "    # Merge with original dataframe\n",
        "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
        "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
        "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
        "    return df\n",
        "    \n",
        "# Funtion to make preprocessing function in parallel (for each stock id)\n",
        "def preprocessor(list_stock_ids, is_train = True):\n",
        "    \n",
        "    # Parrallel for loop\n",
        "    def for_joblib(stock_id):\n",
        "        # Train\n",
        "        if is_train:\n",
        "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
        "        # Test\n",
        "        else:\n",
        "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
        "    \n",
        "        # Preprocess book and trade data and merge them\n",
        "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
        "        \n",
        "        # Return the merge dataframe\n",
        "        return df_tmp\n",
        "    \n",
        "    # Use parallel api to call paralle for loop\n",
        "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
        "    # Concatenate all the dataframes that return from Parallel\n",
        "    df = pd.concat(df, ignore_index = True)\n",
        "    return df\n",
        "\n",
        "# Function to calculate the root mean squared percentage error\n",
        "def rmspe(y_true, y_pred):\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "# Function to early stop with root mean squared percentage error\n",
        "def feval_rmspe(y_pred, lgb_train):\n",
        "    y_true = lgb_train.get_label()\n",
        "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
        "\n",
        "def train_and_evaluate(train, test):\n",
        "    # Hyperparammeters (just basic)\n",
        "    params = {\n",
        "      'objective': 'rmse',  \n",
        "      'boosting_type': 'gbdt',\n",
        "      'num_leaves': 100,\n",
        "      'n_jobs': -1,\n",
        "      'learning_rate': 0.1,\n",
        "      'feature_fraction': 0.8,\n",
        "      'bagging_fraction': 0.8,\n",
        "      'verbose': -1\n",
        "    }\n",
        "    \n",
        "    # Split features and target\n",
        "    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n",
        "    y = train['target']\n",
        "    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n",
        "    # Transform stock id to a numeric value\n",
        "    x['stock_id'] = x['stock_id'].astype(int)\n",
        "    x_test['stock_id'] = x_test['stock_id'].astype(int)\n",
        "    \n",
        "    # Create out of folds array\n",
        "    oof_predictions = np.zeros(x.shape[0])\n",
        "    # Create test array to store predictions\n",
        "    test_predictions = np.zeros(x_test.shape[0])\n",
        "    # Create a KFold object\n",
        "    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n",
        "    # Iterate through each fold\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n",
        "        print(f'Training fold {fold + 1}')\n",
        "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
        "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
        "        # Root mean squared percentage error weights\n",
        "        train_weights = 1 / np.square(y_train)\n",
        "        val_weights = 1 / np.square(y_val)\n",
        "        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n",
        "        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n",
        "        model = lgb.train(params = params, \n",
        "                          train_set = train_dataset, \n",
        "                          valid_sets = [train_dataset, val_dataset], \n",
        "                          num_boost_round = 10000, \n",
        "                          early_stopping_rounds = 50, \n",
        "                          verbose_eval = 50,\n",
        "                          feval = feval_rmspe)\n",
        "        # Add predictions to the out of folds array\n",
        "        oof_predictions[val_ind] = model.predict(x_val)\n",
        "        # Predict the test set\n",
        "        test_predictions += model.predict(x_test) / 5\n",
        "        \n",
        "    rmspe_score = rmspe(y, oof_predictions)\n",
        "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
        "    # Return test predictions\n",
        "    return test_predictions"
      ],
      "metadata": {
        "id": "qSXPg-qru3mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pathB = \"./\"\n",
        "\n",
        "modelpath = [os.path.join(pathB,s) for s in os.listdir(pathB) if (\"zip\" in s)]\n",
        "\n",
        "# Read train and test\n",
        "train2, test = read_train_test()\n",
        "\n",
        "# Get unique stock ids \n",
        "#train_stock_ids = train['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "#train_ = preprocessor(train_stock_ids, is_train = True)\n",
        "#train = train.merge(train_, on = ['row_id'], how = 'left')\n",
        "\n",
        "# Get unique stock ids \n",
        "test_stock_ids = test['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "test_ = preprocessor(test_stock_ids, is_train = False)\n",
        "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
        "\n",
        "# Get group stats of time_id and stock_id\n",
        "#train = get_time_stock(train)\n",
        "test = get_time_stock(test)\n",
        "\n",
        "#train.to_pickle(\"train.pkl\")\n",
        "\n",
        "# Traing and evaluate\n",
        "#test_predictions = train_and_evaluate(train, test)\n",
        "\n",
        "\n",
        "## fillna for test data ##\n",
        "\n",
        "train=train.drop(\"kfold\",axis=1)\n",
        "        \n",
        "for col in train.columns.to_list()[4:]:\n",
        "    test[col] = test[col].fillna(train[col].mean())\n",
        "\n",
        "\n",
        "### normarize ###    \n",
        "\n",
        "x_test = test.drop(['row_id', 'time_id',\"stock_id\"], axis = 1).values\n",
        "    # Transform stock id to a numeric value\n",
        "\n",
        "x_test = scaler.transform(x_test)\n",
        "X_testdf = pd.DataFrame(x_test)\n",
        "\n",
        "X_testdf[\"stock_id\"]=test[\"stock_id\"]\n",
        "\n",
        "# Label encoding\n",
        "X_testdf[\"stock_id\"] = le.transform(X_testdf[\"stock_id\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x_test = X_testdf.values\n",
        "    \n",
        "    \n",
        "preds=[]\n",
        "for path in modelpath:\n",
        "    \n",
        "    clf.load_model(path)\n",
        "    preds.append(clf.predict(x_test).squeeze(-1))\n",
        "    \n",
        "preds = np.mean(preds,axis=0)\n",
        "\n",
        "\n",
        "test['target'] = preds\n",
        "test[['row_id', 'target']].to_csv('submission.csv',index = False)"
      ],
      "metadata": {
        "id": "17PtXI6xu7vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test[['row_id', 'target']])"
      ],
      "metadata": {
        "id": "2Qw8hy6mu_4l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}