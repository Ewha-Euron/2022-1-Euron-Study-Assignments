Activation functions
딥러닝에 있어서 비선형성을 가해주는 중요한 역할
몇 가지 활성화 함수들의 예시이 있는데 그중에서 시그모이드 함수는 그동안 많이 다뤘다.
그런데 이 함수는 비선형성을 갖추고 있음에도 최종 output을 낼 때만 이용한다.
그 이유는
1. 기울기 gradient 값을 없애버린다. (vanishing gradient)
Neural Network는 backpropagation이라는 기법 때문에 강력한 모델을 만들 수 있었다.
하지만 sigmoid 함수를 사용하면 backprop에 문제가 생긴다.
sigmoid 함수에서 x=-10 일때는 거의 0이고, x=0일 때 기울기가 거의 최대일 때도 0.25 정도이다.
back prop 과정이 이전의 기울기 값에 계속해서 local gradient를 곱해주는 식인데, global gradient가 거의 0이니 결국 모두 0에 수렴해서 기울기 값이 소실되어버리는 것이다.
결과적으로 w가 update되지 못한다는 것이다.

2. Zero-center의 문제도 있다.
시그모이드 함수를 보면 output 값이 항상 양수이다. 0을 중심으로 값이 분포하고 있지 않고
Neural Network는 층을 계속해서 쌓았기 때문에 이전 노드의 output 값이 다음의 입력 값이 된다.
시그모이드를 활성화 함수로 이용하면 계속해서 양수의 값만 input 된다는 것이다.
또 입력값이 항상 양수이면 gradient에서 문제가 생기기도 한다.
back prop 과정에서 시그모이드 함수이므로 dl/df * df/dwi로 w의 기울기를 구할 때 체인 룰을 구한다.
df/dwi는 x_i이고 여기서 입력값은 항상 양수이므로 dl/df의 부호를 따라가게 된다.
전부 양수이거나 전부 음수가 되는 것이다.
그래서 위 슬라이드 오른쪽 그래프를 보면 w1을 x축, w2를 y축인 2차원이라고 보았을 때, update가 초기 가중치 0에서 강의록 그림처럼 파란색 화살표가 best w라 하자.
빨간색 화살표처럼 지그재그로 가중치가 update 되기 때문에 비효율적이라는 문제가 있다. 입력값이 양수이기 때문에 생기는 문제이다.

3. exp() is a bit compute expensive 연산이 무겁다고 한다.

위의 1~3 문제로 인해 시그모이드 함수는 activation 함수로는 이용하지 않는다.

다음은 하이퍼볼릭 탄젠트 함수이다.
시그모이드 함수와 유사하지만 zero centered 문제를 해소했지만 마찬가지로 기울기 소실 문제가 해결되지 못 해 활성화 함수로는 적합하지 않습니다.

ReLU 함수는 양의 방향에서 입력값 x를 그대로 배출하기 때문에 기울기가 살아있다. f(x) = max(0, x) 식을 사용하는 이 함수는 기울기 소실 문제를 어느 정도 해결했다.
따라서 양의 영역에서는 가중치 w의 update가 가능해진다.
exp 연산도 사라져서 계산적으로도 효율적이다.
그래서 ReLU 함수는 2012 AlexNet에서 쓰이기 시작하면서 CNN과 큰 규모의 데이터에 매우 잘 작동했다.
하지만 음의 영역에서는 기울기가 모두 0이 되고 zero - centered 문제가 남아있어 입력 값의 음의 부분은 전부 0이 되기 때문에 vanishing gradient, zero centered 문제로 효율성 문제도 생긴다.

그 문제를 해결한 것이 dead ReLU이다.
w initialization에서 dead ReLU에 빠져버리면 업데이트가 안 될 수도 있고 learning rate를 크게 설정할 경우, w가 너무 크게 비약해서 학습이 안 되는 상황이 발생하는 단점이 있다.

이렇게 ReLU를 활성화 함수로 한 모델을 훈련 도중에 network의 일부가 freeze 되는데 이는 10~20% 정도가 dead ReLU에 빠진 것이라 한다. 이는 문제이긴 하지만 train은 잘 된다고 한다.
즉, 시그모이드 함수에 비해 기울기 소실 문제가 적으면서 학습이 되고 큰 문제가 있지 않아 상용화된 것이라 생각하면 된다.

그래도 어쨋든 문제가 없는 건 아니라서.. ReLU 함수에 약간의 bias를 주어서 시작하려는 시도가 있다.
(처음부터 dead ReLU로 빠뜨리지 않게 하려는 시도인 듯)
좀더 active의 가능성을 높여주는 것이다. 위에서 말했듯이 initialization은 중요하기 때문이다. (효과가 미미해 안 하는 사람도 있다.)

Leaky ReLU는 렐루 함수의 효율성과 함께 dead ReLU의 단점을 보완한 것이다.
음의 영역에서도 경미한 기울기를 주어 vanishing gradient를 해결한 것이다.

PReLU도 있다. Leaky는 음의 영역의 기울기가 있다면 PReLU는 파라미터로 알파 X를 만들어서 융통성 있게 해결한 것이다.

ELU 함수도 있는데 _| 모양의 함수를 모두 LU 함수라고 한다.
ELU는 0에서의 미분 불가점을 미분 가능하게 smoothing 시켰다.
이 함수가 Leaky ReLU와 비교했을 때 노이즈에 대해서 더 강건하다고 하는데 이는 zero mean에서 함수가 스무스하기 때문에 생긴 특징이다.
따라서 zero mean의 결과값들을 출력하는데 장점이 있다. 하지만 여전히 음의 영역에서 기울기 값 소실이 있고 exp 연산으로 인한 효율성 저하도 있다.
ReLU < ELU < Leaky ReLU 정도 이다.
보통은 각각의 장단점을 보고 경험에 따라 사용할 함수를 결정한다고 한다.
(그리고 다 한 번씩 사용해보면서 성능이 가장 좋은 것을 선택해서 쓰면 된다고 하는데 sigmoid는 그냥 쓰지 말라고 한다.)

Maxout Neuron은 파라미터를 더 두어서 각각 출력 갑시 다른 함수 2개 중에 max 값을 취하는 함수이다.
maxout은 기울기 소실은 없지만 연산량이 2배라는 비효율성이 있다.

데이터 전처리 내용도 다룬다.
zero centering / normalize가 전처리 방식 중 하나로 나온다.
데이터들의 중앙값이 0에 가깝도록 배치한 뒤 각각의 들쑥날쑥한 feature들을 어떤 범위 내로 모아주었다.

하지만 이미지 데이터에서는 zero centering만 한다. 어차피 이미지 데이터들은 픽셀 값이 전부 같은 범위이기 때문이다. (0~255)

zero centering을 하는 이유는 입력값이 전부 양수이면 w가 전부 음수나 양수가 되어서 업데이트가 일방향으로만 일어나기 때문에 비효율적이기 때문이다.
zero mean으로는 그 문제를 해결할 수 없고 오직 첫 layer에서 zero centering이 되고 그 이후에 네트워크가 깊어짐녀 깊어질수록 활성화 함수로 값이 양수만 나와서 첫 layer 정도에만 의미가 있다고 한다.

전처리로 PCA나 whitening 기법들을 생각해볼 수 있다. 하지만 이미지 데이터에서는 zero mean 정도만 학 다른 복잡한 기법은 안 쓴다고 한다.
왜냐면 이미지 데이터를 저차원의 새로운 피쳐들로 이루어서 학습을 진행하는 것이 아니라 convolutional layer에서 오리지널의 이미지 데이터를 필터 등으로 공간적인 구조로 바라봐야 함을 알기 때문이라고 합니다.

train set과 test set을 전처리할 때는 동일한 mean을 이용해 zero mean을 해준다.
여기서 mean값을 이미지의 전체 데이터로 할지, 아니면 각각 독립된 채널마다의 mean으로 이용할지 선택할 수 있는데, 이는 큰 차이는 나지 않는다고 한다.

VGGNet은 알렉스 넷보다 이후에 나온 것인데 각각의 채널별로 mean값을 두어 zero mean을 했다고 한다.


Weight Initialization
Neural Net에서 weight를 전부 0으로 맞추면 뉴런들은 전부 똑같은 것을 수행하게 됩니다.
w가 전부 0이기 때문에 입력값에서부터 모든 뉴런이 계속 똑같은 값을 내놓는다는 것이고, 활성화 함수가 작동하지 않으니, layer가 의미 없어지는 상황이라고 생각하면 된다.

그래서 이를 해결하기 위한 방식으로는
1. 맨 처음 w에 아주 작은 랜덤의 값을 부여하는 것이다. 정규 분포로부터 가져온 작은 편차들의 값들을 랜덤으로 w initialization 해주는 것이다. 하지만 이 방법도 Neural Net이 깊어지면 문제가 발생한다.
w initialization에서 임의의 작은 값들을 랜덤하게 주니 1보다 작은 w값들이 layer가 깊어질수록 계속해서 곱해지니까, 0으로 수렴해버린 것이다.
값이 너무 작아져서 collapse 되어 버린 것이다.

-> 이런 상황에서의 w의 gradient는 입력값, 출력값들이 애초에 0에 수렴할 정도로 작은 값들이니 gradient마저도 엄청나게 작아지게 된다. 그래서 update도 잘 이루어지지 않는다.
그래서 항상 활성화 함수에 대해서 foward pass와 backward에서의 gradient의 흐름을 잘 파악하고 생각해야한다고 강조한다.

가중치를 0.01이 아니라 1을 곱해서 0으로 수렴하지 않도록 한 결과가 있다. 그랬더니 출력값들이 1과 -1로만 가버려서 explode 해버린다.
tanh 함수에서 1과 -1 값들을 전부 기울기아 0이 되는 saturated 지점이었는데 기울기가 전부 0이 되어 update가 마찬가지로 되지 않는다.
그래서 강의에서는 initialization이 w를 작게 하면 collapse가 되고, 너무 크게 하면 saturate되기 때문에 매우 어려운 것이라고 한다.
(initialization이 잘된 예시로 2010년의 Xavier initialization)
(*입력 값들의 수가 적으면 우리는 큰 값의 w가 필요하다. 그래서 더 적은 수로 나누어 스케일링 해주고, 반대로 입력값들의 수가 많으면 우리는 작은 값은 w가 필요해서 큰 수로 나누어 스케일링을 하는 것이다.)

하지만 이 방식은 비선형 함수인 ReLU를 이용하면, 다시금 문제가 발생한다. zero-mean 함수였던 tanh와는 다르게 ReLU는 음수 부분은 전부 0이 되기 때문에 강의록과 같은 그래프가 나타난다.
활성화 함수를 지나면서 편차가 강제로 절반이 된다. layer가 깊어질수록 std가 0에 가까워지는 것이 보인다. 출력값들이 0이 많다면 w의 update는 잘 이루어지지 않을 것이다.
여기서 np.sqrt(fan_in/2)로 수정해 2로 더 나누어주면 (뉴런의 절반이 죽기 때문에) 그래프의 상황이 더 나아지는 것을 볼 수 있고 마지막 layer에도 출력값이 살아있다.
이렇게 작은 차이에도 weight의 값들에 대해서 주의를 기울이면, 큰 차이가 난다는 것을 강조한다.

Batch normalization
우리가 원하는 가우시안의 범위 내에서 activation 값들을 뽑고 싶은 idea에서 기인한 방식이다.
현재 batch에서의 평균과 분산을 이용해서 훈련 처음부터 batch normalization을 취해 주어 모든 층에서 정규 분포를 따르게끔 하는 것이다.
각 뉴런에서 평균과 분산을 구하는 function을 이용해서 batch마다 normalization을 해주는 것인데 이는 미분 가능한 함수라서 back prop에도 문제가 없다.
이걸 이용하면 굳이 weight init을 안 해도 된다고 한다. 이는 기본적으로 training과정에서 gradient vanishing의 문제를 일어나지 않도록 한다.

batch normalization은 각 층의 input distribution을 평균 0, 표준편차 1로 만드는 것이다.
network 각 층의 activation마다 input distribution이 달라지만 internal covariance shift이 달라지는 문제를 해결하기 위해 사용된다.
보통 batch 별로 데이터를 train 시키는데, 이때 N*D의 batch input이 들어오면 이것을 normalize한다.
이는 일반적으로 activation layer 전에 사용되어 잘 분포되도록 한 후, activation을 진행할 수 있도록 한다. BN 목적이 네트워크 연산 결과가 원하는 방향의 분포대로 나오는 것이기 때문에 activation function이 적용되 분포가 달라지기 전에 적용하는 것이다.

그런데 BN을 사용할 때 과연 unit gaussian이 적합한지에 대해서 판단하여야 한다.
BN이 적절한지에 대한 판단을 학습에 의ㅣ하여 조절할 수 있다고 한다. 처음 normalize를 진행하고 이후 감마 값과 같은 하이퍼 파라미터 값들을 조정하여 batch norm을 할지 않 할지 선택하는 것이다.
감마 값은 normalizing scale를 조절해주고 베타 값은 shift를 조절해주는 파라미터 값이다.
이 값들을 학습을 통해 조절함으로서 normalize 정도를 조절할 수 있다.

먼저 mean과 variance를 구하고 normalize를 한다.
그리고 scale, shift를 얼마나 할지 학습을 통해 값을 정하게 된다.

BN을 하게 되면 dropout를 사용하지 않아도 된다고 한다. BN의 논문 저자는 dropout을 대체할 수 있다고 하였으나 실제적으로 둘 다 모두 사용했을 때 성능이 좋은 경우도 있고 의견이 다양하게 존재하는 것이다.

Babysitting the Learning Process
이 과정은
1. 먼저 전처리 과정을 진행해준다. 이미지의 경우 zero - centered를 사용한다.
2. 사용할 neural architecture를 정해준다.
3. sanity check를 통해 loss가 적절하게 나오는지 확인한다.
4. 규제 값을 올렸을 때 loss가 증가하였고, network가 잘 동작하는지 확인할 수 있다.
5. 그리고 먼저 작은 데이터셋을 이용해서 훈련을 시켜본다.
6. data 수가 작기 때문에 overfitting이 발생하고, train acc가 100% 나오는 것을 확인할 수 있다..
   overfitting이 나온다는 것은 모델이 제대로 동작하고 있다는 의미로 받아드려고 된다.
7. 이제 적절한 hyper parameter 값들을 설정해줄 것이다. 강의록에 따라 lr를 정하면 loss가 매우 조금씩 바뀌는 것을 확인할 수 있었다.
하지만 train acc가 조금씩 높아지는 것을 보면 느리지만 train이 되는 것을 확인할 수 있다.
8. 다시 lr로 설정하니 cost 값이 Nan이 되는 것을 확인할 수 있다. 이것은 값이 너무 커서 튕겨 나간것이라 할 수 있다.
9. 또 lr를 수정해도 inf로 튕겨져 나가는 것을 확인할 수 있고 몇 번의 실험을 통해 1e-3~1e-5 값이 적절한 값이라는 것을 추측할 수 있다.

Hyperparameter Optimization
처음에는 범위를 크게 잡았다가 좁은 범위로 좁혀나가는 방법을 사용한다.

이렇게 hyperparameter를 찾기 위한 방법으로 Grid Search와 Random Search가 있는데 Grid Search는 일정한 간격을 가지고 있어, 제일 best case를 찾지 못할 수도 있다.
그래서 Random Search 방법을 더 많이 사용한다고 한다.
이렇게 적절한 hyperparameter를 찾음으로서 좋은 train이 이루어지는 환경을 설정할 수 있다.
hyperparamter에는 network architecture, lr, decay, regularization 등이 있다고 한다.