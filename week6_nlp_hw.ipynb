{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week6_nlp_hw의 사본",
      "provenance": [],
      "collapsed_sections": [
        "-vPZn15zBHIv",
        "xUWWDwdiPLS9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "📌 week6 과제는 **6주차의 Language Model 및 Vanila RNN 실습**으로 구성되어 있습니다.\n",
        "\n",
        "📌 위키독스의 딥러닝을 이용한 자연어 처리 입문 교재 실습, \b텐서플로우 및 케라스 등의 공식 문서 자료로 구성되어있는 과제입니다. \n",
        "\n",
        "📌 안내된 링크에 맞추어 **직접 코드를 따라 치면서 (필사)** 해당 nlp task 의 기본적인 라이브러리와 메서드를 숙지해보시면 좋을 것 같습니다😊 필수라고 체크한 부분은 과제에 반드시 포함시켜주시고, 선택으로 체크한 부분은 자율적으로 스터디 하시면 됩니다.\n",
        "\n",
        "📌 궁금한 사항은 깃허브 이슈나, 카톡방, 세션 발표 시작 이전 시간 등을 활용하여 자유롭게 공유해주세요!"
      ],
      "metadata": {
        "id": "QhUHfXkPAORh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk colab 환경에서 실행시 필요한 코드입니다. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('reuters')"
      ],
      "metadata": {
        "id": "3XjTSbcxBB6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54399ccc-0111-4279-fd79-f4195de13a4f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1️⃣ **Language Model**\n",
        "\n"
      ],
      "metadata": {
        "id": "-vPZn15zBHIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👀 **내용 복습** \n",
        "* Language Model은 문장이 얼마나 자연스러운지를 판단하여 다음에 나타날 단어가 어떤 것인지를 예측합니다.\n",
        "* [Language Model 설명 참고 자료](https://wikidocs.net/21695)"
      ],
      "metadata": {
        "id": "cfTJoGzkEBlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 1-(1) \b**Basic Language Model** 구현하기\n",
        "\n",
        "\n",
        "📌 [Baseline](https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d) 👉 필수 (**아래 기재된 범위 확인!**)\n",
        "\n",
        "* 범위: **Building a Basic Language Model~ Results** (Natural Language Generation using OpenAI’s GPT-2 **전**까지만 실습해 주시면 됩니다.)"
      ],
      "metadata": {
        "id": "3PFUqUFMVzv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code courtesy of https://nlpforhackers.io/language-models/\n",
        "import nltk\n",
        "nltk.download(\"reuters\")\n",
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "\n",
        "# Create a placeholder for model\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1\n",
        " \n",
        "# Let's transform the counts to probabilities\n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count"
      ],
      "metadata": {
        "id": "FGxBTM7aXHSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code courtesy of https://nlpforhackers.io/language-models/\n",
        "\n",
        "import random\n",
        "\n",
        "# starting words\n",
        "text = [\"today\", \"the\"]\n",
        "sentence_finished = False\n",
        " \n",
        "while not sentence_finished:\n",
        "  # select a random probability threshold  \n",
        "  r = random.random()\n",
        "  accumulator = .0\n",
        "\n",
        "  for word in model[tuple(text[-2:])].keys():\n",
        "      accumulator += model[tuple(text[-2:])][word]\n",
        "      # select words that are above the probability threshold\n",
        "      if accumulator >= r:\n",
        "          text.append(word)\n",
        "          break\n",
        "\n",
        "  if text[-2:] == [None, None]:\n",
        "      sentence_finished = True\n",
        " \n",
        "print (' '.join([t for t in text if t]))"
      ],
      "metadata": {
        "id": "bjE8mIBx0Drw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "-egHx-wQ0FDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def text_cleaner(text):\n",
        "    # lower case text\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    # remove punctuations\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    long_words=[]\n",
        "    # remove short word\n",
        "    for i in newString.split():\n",
        "        if len(i)>=3:                  \n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "# preprocess the text\n",
        "data_new = text_cleaner(data_text)"
      ],
      "metadata": {
        "id": "4MDrXKth0ick"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_seq(text):\n",
        "    length = 30\n",
        "    sequences = list()\n",
        "    for i in range(length, len(text)):\n",
        "        # select sequence of tokens\n",
        "        seq = text[i-length:i+1]\n",
        "        # store\n",
        "        sequences.append(seq)\n",
        "    print('Total Sequences: %d' % len(sequences))\n",
        "    return sequences\n",
        "\n",
        "# create sequences   \n",
        "sequences = create_seq(data_new)"
      ],
      "metadata": {
        "id": "LLjq86em6cab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a character mapping index\n",
        "chars = sorted(list(set(data_new)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "def encode_seq(seq):\n",
        "    sequences = list()\n",
        "    for line in seq:\n",
        "        # integer encode line\n",
        "        encoded_seq = [mapping[char] for char in line]\n",
        "        # store\n",
        "        sequences.append(encoded_seq)\n",
        "    return sequences\n",
        "\n",
        "# encode the sequences\n",
        "sequences = encode_seq(sequences)"
      ],
      "metadata": {
        "id": "HdkM2MD46nHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# vocabulary size\n",
        "vocab = len(mapping)\n",
        "sequences = np.array(sequences)\n",
        "# create X and y\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "# one hot encode y\n",
        "y = to_categorical(y, num_classes=vocab)\n",
        "# create train and validation sets\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
      ],
      "metadata": {
        "id": "ZOc1g9G86qbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
        "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
        "model.add(Dense(vocab, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
        "# fit the model\n",
        "model.fit(X_tr, y_tr, epochs=100, verbose=2, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "1_SXB06s6rLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of characters\n",
        "\tfor _ in range(n_chars):\n",
        "\t\t# encode the characters as integers\n",
        "\t\tencoded = [mapping[char] for char in in_text]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict character\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# reverse map integer to character\n",
        "\t\tout_char = ''\n",
        "\t\tfor char, index in mapping.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_char = char\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += char\n",
        "\treturn in_text"
      ],
      "metadata": {
        "id": "mCdXB0Cm6vMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 1-(2) \b**Keras**에서 NNLM 이용하여 분류 모델 생성하기\n",
        "\n",
        "* Keras Hub에서 미리 구현된 NNLM을 이용하여 분류 모델을 제작할 수 있습니다.\n",
        "\n",
        "📌 [케라스와 텐서플로 허브를 사용한 영화 리뷰 텍스트 분류하기](https://www.tensorflow.org/tutorials/keras/text_classification_with_hub?hl=ko) 👉 필수\n",
        "\n",
        "- 텐서플로우 허브를 이용한 문장 분류 관련 참고 사이트\n",
        "  - 📍 http://solarisailab.com/archives/2497\n",
        "  - 📍 https://minding-deep-learning.tistory.com/27"
      ],
      "metadata": {
        "id": "3FoiMuX9Y2P7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-hub\n",
        "!pip install tensorflow-datasets"
      ],
      "metadata": {
        "id": "iJksiYc-Y6zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "metadata": {
        "id": "r4iU2q9-940p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training set into 60% and 40% to end up with 15,000 examples\n",
        "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
        "train_data, validation_data, test_data = tfds.load(\n",
        "    name=\"imdb_reviews\", \n",
        "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
        "    as_supervised=True)"
      ],
      "metadata": {
        "id": "mp99FkNL97ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
        "train_examples_batch"
      ],
      "metadata": {
        "id": "rlzz7gB999t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_batch"
      ],
      "metadata": {
        "id": "zPuNRAfA9_wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
        "                           dtype=tf.string, trainable=True)\n",
        "hub_layer(train_examples_batch[:3])"
      ],
      "metadata": {
        "id": "giGkRc44-Bca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "eiJzF9rR-E-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "mXsDLNjw-FkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_data.shuffle(10000).batch(512),\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_data.batch(512),\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "DAITui5--HpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data.batch(512), verbose=2)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "metadata": {
        "id": "UKIr_9LA-JqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "pJIWoJooXIj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2️⃣ RNN**"
      ],
      "metadata": {
        "id": "xUWWDwdiPLS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👀 **내용 복습** \n",
        "* 자연어 처리에 주로 이용되는 순한 신경망(Recurrent Neural Network)\n",
        "* 다양한 길이의 입력 시퀀스 처리 가능\n",
        "* input뿐 아니라 이전 레이어의의 출력 또한 다음 레이어의 입력으로 받음"
      ],
      "metadata": {
        "id": "9N0B4VknPkTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "🔹 **2-(1)** **Keras**로 **RNN** 구현하기\n",
        "\n",
        "\n",
        "* Keras Library는 RNN을 보다 쉽게 구현할 수 있도록 모듈을 제공하고 있습니다. \n",
        "* SimpleRNN 셀로 이루어진 layer을 쌓아 RNN 아키텍처를 만들어 봅시다.\n",
        "\n",
        "📌 [basic code](https://wikidocs.net/22886) 👉 필수 (**2번만**)\n",
        "\n",
        "* Tensorflow, Keras 공식 문서에서는 vanila RNN뿐 아니라 LSTM, GRU 또한 이용하고 있습니다. week7에서 다루고 있는 내용이니, Tensorflow 공식 문서를 참고하여 미리 익혀 보세요!\n",
        "\n",
        "📌 [Tensorflow 공식 문서](https://www.tensorflow.org/guide/keras/rnn?hl=ko) 👉 선택(권장)\n",
        "\n",
        "📌 [Keras 공식 문서](https://keras.io/guides/working_with_rnns/) 👉 선택\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QWgla1BuPRqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import SimpleRNN\n",
        "\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "\n",
        "# 추가 인자를 사용할 때\n",
        "model.add(SimpleRNN(hidden_units, input_shape=(timesteps, input_dim)))\n",
        "\n",
        "# 다른 표기\n",
        "model.add(SimpleRNN(hidden_units, input_length=M, input_dim=N))"
      ],
      "metadata": {
        "id": "diaZweMyAxJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(3, input_shape=(2,10)))\n",
        "# model.add(SimpleRNN(3, input_length=2, input_dim=10))와 동일함.\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "QhxiOeb57pLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(SimpleRNN(3, batch_input_shape=(8,2,10)))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "nKLkckHl7pp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=True))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "lnstYCVV7sN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 **2-(2)** **python numpy**로 **RNN layer** 구현하기\n",
        "\n",
        "\n",
        "* 라이브러리를 쓰지 않고 numpy만을 이용해 간단한 RNN layer를 구현해 봅니다.\n",
        "\n",
        "📌 [basic code](https://wikidocs.net/22886) 👉 필수 (**3번만**)"
      ],
      "metadata": {
        "id": "TPX-WtSvPmm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 아래의 코드는 가상의 코드(pseudocode)로 실제 동작하는 코드가 아님. \n",
        "\n",
        "hidden_state_t = 0 # 초기 은닉 상태를 0(벡터)로 초기화\n",
        "for input_t in input_length: # 각 시점마다 입력을 받는다.\n",
        "    output_t = tanh(input_t, hidden_state_t) # 각 시점에 대해서 입력과 은닉 상태를 가지고 연산\n",
        "    hidden_state_t = output_t # 계산 결과는 현재 시점의 은닉 상태가 된다."
      ],
      "metadata": {
        "id": "my3CWhEtXEj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "timesteps = 10\n",
        "input_dim = 4\n",
        "hidden_units = 8\n",
        "\n",
        "# 입력에 해당되는 2D 텐서\n",
        "inputs = np.random.random((timesteps, input_dim))\n",
        "\n",
        "# 초기 은닉 상태는 0(벡터)로 초기화\n",
        "hidden_state_t = np.zeros((hidden_units,)) \n",
        "\n",
        "print('초기 은닉 상태 :',hidden_state_t)"
      ],
      "metadata": {
        "id": "5OhUo-PH72tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Wx = np.random.random((hidden_units, input_dim))  # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치.\n",
        "Wh = np.random.random((hidden_units, hidden_units)) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치.\n",
        "b = np.random.random((hidden_units,)) # (8,)크기의 1D 텐서 생성. 이 값은 편향(bias).\n",
        "\n",
        "print('가중치 Wx의 크기(shape) :',np.shape(Wx))\n",
        "print('가중치 Wh의 크기(shape) :',np.shape(Wh))\n",
        "print('편향의 크기(shape) :',np.shape(b))"
      ],
      "metadata": {
        "id": "qbrJkav774Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_hidden_states = []\n",
        "\n",
        "# 각 시점 별 입력값.\n",
        "for input_t in inputs:\n",
        "\n",
        "  # Wx * Xt + Wh * Ht-1 + b(bias)\n",
        "  output_t = np.tanh(np.dot(Wx,input_t) + np.dot(Wh,hidden_state_t) + b)\n",
        "\n",
        "  # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep t, output_dim)\n",
        "  # 각 시점의 은닉 상태의 값을 계속해서 누적\n",
        "  total_hidden_states.append(list(output_t))\n",
        "  hidden_state_t = output_t\n",
        "\n",
        "# 출력 시 값을 깔끔하게 해주는 용도.\n",
        "total_hidden_states = np.stack(total_hidden_states, axis = 0) \n",
        "\n",
        "# (timesteps, output_dim)\n",
        "print('모든 시점의 은닉 상태 :')\n",
        "print(total_hidden_states)"
      ],
      "metadata": {
        "id": "2OI9H2Xl75yI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}