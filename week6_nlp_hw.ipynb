{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week6_nlp_hwì˜ ì‚¬ë³¸",
      "provenance": [],
      "collapsed_sections": [
        "-vPZn15zBHIv",
        "xUWWDwdiPLS9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“Œ week6 ê³¼ì œëŠ” **6ì£¼ì°¨ì˜ Language Model ë° Vanila RNN ì‹¤ìŠµ**ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ìœ„í‚¤ë…ìŠ¤ì˜ ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ êµì¬ ì‹¤ìŠµ, \bí…ì„œí”Œë¡œìš° ë° ì¼€ë¼ìŠ¤ ë“±ì˜ ê³µì‹ ë¬¸ì„œ ìë£Œë¡œ êµ¬ì„±ë˜ì–´ìˆëŠ” ê³¼ì œì…ë‹ˆë‹¤. \n",
        "\n",
        "ğŸ“Œ ì•ˆë‚´ëœ ë§í¬ì— ë§ì¶”ì–´ **ì§ì ‘ ì½”ë“œë¥¼ ë”°ë¼ ì¹˜ë©´ì„œ (í•„ì‚¬)** í•´ë‹¹ nlp task ì˜ ê¸°ë³¸ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë©”ì„œë“œë¥¼ ìˆ™ì§€í•´ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ğŸ˜Š í•„ìˆ˜ë¼ê³  ì²´í¬í•œ ë¶€ë¶„ì€ ê³¼ì œì— ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì£¼ì‹œê³ , ì„ íƒìœ¼ë¡œ ì²´í¬í•œ ë¶€ë¶„ì€ ììœ¨ì ìœ¼ë¡œ ìŠ¤í„°ë”” í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ê¶ê¸ˆí•œ ì‚¬í•­ì€ ê¹ƒí—ˆë¸Œ ì´ìŠˆë‚˜, ì¹´í†¡ë°©, ì„¸ì…˜ ë°œí‘œ ì‹œì‘ ì´ì „ ì‹œê°„ ë“±ì„ í™œìš©í•˜ì—¬ ììœ ë¡­ê²Œ ê³µìœ í•´ì£¼ì„¸ìš”!"
      ],
      "metadata": {
        "id": "QhUHfXkPAORh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk colab í™˜ê²½ì—ì„œ ì‹¤í–‰ì‹œ í•„ìš”í•œ ì½”ë“œì…ë‹ˆë‹¤. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('reuters')"
      ],
      "metadata": {
        "id": "3XjTSbcxBB6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54399ccc-0111-4279-fd79-f4195de13a4f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1ï¸âƒ£ **Language Model**\n",
        "\n"
      ],
      "metadata": {
        "id": "-vPZn15zBHIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‘€ **ë‚´ìš© ë³µìŠµ** \n",
        "* Language Modelì€ ë¬¸ì¥ì´ ì–¼ë§ˆë‚˜ ìì—°ìŠ¤ëŸ¬ìš´ì§€ë¥¼ íŒë‹¨í•˜ì—¬ ë‹¤ìŒì— ë‚˜íƒ€ë‚  ë‹¨ì–´ê°€ ì–´ë–¤ ê²ƒì¸ì§€ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
        "* [Language Model ì„¤ëª… ì°¸ê³  ìë£Œ](https://wikidocs.net/21695)"
      ],
      "metadata": {
        "id": "cfTJoGzkEBlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 1-(1) \b**Basic Language Model** êµ¬í˜„í•˜ê¸°\n",
        "\n",
        "\n",
        "ğŸ“Œ [Baseline](https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d) ğŸ‘‰ í•„ìˆ˜ (**ì•„ë˜ ê¸°ì¬ëœ ë²”ìœ„ í™•ì¸!**)\n",
        "\n",
        "* ë²”ìœ„: **Building a Basic Language Model~ Results** (Natural Language Generation using OpenAIâ€™s GPT-2 **ì „**ê¹Œì§€ë§Œ ì‹¤ìŠµí•´ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.)"
      ],
      "metadata": {
        "id": "3PFUqUFMVzv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code courtesy of https://nlpforhackers.io/language-models/\n",
        "import nltk\n",
        "nltk.download(\"reuters\")\n",
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "\n",
        "# Create a placeholder for model\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1\n",
        " \n",
        "# Let's transform the counts to probabilities\n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count"
      ],
      "metadata": {
        "id": "FGxBTM7aXHSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code courtesy of https://nlpforhackers.io/language-models/\n",
        "\n",
        "import random\n",
        "\n",
        "# starting words\n",
        "text = [\"today\", \"the\"]\n",
        "sentence_finished = False\n",
        " \n",
        "while not sentence_finished:\n",
        "  # select a random probability threshold  \n",
        "  r = random.random()\n",
        "  accumulator = .0\n",
        "\n",
        "  for word in model[tuple(text[-2:])].keys():\n",
        "      accumulator += model[tuple(text[-2:])][word]\n",
        "      # select words that are above the probability threshold\n",
        "      if accumulator >= r:\n",
        "          text.append(word)\n",
        "          break\n",
        "\n",
        "  if text[-2:] == [None, None]:\n",
        "      sentence_finished = True\n",
        " \n",
        "print (' '.join([t for t in text if t]))"
      ],
      "metadata": {
        "id": "bjE8mIBx0Drw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "-egHx-wQ0FDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def text_cleaner(text):\n",
        "    # lower case text\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    # remove punctuations\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    long_words=[]\n",
        "    # remove short word\n",
        "    for i in newString.split():\n",
        "        if len(i)>=3:                  \n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "# preprocess the text\n",
        "data_new = text_cleaner(data_text)"
      ],
      "metadata": {
        "id": "4MDrXKth0ick"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_seq(text):\n",
        "    length = 30\n",
        "    sequences = list()\n",
        "    for i in range(length, len(text)):\n",
        "        # select sequence of tokens\n",
        "        seq = text[i-length:i+1]\n",
        "        # store\n",
        "        sequences.append(seq)\n",
        "    print('Total Sequences: %d' % len(sequences))\n",
        "    return sequences\n",
        "\n",
        "# create sequences   \n",
        "sequences = create_seq(data_new)"
      ],
      "metadata": {
        "id": "LLjq86em6cab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a character mapping index\n",
        "chars = sorted(list(set(data_new)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "def encode_seq(seq):\n",
        "    sequences = list()\n",
        "    for line in seq:\n",
        "        # integer encode line\n",
        "        encoded_seq = [mapping[char] for char in line]\n",
        "        # store\n",
        "        sequences.append(encoded_seq)\n",
        "    return sequences\n",
        "\n",
        "# encode the sequences\n",
        "sequences = encode_seq(sequences)"
      ],
      "metadata": {
        "id": "HdkM2MD46nHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# vocabulary size\n",
        "vocab = len(mapping)\n",
        "sequences = np.array(sequences)\n",
        "# create X and y\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "# one hot encode y\n",
        "y = to_categorical(y, num_classes=vocab)\n",
        "# create train and validation sets\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
      ],
      "metadata": {
        "id": "ZOc1g9G86qbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
        "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
        "model.add(Dense(vocab, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
        "# fit the model\n",
        "model.fit(X_tr, y_tr, epochs=100, verbose=2, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "1_SXB06s6rLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of characters\n",
        "\tfor _ in range(n_chars):\n",
        "\t\t# encode the characters as integers\n",
        "\t\tencoded = [mapping[char] for char in in_text]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict character\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# reverse map integer to character\n",
        "\t\tout_char = ''\n",
        "\t\tfor char, index in mapping.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_char = char\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += char\n",
        "\treturn in_text"
      ],
      "metadata": {
        "id": "mCdXB0Cm6vMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ 1-(2) \b**Keras**ì—ì„œ NNLM ì´ìš©í•˜ì—¬ ë¶„ë¥˜ ëª¨ë¸ ìƒì„±í•˜ê¸°\n",
        "\n",
        "* Keras Hubì—ì„œ ë¯¸ë¦¬ êµ¬í˜„ëœ NNLMì„ ì´ìš©í•˜ì—¬ ë¶„ë¥˜ ëª¨ë¸ì„ ì œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ [ì¼€ë¼ìŠ¤ì™€ í…ì„œí”Œë¡œ í—ˆë¸Œë¥¼ ì‚¬ìš©í•œ ì˜í™” ë¦¬ë·° í…ìŠ¤íŠ¸ ë¶„ë¥˜í•˜ê¸°](https://www.tensorflow.org/tutorials/keras/text_classification_with_hub?hl=ko) ğŸ‘‰ í•„ìˆ˜\n",
        "\n",
        "- í…ì„œí”Œë¡œìš° í—ˆë¸Œë¥¼ ì´ìš©í•œ ë¬¸ì¥ ë¶„ë¥˜ ê´€ë ¨ ì°¸ê³  ì‚¬ì´íŠ¸\n",
        "  - ğŸ“ http://solarisailab.com/archives/2497\n",
        "  - ğŸ“ https://minding-deep-learning.tistory.com/27"
      ],
      "metadata": {
        "id": "3FoiMuX9Y2P7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-hub\n",
        "!pip install tensorflow-datasets"
      ],
      "metadata": {
        "id": "iJksiYc-Y6zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "metadata": {
        "id": "r4iU2q9-940p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training set into 60% and 40% to end up with 15,000 examples\n",
        "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
        "train_data, validation_data, test_data = tfds.load(\n",
        "    name=\"imdb_reviews\", \n",
        "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
        "    as_supervised=True)"
      ],
      "metadata": {
        "id": "mp99FkNL97ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
        "train_examples_batch"
      ],
      "metadata": {
        "id": "rlzz7gB999t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_batch"
      ],
      "metadata": {
        "id": "zPuNRAfA9_wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
        "                           dtype=tf.string, trainable=True)\n",
        "hub_layer(train_examples_batch[:3])"
      ],
      "metadata": {
        "id": "giGkRc44-Bca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "eiJzF9rR-E-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "mXsDLNjw-FkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_data.shuffle(10000).batch(512),\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_data.batch(512),\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "DAITui5--HpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data.batch(512), verbose=2)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "metadata": {
        "id": "UKIr_9LA-JqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "pJIWoJooXIj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2ï¸âƒ£ RNN**"
      ],
      "metadata": {
        "id": "xUWWDwdiPLS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‘€ **ë‚´ìš© ë³µìŠµ** \n",
        "* ìì—°ì–´ ì²˜ë¦¬ì— ì£¼ë¡œ ì´ìš©ë˜ëŠ” ìˆœí•œ ì‹ ê²½ë§(Recurrent Neural Network)\n",
        "* ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ì…ë ¥ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ê°€ëŠ¥\n",
        "* inputë¿ ì•„ë‹ˆë¼ ì´ì „ ë ˆì´ì–´ì˜ì˜ ì¶œë ¥ ë˜í•œ ë‹¤ìŒ ë ˆì´ì–´ì˜ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ"
      ],
      "metadata": {
        "id": "9N0B4VknPkTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "ğŸ”¹ **2-(1)** **Keras**ë¡œ **RNN** êµ¬í˜„í•˜ê¸°\n",
        "\n",
        "\n",
        "* Keras LibraryëŠ” RNNì„ ë³´ë‹¤ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë„ë¡ ëª¨ë“ˆì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. \n",
        "* SimpleRNN ì…€ë¡œ ì´ë£¨ì–´ì§„ layerì„ ìŒ“ì•„ RNN ì•„í‚¤í…ì²˜ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤.\n",
        "\n",
        "ğŸ“Œ [basic code](https://wikidocs.net/22886) ğŸ‘‰ í•„ìˆ˜ (**2ë²ˆë§Œ**)\n",
        "\n",
        "* Tensorflow, Keras ê³µì‹ ë¬¸ì„œì—ì„œëŠ” vanila RNNë¿ ì•„ë‹ˆë¼ LSTM, GRU ë˜í•œ ì´ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. week7ì—ì„œ ë‹¤ë£¨ê³  ìˆëŠ” ë‚´ìš©ì´ë‹ˆ, Tensorflow ê³µì‹ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë¯¸ë¦¬ ìµí˜€ ë³´ì„¸ìš”!\n",
        "\n",
        "ğŸ“Œ [Tensorflow ê³µì‹ ë¬¸ì„œ](https://www.tensorflow.org/guide/keras/rnn?hl=ko) ğŸ‘‰ ì„ íƒ(ê¶Œì¥)\n",
        "\n",
        "ğŸ“Œ [Keras ê³µì‹ ë¬¸ì„œ](https://keras.io/guides/working_with_rnns/) ğŸ‘‰ ì„ íƒ\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QWgla1BuPRqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import SimpleRNN\n",
        "\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "\n",
        "# ì¶”ê°€ ì¸ìë¥¼ ì‚¬ìš©í•  ë•Œ\n",
        "model.add(SimpleRNN(hidden_units, input_shape=(timesteps, input_dim)))\n",
        "\n",
        "# ë‹¤ë¥¸ í‘œê¸°\n",
        "model.add(SimpleRNN(hidden_units, input_length=M, input_dim=N))"
      ],
      "metadata": {
        "id": "diaZweMyAxJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(3, input_shape=(2,10)))\n",
        "# model.add(SimpleRNN(3, input_length=2, input_dim=10))ì™€ ë™ì¼í•¨.\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "QhxiOeb57pLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(SimpleRNN(3, batch_input_shape=(8,2,10)))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "nKLkckHl7pp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=True))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "lnstYCVV7sN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ **2-(2)** **python numpy**ë¡œ **RNN layer** êµ¬í˜„í•˜ê¸°\n",
        "\n",
        "\n",
        "* ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì“°ì§€ ì•Šê³  numpyë§Œì„ ì´ìš©í•´ ê°„ë‹¨í•œ RNN layerë¥¼ êµ¬í˜„í•´ ë´…ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ [basic code](https://wikidocs.net/22886) ğŸ‘‰ í•„ìˆ˜ (**3ë²ˆë§Œ**)"
      ],
      "metadata": {
        "id": "TPX-WtSvPmm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì•„ë˜ì˜ ì½”ë“œëŠ” ê°€ìƒì˜ ì½”ë“œ(pseudocode)ë¡œ ì‹¤ì œ ë™ì‘í•˜ëŠ” ì½”ë“œê°€ ì•„ë‹˜. \n",
        "\n",
        "hidden_state_t = 0 # ì´ˆê¸° ì€ë‹‰ ìƒíƒœë¥¼ 0(ë²¡í„°)ë¡œ ì´ˆê¸°í™”\n",
        "for input_t in input_length: # ê° ì‹œì ë§ˆë‹¤ ì…ë ¥ì„ ë°›ëŠ”ë‹¤.\n",
        "    output_t = tanh(input_t, hidden_state_t) # ê° ì‹œì ì— ëŒ€í•´ì„œ ì…ë ¥ê³¼ ì€ë‹‰ ìƒíƒœë¥¼ ê°€ì§€ê³  ì—°ì‚°\n",
        "    hidden_state_t = output_t # ê³„ì‚° ê²°ê³¼ëŠ” í˜„ì¬ ì‹œì ì˜ ì€ë‹‰ ìƒíƒœê°€ ëœë‹¤."
      ],
      "metadata": {
        "id": "my3CWhEtXEj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "timesteps = 10\n",
        "input_dim = 4\n",
        "hidden_units = 8\n",
        "\n",
        "# ì…ë ¥ì— í•´ë‹¹ë˜ëŠ” 2D í…ì„œ\n",
        "inputs = np.random.random((timesteps, input_dim))\n",
        "\n",
        "# ì´ˆê¸° ì€ë‹‰ ìƒíƒœëŠ” 0(ë²¡í„°)ë¡œ ì´ˆê¸°í™”\n",
        "hidden_state_t = np.zeros((hidden_units,)) \n",
        "\n",
        "print('ì´ˆê¸° ì€ë‹‰ ìƒíƒœ :',hidden_state_t)"
      ],
      "metadata": {
        "id": "5OhUo-PH72tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Wx = np.random.random((hidden_units, input_dim))  # (8, 4)í¬ê¸°ì˜ 2D í…ì„œ ìƒì„±. ì…ë ¥ì— ëŒ€í•œ ê°€ì¤‘ì¹˜.\n",
        "Wh = np.random.random((hidden_units, hidden_units)) # (8, 8)í¬ê¸°ì˜ 2D í…ì„œ ìƒì„±. ì€ë‹‰ ìƒíƒœì— ëŒ€í•œ ê°€ì¤‘ì¹˜.\n",
        "b = np.random.random((hidden_units,)) # (8,)í¬ê¸°ì˜ 1D í…ì„œ ìƒì„±. ì´ ê°’ì€ í¸í–¥(bias).\n",
        "\n",
        "print('ê°€ì¤‘ì¹˜ Wxì˜ í¬ê¸°(shape) :',np.shape(Wx))\n",
        "print('ê°€ì¤‘ì¹˜ Whì˜ í¬ê¸°(shape) :',np.shape(Wh))\n",
        "print('í¸í–¥ì˜ í¬ê¸°(shape) :',np.shape(b))"
      ],
      "metadata": {
        "id": "qbrJkav774Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_hidden_states = []\n",
        "\n",
        "# ê° ì‹œì  ë³„ ì…ë ¥ê°’.\n",
        "for input_t in inputs:\n",
        "\n",
        "  # Wx * Xt + Wh * Ht-1 + b(bias)\n",
        "  output_t = np.tanh(np.dot(Wx,input_t) + np.dot(Wh,hidden_state_t) + b)\n",
        "\n",
        "  # ê° ì‹œì  të³„ ë©”ëª¨ë¦¬ ì…€ì˜ ì¶œë ¥ì˜ í¬ê¸°ëŠ” (timestep t, output_dim)\n",
        "  # ê° ì‹œì ì˜ ì€ë‹‰ ìƒíƒœì˜ ê°’ì„ ê³„ì†í•´ì„œ ëˆ„ì \n",
        "  total_hidden_states.append(list(output_t))\n",
        "  hidden_state_t = output_t\n",
        "\n",
        "# ì¶œë ¥ ì‹œ ê°’ì„ ê¹”ë”í•˜ê²Œ í•´ì£¼ëŠ” ìš©ë„.\n",
        "total_hidden_states = np.stack(total_hidden_states, axis = 0) \n",
        "\n",
        "# (timesteps, output_dim)\n",
        "print('ëª¨ë“  ì‹œì ì˜ ì€ë‹‰ ìƒíƒœ :')\n",
        "print(total_hidden_states)"
      ],
      "metadata": {
        "id": "2OI9H2Xl75yI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}