{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week5_hw_ì„¸ì˜.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-vPZn15zBHIv",
        "xUWWDwdiPLS9",
        "008-V5QsQG25"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“Œ week5 ë‚´ìš© ì£¼ì°¨ì— í•´ë‹¹ë˜ëŠ” ê³¼ì œëŠ” 3ì£¼ì°¨ì˜ Glove ëª¨ë¸ ì‹¤ìŠµ, 4ì£¼ì°¨ì˜ NER task ì‹¤ìŠµ, 5ì£¼ì°¨ì˜ Dependency Parsing task ì‹¤ìŠµìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (**ì°¸ê³ ** : ì œì¶œì€ week6 branch ë³µìŠµê³¼ì œë¡œ!) \n",
        "\n",
        "ğŸ“Œ ìœ„í‚¤ë…ìŠ¤ì˜ ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ êµì¬ ì‹¤ìŠµ, ìºê¸€ ë…¸íŠ¸ë¶ ë“±ì˜ ìë£Œë¡œ êµ¬ì„±ë˜ì–´ìˆëŠ” ê³¼ì œì…ë‹ˆë‹¤. \n",
        "\n",
        "ğŸ“Œ ì•ˆë‚´ëœ ë§í¬ì— ë§ì¶”ì–´ **ì§ì ‘ ì½”ë“œë¥¼ ë”°ë¼ ì¹˜ë©´ì„œ (í•„ì‚¬)** í•´ë‹¹ nlp task ì˜ ê¸°ë³¸ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë©”ì„œë“œë¥¼ ìˆ™ì§€í•´ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ğŸ˜Š í•„ìˆ˜ë¼ê³  ì²´í¬í•œ ë¶€ë¶„ì€ ê³¼ì œì— ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì£¼ì‹œê³ , ì„ íƒìœ¼ë¡œ ì²´í¬í•œ ë¶€ë¶„ì€ ììœ¨ì ìœ¼ë¡œ ìŠ¤í„°ë”” í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ê¶ê¸ˆí•œ ì‚¬í•­ì€ ê¹ƒí—ˆë¸Œ ì´ìŠˆë‚˜, ì¹´í†¡ë°©, ì„¸ì…˜ ë°œí‘œ ì‹œì‘ ì´ì „ ì‹œê°„ ë“±ì„ í™œìš©í•˜ì—¬ ììœ ë¡­ê²Œ ê³µìœ í•´ì£¼ì„¸ìš”!"
      ],
      "metadata": {
        "id": "QhUHfXkPAORh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk colab í™˜ê²½ì—ì„œ ì‹¤í–‰ì‹œ í•„ìš”í•œ ì½”ë“œì…ë‹ˆë‹¤. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "3XjTSbcxBB6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1ï¸âƒ£ **Glove**\n",
        "\n"
      ],
      "metadata": {
        "id": "-vPZn15zBHIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‘€ **ë‚´ìš© ë³µìŠµ** \n",
        "* ìŠ¤íƒ í¬ë“œ ëŒ€í•™ì—ì„œ ê°œë°œí•œ ì¹´ìš´íŠ¸ ê¸°ë°˜ê³¼ ì˜ˆì¸¡ ê¸°ë°˜ì„ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ ì„ë² ë”© ë°©ë²•ë¡  \n",
        "* word2vec ì˜ ë‹¨ì ì„ ë³´ì™„í•´ì„œ ë‚˜ì˜¨ ëª¨ë¸ \n",
        "* glove model ì˜ **input ì€ ë°˜ë“œì‹œ ë™ì‹œë“±ì¥í–‰ë ¬ í˜•íƒœ**ì—¬ì•¼ í•œë‹¤ â­\n",
        "\n",
        "![1](https://www.dropbox.com/s/nz0ji4yzre56ifv/word_presentation.png?raw=1) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ğŸ¤” í•œêµ­ì–´ ì˜ˆì œëŠ” ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” k-Glove ë¡œ ì†Œê°œë˜ëŠ” ì—°êµ¬ê°€ ìˆê¸´ í•œë°, ì¢€ ë” ì•Œì•„ë´ì•¼ í•  ê²ƒ ê°™ì•„ìš”!\n",
        "\n",
        "â• [ë…¼ë¬¸1](https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=NPAP13255003&dbt=NPAP)\n",
        "\n",
        "\n",
        "â•[ë…¼ë¬¸2](https://scienceon.kisti.re.kr/commons/util/originalView.do?cn=CFKO201832073078664&oCn=NPAP13255064&dbt=CFKO&journal=NPRO00383361&keyword=%ED%95%9C%EA%B5%AD%EC%96%B4%20%EB%8C%80%ED%99%94%20%EC%97%94%EC%A7%84%EC%97%90%EC%84%9C%EC%9D%98%20%EB%AC%B8%EC%9E%A5%EB%B6%84%EB%A5%98)"
      ],
      "metadata": {
        "id": "P11biHcUuBaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ **1-(1)** glove python\n",
        "\n",
        "* [ì‹¤ìŠµ : basic code](https://wikidocs.net/22885) ğŸ‘‰ í•„ìˆ˜"
      ],
      "metadata": {
        "id": "asGcGy6fBM1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install glove_python_binary"
      ],
      "metadata": {
        "id": "hBJb4Vf2BFnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec1eec6-457d-4532-83ac-b5ec6e922394"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting glove_python_binary\n",
            "  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 948 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.4.1)\n",
            "Installing collected packages: glove-python-binary\n",
            "Successfully installed glove-python-binary-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from lxml import etree\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-_bCjghmgqW",
        "outputId": "b933de91-d113-4d71-effb-424d73fe9db7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dUMK-gnp2VI",
        "outputId": "4fb8a519-6363-46a0-d0e3-ff2979d126e8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x7ff61a572750>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "í•´ë‹¹ ë°ì´í„°ëŠ” xml ë¬¸ë²•ìœ¼ë¡œ ë˜ì–´ ìˆì–´ ìì—°ì–´ë¥¼ ì–»ê¸° ìœ„í•´ì„œëŠ” ì „ì²˜ë¦¬ê°€ í•„ìš”í•˜ë‹¤. ì‹¤ì§ˆì ìœ¼ë¡œ í•„ìš”í•œ ë¶€ë¶„ì€ ì˜ì–´ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” ì™€ ì‚¬ì´ì˜ ë‚´ìš©ì´ë‹¤. ì „ì²˜ë¦¬ ì‘ì—…ì„ í†µí•´ xml ë¬¸ë²•ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë¥¼ ì‚­ì œí•´ì•¼í•˜ê³ , (Laughter)ë‚˜ (Applause)ì™€ ê°™ì€ ë°°ê²½ìŒì„ ë‚˜íƒ€ë‚´ëŠ” ë‹¨ì–´ë„ ë“±ì¥í•˜ëŠ” ë° ì´ëŸ° ë‹¨ì–´ë“¤ ë˜í•œ ì œê±°ë˜ì–´ì•¼ í•œë‹¤."
      ],
      "metadata": {
        "id": "-MSeLeSymwiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ë°ì´í„° ì „ì²˜ë¦¬\n",
        "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "target_text = etree.parse(targetXML)\n",
        "\n",
        "# xml íŒŒì¼ë¡œë¶€í„° <content>ì™€ </content> ì‚¬ì´ì˜ ë‚´ìš©ë§Œ ê°€ì ¸ì˜¨ë‹¤.\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# ì •ê·œ í‘œí˜„ì‹ì˜ sub ëª¨ë“ˆì„ í†µí•´ content ì¤‘ê°„ì— ë“±ì¥í•˜ëŠ” (Audio), (Laughter) ë“±ì˜ ë°°ê²½ìŒ ë¶€ë¶„ì„ ì œê±°.\n",
        "# í•´ë‹¹ ì½”ë“œëŠ” ê´„í˜¸ë¡œ êµ¬ì„±ëœ ë‚´ìš©ì„ ì œê±°.\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# ì…ë ¥ ì½”í¼ìŠ¤ì— ëŒ€í•´ì„œ NLTKë¥¼ ì´ìš©í•˜ì—¬ ë¬¸ì¥ í† í°í™”ë¥¼ ìˆ˜í–‰.\n",
        "sent_text = sent_tokenize(content_text)\n",
        "\n",
        "# ê° ë¬¸ì¥ì— ëŒ€í•´ì„œ êµ¬ë‘ì ì„ ì œê±°í•˜ê³ , ëŒ€ë¬¸ìë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜.\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# ê° ë¬¸ì¥ì— ëŒ€í•´ì„œ NLTKë¥¼ ì´ìš©í•˜ì—¬ ë‹¨ì–´ í† í°í™”ë¥¼ ìˆ˜í–‰.\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]"
      ],
      "metadata": {
        "id": "uIRB1DB3qBIB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "corpus = Corpus()\n",
        "\n",
        "#í›ˆë ¨ ë°ì´í„°ë¡œë¶€í„° GloVeì—ì„œ ì‚¬ìš©í•  ë™ì‹± ë“±ì¥ í–‰ë ¬ ìƒì„±\n",
        "corpus.fit(result, window = 5)\n",
        "glove = Glove(no_components= 100, learning_rate = 0.05)\n",
        "\n",
        "# í•™ìŠµì— ì´ìš©í•  ì“°ë ˆë“œì˜ ê°œìˆ˜ëŠ” 4ë¡œ ì„¤ì •, ì—í¬í¬ëŠ” 20.\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHMuzUznpRkh",
        "outputId": "eddb97b9-3958-411b-cccd-00657a53d7f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing 20 training epochs with 4 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "glove.most_similar()ëŠ” ì…ë ¥ ë‹¨ì–´ì˜ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¦¬í„´"
      ],
      "metadata": {
        "id": "WuyCw6j_nZVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(glove.most_similar(\"water\"))\n",
        "print(glove.most_similar(\"physics\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKNT8GJeqq1H",
        "outputId": "d6da7a0f-8553-4fd4-fe63-4bde92bb72ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('clean', 0.8422758803162379), ('air', 0.8292722504344053), ('fresh', 0.8257940480149598), ('food', 0.8147950610055273)]\n",
            "[('chemistry', 0.8939304397042503), ('economics', 0.87753551378986), ('mathematics', 0.8662224685448278), ('beauty', 0.8653894715358409)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ **1-(2)** pre-trained glove \n",
        "\n",
        "* **ì‚¬ì „í•™ìŠµëª¨ë¸** : ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ë˜ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë“¤ì„ ë‹¤ë¥¸ ë¬¸ì œì— í•™ìŠµì‹œí‚¨ ê°€ì¤‘ì¹˜ë“¤ë¡œ ì´ˆê¸°í™”í•˜ëŠ” ë°©ë²•ì´ë‹¤.ì‚¬ì „ í•™ìŠµí•œ ê°€ì¤‘ì¹˜ë¥¼ í™œìš©í•´ í•™ìŠµí•˜ê³ ì í•˜ëŠ” ë³¸ë˜ ë¬¸ì œë¥¼ í•˜ìœ„ë¬¸ì œë¼ê³  í•œë‹¤. \n",
        "\n",
        "* [ì‹¤ìŠµ : ë¬¸ì¥ì˜ ê¸ë¶€ì •ì„ íŒë‹¨í•˜ëŠ” ê°ì„± ë¶„ë¥˜ ëª¨ë¸ ë§Œë“¤ê¸°](https://wikidocs.net/33793) ğŸ‘‰ í•„ìˆ˜\n",
        "  * [ì„¤ëª…ì°¸ê³ ](https://omicro03.medium.com/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-16%EC%9D%BC%EC%B0%A8-pre-trained-word-embedding-bb30db424a35)\n",
        "* pre-trained data ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦¼\n",
        "* kaggle ëŒ€íšŒì—ì„œ ì£¼ë¡œ ì´ ë°©ì‹ì„ ë§ì´ ì‚¬ìš©í•¨\n",
        "  * [ì°¸ê³ ](https://lsjsj92.tistory.com/455)"
      ],
      "metadata": {
        "id": "3ADfVM9lO9NE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-trained GLove "
      ],
      "metadata": {
        "id": "jjEdsoKjr_Up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve, urlopen\n",
        "import gzip\n",
        "import zipfile\n",
        "\n",
        "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
        "zf = zipfile.ZipFile('glove.6B.zip')\n",
        "zf.extractall() \n",
        "zf.close()"
      ],
      "metadata": {
        "id": "i4tfK0zEgoxe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì„ë² ë”© ì¸µ ì‚¬ìš©í•˜ê¸° \n",
        "- ê¸ì •ì ì¸ ë ˆì´ë¸” 1\n",
        "- ë¶€ì •ì ì¸ ë ˆì´ë¸” 0 "
      ],
      "metadata": {
        "id": "EddEtVDrtknF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ì „ì²˜ë¦¬ ë‹¨ê³„"
      ],
      "metadata": {
        "id": "7zXgAscYt6nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
        "y_train = [1, 0, 0, 1, 1, 0, 1]"
      ],
      "metadata": {
        "id": "Dv_ut0CUtZIl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ì¼€ë¼ìŠ¤ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ì§‘í•©ì„ ë§Œë“¤ê³  ê·¸ í¬ê¸°ë¥¼ í™•ì¸\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1 # íŒ¨ë”©ì„ ê³ ë ¤í•˜ì—¬ +1"
      ],
      "metadata": {
        "id": "MUxj0C3VtgoB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ê° ë¬¸ì¥ì— ëŒ€í•´ì„œ ì •ìˆ˜ ì¸ì½”ë”©ì„ ìˆ˜í–‰\n",
        "X_encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print('ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼ :',X_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qot_zLgYpJML",
        "outputId": "fc26b611-8113-4b40-8851-b4980b494e58"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼ : [[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ê°€ì¥ ê¸¸ì´ê°€ ê¸´ ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ êµ¬í•˜ê¸°\n",
        "max_len = max(len(l) for l in X_encoded)\n",
        "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
        "y_train = np.array(y_train)"
      ],
      "metadata": {
        "id": "0tiC7vMMpK9Y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ìµœëŒ€ ê¸¸ì´ë¡œ ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•´ì„œ íŒ¨ë”©ì„ ì§„í–‰\n",
        "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
        "y_train = np.array(y_train)\n",
        "print('íŒ¨ë”© ê²°ê³¼ :')\n",
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mALO9koUpdKq",
        "outputId": "9d28d20c-10ec-456e-9d05-76fb44e046f7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "íŒ¨ë”© ê²°ê³¼ :\n",
            "[[ 1  2  3  4]\n",
            " [ 5  6  0  0]\n",
            " [ 7  8  0  0]\n",
            " [ 9 10  0  0]\n",
            " [11 12  0  0]\n",
            " [13  0  0  0]\n",
            " [14 15  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ì„¤ê³„ "
      ],
      "metadata": {
        "id": "nPnKiGnKuE8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì¶œë ¥ì¸µì— 1ê°œì˜ ë‰´ëŸ°ì„ ë°°ì¹˜í•˜ê³  í™œì„±í™” í•¨ìˆ˜ë¡œëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼, ê·¸ë¦¬ê³  ì†ì‹¤ í•¨ìˆ˜ë¡œ binary_crossentropyë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ í›„ 100 ì—í¬í¬ í•™ìŠµí•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "3Y1-VgeqphKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "embedding_dim = 4\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "metadata": {
        "id": "-Gh4yQTPuJ_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762e362b-76b1-4d45-8021-2d10697bb3a5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 1s - loss: 0.6985 - acc: 0.4286 - 815ms/epoch - 815ms/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.6972 - acc: 0.4286 - 9ms/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.6958 - acc: 0.4286 - 5ms/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.6945 - acc: 0.4286 - 4ms/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.6932 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.6918 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.6905 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.6892 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.6878 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.6865 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.6852 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.6839 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.6826 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.6812 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.6799 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.6786 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.6772 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.6759 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.6745 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.6732 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.6718 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.6705 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.6691 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.6677 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.6663 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.6649 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.6635 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.6621 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.6607 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.6593 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.6578 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.6564 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.6549 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.6534 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.6520 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.6505 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.6490 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.6475 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.6459 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.6444 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.6429 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.6413 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.6398 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.6382 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.6366 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.6350 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.6334 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.6318 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.6302 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.6285 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.6269 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.6252 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.6236 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.6219 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.6202 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.6185 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.6168 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.6151 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.6134 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.6116 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.6099 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.6081 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.6063 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.6046 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.6028 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.6010 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.5992 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.5974 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.5955 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.5937 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.5919 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.5900 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.5881 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.5863 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.5844 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.5825 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.5806 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.5787 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.5768 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.5749 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.5729 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.5710 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.5690 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.5671 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.5651 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.5631 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.5612 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.5592 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.5572 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.5552 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.5532 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.5512 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.5491 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.5471 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.5451 - acc: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.5430 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.5410 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.5389 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.5369 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.5348 - acc: 1.0000 - 7ms/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff587d34810>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "í•™ìŠµ ê³¼ì •ì—ì„œ í˜„ì¬ ê° ë‹¨ì–´ë“¤ì˜ ì„ë² ë”© ë²¡í„°ë“¤ì˜ ê°’ì€ ì¶œë ¥ì¸µì˜ ê°€ì¤‘ì¹˜ì™€ í•¨ê»˜ í•™ìŠµë©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "r4c2CmukpiyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì‚¬ì „í›ˆë ¨ëœ GloVe ì‚¬ìš©í•˜ê¸° "
      ],
      "metadata": {
        "id": "ZFp6WXHzuctW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "glove.6B.zipë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì••ì¶•ì„ í’€ë©´ ë‹¤ìˆ˜ì˜ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ë° ì—¬ê¸°ì„œëŠ” glove.6B.100d.txt íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "VhULqZs1ptCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve, urlopen\n",
        "import gzip\n",
        "import zipfile\n",
        "\n",
        "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
        "zf = zipfile.ZipFile('glove.6B.zip')\n",
        "zf.extractall() \n",
        "zf.close()"
      ],
      "metadata": {
        "id": "CiZRlBzipto0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "glove.6B.100d.txtì— ìˆëŠ” ëª¨ë“  ì„ë² ë”© ë²¡í„°ë“¤ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. íŒŒì´ì¬ì˜ ìë£Œêµ¬ì¡° ë”•ì…”ë„ˆë¦¬(dictionary)ë¥¼ ì‚¬ìš©í•˜ë©°, ë¡œë“œí•œ ì„ë² ë”© ë²¡í„°ì˜ ê°œìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "AnO9IR2cpwIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dict = dict()\n",
        "\n",
        "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "\n",
        "    # 100ê°œì˜ ê°’ì„ ê°€ì§€ëŠ” arrayë¡œ ë³€í™˜\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "\n",
        "print('%sê°œì˜ Embedding vectorê°€ ìˆìŠµë‹ˆë‹¤.' % len(embedding_dict))\n"
      ],
      "metadata": {
        "id": "b1ktg4xggolx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e37c8cc4-de98-48b0-c54b-ad5aca316d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.013786   0.38216    0.53236    0.15261   -0.29694   -0.20558\n",
            " -0.41846   -0.58437   -0.77355   -0.87866   -0.37858   -0.18516\n",
            " -0.128     -0.20584   -0.22925   -0.42599    0.3725     0.26077\n",
            " -1.0702     0.62916   -0.091469   0.70348   -0.4973    -0.77691\n",
            "  0.66045    0.09465   -0.44893    0.018917   0.33146   -0.35022\n",
            " -0.35789    0.030313   0.22253   -0.23236   -0.19719   -0.0053125\n",
            " -0.25848    0.58081   -0.10705   -0.17845   -0.16206    0.087086\n",
            "  0.63029   -0.76649    0.51619    0.14073    1.019     -0.43136\n",
            "  0.46138   -0.43585   -0.47568    0.19226    0.36065    0.78987\n",
            "  0.088945  -2.7814    -0.15366    0.01015    1.1798     0.15168\n",
            " -0.050112   1.2626    -0.77527    0.36031    0.95761   -0.11385\n",
            "  0.28035   -0.02591    0.31246   -0.15424    0.3778    -0.13599\n",
            "  0.2946    -0.31579    0.42943    0.086969   0.019169  -0.27242\n",
            " -0.31696    0.37327    0.61997    0.13889    0.17188    0.30363\n",
            " -1.2776     0.044423  -0.52736   -0.88536   -0.19428   -0.61947\n",
            " -0.10146   -0.26301   -0.061707   0.36627   -0.95223   -0.39346\n",
            " -0.69183   -1.0426     0.28855    0.63056  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ 40ë§Œê°œì˜ ì„ë² ë”© ë²¡í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì„ì˜ì˜ ë‹¨ì–´ 'respectable'ì˜ ì„ë² ë”© ë²¡í„°ê°’ê³¼ í¬ê¸°ë¥¼ ì¶œë ¥í•´ë´…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "hoqILdupp736"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_dict['respectable'])\n",
        "print('ë²¡í„°ì˜ ì°¨ì› ìˆ˜ :',len(embedding_dict['respectable']))"
      ],
      "metadata": {
        "id": "hUBqwYigp8fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë²¡í„°ê°’ì´ ì¶œë ¥ë˜ë©° ë²¡í„°ì˜ ì°¨ì› ìˆ˜ëŠ” 100ì…ë‹ˆë‹¤. í’€ê³ ì í•˜ëŠ” ë¬¸ì œì˜ ë‹¨ì–´ ì§‘í•© í¬ê¸°ì˜ í–‰ê³¼ 100ê°œì˜ ì—´ì„ ê°€ì§€ëŠ” í–‰ë ¬ ìƒì„±í•©ë‹ˆë‹¤. ì´ í–‰ë ¬ì˜ ê°’ì€ ì „ë¶€ 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤. ì´ í–‰ë ¬ì— ì‚¬ì „ í›ˆë ¨ëœ ì„ë² ë”© ê°’ì„ ë„£ì–´ì¤„ ê²ƒì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "JZgOXoKKqIxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "print('ì„ë² ë”© í–‰ë ¬ì˜ í¬ê¸°(shape) :',np.shape(embedding_matrix)"
      ],
      "metadata": {
        "id": "ppUajMutvXJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë‹¨ì–´ ì§‘í•©ì˜ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•´ì„œ ì‚¬ì „ í›ˆë ¨ëœ GloVeì˜ ì„ë² ë”© ë²¡í„°ë“¤ì„ ë§µí•‘í•œ í›„ 'great'ì˜ ë²¡í„°ê°’ì´ ì˜ë„í•œ ì¸ë±ìŠ¤ì˜ ìœ„ì¹˜ì— ì‚½ì…ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "gal4s-_CqQI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in tokenizer.word_index.items():\n",
        "    # ë‹¨ì–´ì™€ ë§µí•‘ë˜ëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì„ë² ë”© ë²¡í„°ê°’\n",
        "    vector_value = embedding_dict.get(word)\n",
        "    if vector_value is not None:\n",
        "        embedding_matrix[index] = vector_value "
      ],
      "metadata": {
        "id": "ljZQ3kefqFcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "embedding_matrixì˜ ì¸ë±ìŠ¤ 2ì—ì„œì˜ ê°’ì„ í™•ì¸í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "NOtOSZmXqTlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eidUoceAsn3c",
        "outputId": "9796312b-ed8e-4e6b-d661-038085c369ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.013786    0.38216001  0.53236002  0.15261    -0.29694    -0.20558\n",
            " -0.41846001 -0.58437002 -0.77354997 -0.87866002 -0.37858    -0.18516\n",
            " -0.12800001 -0.20584001 -0.22925    -0.42598999  0.3725      0.26076999\n",
            " -1.07019997  0.62915999 -0.091469    0.70348001 -0.4973     -0.77691001\n",
            "  0.66044998  0.09465    -0.44893     0.018917    0.33146    -0.35021999\n",
            " -0.35789001  0.030313    0.22253001 -0.23236001 -0.19719    -0.0053125\n",
            " -0.25848001  0.58081001 -0.10705    -0.17845    -0.16205999  0.087086\n",
            "  0.63028997 -0.76648998  0.51618999  0.14072999  1.01900005 -0.43136001\n",
            "  0.46138    -0.43584999 -0.47567999  0.19226     0.36065     0.78987002\n",
            "  0.088945   -2.78139997 -0.15366     0.01015     1.17980003  0.15167999\n",
            " -0.050112    1.26259995 -0.77526999  0.36030999  0.95761001 -0.11385\n",
            "  0.28035    -0.02591     0.31246001 -0.15424     0.37779999 -0.13598999\n",
            "  0.29460001 -0.31579     0.42943001  0.086969    0.019169   -0.27241999\n",
            " -0.31696001  0.37327     0.61997002  0.13889     0.17188001  0.30362999\n",
            " -1.27760005  0.044423   -0.52736002 -0.88536    -0.19428    -0.61947\n",
            " -0.10146    -0.26301    -0.061707    0.36627001 -0.95222998 -0.39346001\n",
            " -0.69182998 -1.04260004  0.28854999  0.63055998]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ **1-(3)** fine tuning glove\n",
        "* ë¯¸ì„¸ì¡°ì • : ì‚¬ì „ í•™ìŠµí•œ ëª¨ë“  ê°€ì¤‘ì¹˜ì™€ ë”ë¶ˆì–´ í•˜ìœ„ ë¬¸ì œë¥¼ ìœ„í•œ ìµœì†Œí•œì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¶”ê°€í•´ ëª¨ë¸ì„ ì¶”ê°€ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ë‹¤. \n",
        "\n",
        "* fine tuning ì´ í•„ìš”í•œ ê²½ìš° \n",
        "  * pretrained model ì— ë°ì´í„°ì…‹ì— ìˆëŠ” ë‹¨ì–´ê°€ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš° \n",
        "  * ë°ì´í„° ì§‘í•©ì´ ë„ˆë¬´ ì‘ì•„ì„œ ì „ì²´ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê¸° ì–´ë ¤ìš´ ê²½ìš° \n",
        "\n",
        "* [Mittens ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ fine tuning](https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39) ğŸ‘‰ í•„ìˆ˜\n",
        "  *  GloVe ì„ë² ë”©ì„ fine-tuning í•˜ê¸° ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "  * [github](https://github.com/roamanalytics/mittens)\n",
        "\n",
        "* [í•œêµ­ì–´ ì†Œì„¤ í…ìŠ¤íŠ¸ ë°ì´í„° ë¯¸ì„¸ì¡°ì • ëª¨ë¸ í•™ìŠµ - GPT2](https://m.blog.naver.com/PostView.nhn?isHttpsRedirect=true&blogId=horajjan&logNo=222104684132&categoryNo=120&proxyReferer=) ğŸ‘‰ ì„ íƒ (glove ëª¨ë¸ ì˜ˆì œëŠ” ì•„ë‹™ë‹ˆë‹¤. fine-tuning ì— ì´ˆì ì„ ë‘ì–´ì„œ ì°¸ê³ í•´ì£¼ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.)"
      ],
      "metadata": {
        "id": "f_wcrE5PtLMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mittens\n",
        "!pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJa7xf-zwaLC",
        "outputId": "7aeff0e8-e419-4ecb-f332-99e467070330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mittens in /usr/local/lib/python3.7/dist-packages (0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mittens) (1.21.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX0ecWn8wmww",
        "outputId": "96fc8c9c-64d6-4d23-847c-5c08eb8ba420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk.corpus import brown\n",
        "from mittens import GloVe, Mittens\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "## Loading pretrained Model \n",
        "def glove2dict(glove_filename):\n",
        "    with open(glove_filename, encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
        "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
        "                for line in reader}\n",
        "    return embed\n",
        "\n",
        "glove_path = \"glove.6B.50d.txt\" # get it from https://nlp.stanford.edu/projects/glove\n",
        "pre_glove = glove2dict(glove_path)\n",
        "\n",
        "## Data pre-processing before building co-occurence matrix\n",
        "sw = list(_stop_words.ENGLISH_STOP_WORDS)\n",
        "brown_data = brown.words()[:200000]\n",
        "brown_nonstop = [token.lower() for token in brown_data if (token.lower() not in sw)]\n",
        "oov = [token for token in brown_nonstop if token not in pre_glove.keys()]\n",
        "\n",
        "def get_rareoov(xdict, val):\n",
        "    return [k for (k,v) in Counter(xdict).items() if v<=val]\n",
        "\n",
        "## optional - use if needed \n",
        "#oov_rare = get_rareoov(oov, 1)\n",
        "#corp_vocab = list(set(oov) - set(oov_rare))\n",
        "#brown_tokens = [token for token in brown_nonstop if token not in oov_rare]\n",
        "#brown_doc = [' '.join(brown_tokens)]\n",
        "\n",
        "corp_vocab = list(set(oov))\n",
        "brown_doc = [' '.join(brown_nonstop)]\n",
        "\n",
        "## Building Co-occurence Matrix \n",
        "cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
        "X = cv.fit_transform(brown_doc)\n",
        "Xc = (X.T * X)\n",
        "Xc.setdiag(0)\n",
        "coocc_ar = Xc.toarray()\n",
        "\n",
        "## Fine-tuning Mittens Model \n",
        "mittens_model = Mittens(n=50, max_iter=1000)\n",
        "\n",
        "new_embeddings = mittens_model.fit(\n",
        "    coocc_ar,\n",
        "    vocab=corp_vocab,\n",
        "    initial_embedding_dict= pre_glove)\n"
      ],
      "metadata": {
        "id": "tW_Z7VULX_i2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c0b8c01-dc7d-43b2-9d56-094c3c1331eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/adagrad.py:139: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 1000: loss: 0.0322125107049942"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newglove = dict(zip(corp_vocab, new_embeddings))\n",
        "f = open(\"repo_glove.pkl\",\"wb\")\n",
        "pickle.dump(newglove, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "x0ALIeATX_e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* (ì°¸ê³ ) word2vec pretrained example\n",
        "\n",
        "â• [word2vec ì‚¬ì „í•™ìŠµ ëª¨ë¸ -í•œêµ­ì–´1](http://doc.mindscale.kr/km/unstructured/11.html)\n",
        "\n",
        "â• [word2vec ì‚¬ì „í•™ìŠµ - í•œêµ­ì–´2](https://monetd.github.io/python/nlp/Word-Embedding-Word2Vec-%EC%8B%A4%EC%8A%B5/#%ED%95%9C%EA%B5%AD%EC%96%B4-word2vec-%EB%A7%8C%EB%93%A4%EA%B8%B0)"
      ],
      "metadata": {
        "id": "I_-OB9Siga3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2ï¸âƒ£ NER**"
      ],
      "metadata": {
        "id": "xUWWDwdiPLS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‘€ **ë‚´ìš© ë³µìŠµ** \n",
        "* ê°œì²´ëª… ì¸ì‹ì„ ì‚¬ìš©í•˜ë©´ ì½”í¼ìŠ¤ë¡œë¶€í„° ì–´ë–¤ ë‹¨ì–´ê°€ ì‚¬ëŒ, ì¥ì†Œ, ì¡°ì§ ë“±ì„ ì˜ë¯¸í•˜ëŠ” ë‹¨ì–´ì¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤. "
      ],
      "metadata": {
        "id": "9N0B4VknPkTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "ğŸ”¹ **2-(1)** NER task by nltk library\n",
        "\n",
        "\n",
        "* nltk ì—ì„œëŠ” ê°œì²´ëª… ì¸ì‹ê¸° (NER chunker) ë¥¼ ì§€ì›í•˜ê³  ìˆë‹¤. \n",
        "* ne_chunk ëŠ” ê°œì²´ëª…ì„ íƒœê¹…í•˜ê¸° ìœ„í•´ì„œ ì•ì„œ í’ˆì‚¬ íƒœê¹… pos_tag ê°€ ìˆ˜í–‰ë˜ì–´ì•¼ í•œë‹¤. \n",
        "\n",
        "\n",
        "ğŸ“Œ [basic code](https://wikidocs.net/30682) ğŸ‘‰ í•„ìˆ˜ \n",
        "\n",
        "ğŸ“Œ [BIO í‘œí˜„, LSTMì„ í™œìš©í•œ NER ì‹¤ìŠµ](https://wikidocs.net/24682) ğŸ‘‰ ì„ íƒ\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QWgla1BuPRqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI9anXwthcTB",
        "outputId": "c7171024-4d4f-46a7-9bd8-3ef691e29766"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "sentence = \"James is working at Disney in London\"\n",
        "# í† í°í™” í›„ í’ˆì‚¬ íƒœê¹…\n",
        "tokenized_sentence = pos_tag(word_tokenize(sentence))\n",
        "print(tokenized_sentence)"
      ],
      "metadata": {
        "id": "diaZweMyAxJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df9e21f-8b36-4021-b90b-6e82da3c40ca"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ê°œì²´ëª… ì¸ì‹\n",
        "ner_sentence = ne_chunk(tokenized_sentence)\n",
        "print(ner_sentence)"
      ],
      "metadata": {
        "id": "1k09tKha3Lgi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f24dbd-a552-4e8b-a073-48ff69f925fe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON James/NNP)\n",
            "  is/VBZ\n",
            "  working/VBG\n",
            "  at/IN\n",
            "  (ORGANIZATION Disney/NNP)\n",
            "  in/IN\n",
            "  (GPE London/NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ **2-(2)** NER task by spacy library\n",
        "\n",
        "\n",
        "* spaCy ëŠ” ìì—°ì–´ì²˜ë¦¬ë¥¼ ìœ„í•œ íŒŒì´ì¬ ê¸°ë°˜ì˜ ì˜¤í”ˆ ì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤. \n",
        "  * Tokenization \n",
        "  * POS tagging \n",
        "  * Lemmatization \n",
        "  * Sentence Boundary Detection (SBD)\n",
        "  * Named Entity Recognition (NER)\n",
        "  * Similarity\n",
        "  * Text Classification\n",
        "  * Rule-based Matching\n",
        "  * Training\n",
        "  * Serialization\n",
        "\n",
        "* spaCy ì™€ NER\n",
        "  * .ents â†’ .label_\n",
        "\n",
        "\n",
        "ğŸ“Œ [basic code](https://frhyme.github.io/python-lib/nlp_spacy_1/) ğŸ‘‰ í•„ìˆ˜ (NER ë¶€ë¶„ë§Œ)\n",
        "\n",
        "ğŸ“Œ [kaggle_Custom NER using SpaCy](https://www.kaggle.com/code/amarsharma768/custom-ner-using-spacy/notebook) ğŸ‘‰ ì„ íƒ\n",
        "\n",
        "  * í›ˆë ¨ë˜ì§€ ì•Šì€ ë°ì´í„° ì„¸íŠ¸ì— ëª…ëª…ëœ ì—”í‹°í‹°ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²• : ì´ë ¥ì„œ pdf ë°ì´í„° í™œìš© \n",
        "  * manually labelled \n",
        "\n",
        "ğŸ“Œ [í•œêµ­ì–´ NER](https://github.com/monologg/KoBERT-NER) ğŸ‘‰ ì°¸ê³ í•˜ë©´ ì¢‹ì„ ìë£Œ\n",
        "\n",
        "â• [ì°¸ê³ ](http://aispiration.com/nlp2/nlp-ner-python.html)"
      ],
      "metadata": {
        "id": "TPX-WtSvPmm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy \n",
        "## ë‹¤ìŒì²˜ëŸ¼ spacyì—ì„œ ë‚´ê°€ ì›í•˜ëŠ” ì–¸ì–´ì˜ ëª¨ë¸ì„ ê°€ì ¸ì˜¤ê³ , \n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "## ë‹¤ìŒì²˜ëŸ¼ ë¬¸ì¥ì„ nlpì— ë„˜ê¸°ê¸°ë§Œ í•˜ë©´ ëë‚©ë‹ˆë‹¤. \n",
        "doc = nlp('Apple is looking at buyin at U.K startup for $1 billion.')\n",
        "print(type(doc)) ## íƒ€ì…ì€, Docê³ , \n",
        "print(doc)## ê·¸ëƒ¥ ì¶œë ¥í•˜ë©´, ì›ë˜ ë¬¸ì¥ì´ ê·¸ëŒ€ë¡œ ë‚˜ì˜¤ê³ , \n",
        "print(list(doc))## ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í˜•í•˜ë©´, tokenizeí•œ ê²°ê³¼ê°€ ë‚˜ì˜¤ê³  \n",
        "print(type(doc[0]))## ë¦¬ìŠ¤íŠ¸ì˜ ê°€ì¥ ì•ì— ìˆëŠ” ê°’ì€ Tokenì´ë¼ëŠ” íƒ€ì…ì´ì£ . "
      ],
      "metadata": {
        "id": "WXjRfz-qP0Xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d30c7b5-5ce3-47b7-cd3a-9dd1fad94574"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'spacy.tokens.doc.Doc'>\n",
            "Apple is looking at buyin at U.K startup for $1 billion.\n",
            "[Apple, is, looking, at, buyin, at, U.K, startup, for, $, 1, billion, .]\n",
            "<class 'spacy.tokens.token.Token'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp('Apple is looking at buying U.K. startup for $1 billion')\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY-rvqSjiCo6",
        "outputId": "3c07340b-06c6-4408-d018-11d13bd59798"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "U.K. GPE\n",
            "$1 billion MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"\"\"But Google is starting from behind. The company made a late push\n",
        "into hardware, and Appleâ€™s Siri, available on iPhones, and Amazonâ€™s Alexa\n",
        "software, which runs on its Echo and Dot devices, have clear leads in\n",
        "consumer adoption.\"\"\".replace(\"\\n\", \" \").strip())\n",
        "\n",
        "## ì•„ë˜ì²˜ëŸ¼ ë¬´ì—‡ì´ organizationì´ê³ , ë¬´ì—‡ì´ productì¸ì§€, ê½¤ ì˜ êµ¬ë³„í•´ì£¼ì§€ë§Œ, \n",
        "## echo, dot ë“±ì— ëŒ€í•´ì„œëŠ” ì •í™•í•˜ì§€ ëª»í•˜ë‹¤. \n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ic40CBDiF5s",
        "outputId": "b6be0dcf-5d61-48ea-fec6-3e6064f2f177"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google ORG\n",
            "Apple ORG\n",
            "Siri PRODUCT\n",
            "Amazon ORG\n",
            "Alexa ORG\n",
            "Echo PRODUCT\n",
            "Dot PRODUCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3ï¸âƒ£ Dependency Parsing**"
      ],
      "metadata": {
        "id": "008-V5QsQG25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‘€ **ë‚´ìš© ë³µìŠµ** \n",
        "* ë¬¸ì¥ì˜ ì „ì²´ì ì¸ êµ¬ì„±/êµ¬ì¡° ë³´ë‹¤ëŠ” ê° ê°œë³„ë‹¨ì–´ ê°„ì˜ 'ì˜ì¡´ê´€ê³„' ë˜ëŠ” 'ìˆ˜ì‹ê´€ê³„' ì™€ ê°™ì€ ë‹¨ì–´ê°„ ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì´ ëª©ì ì¸ NLP Task\n",
        "* ë¬¸ì¥ í•´ì„ì˜ ëª¨í˜¸ì„±ì„ ì—†ì• ê¸° ìœ„í•´ Parsing ì„ í•œë‹¤."
      ],
      "metadata": {
        "id": "oQfcodHQQPlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "ğŸ”¹ **3-(1)** Dependency Parsing by spacy library\n",
        "\n",
        "\n",
        "* [basic](https://frhyme.github.io/python-lib/nlp_spacy_1/#navigating-parse-tree) ğŸ‘‰ dependecy parsing ë¶€ë¶„ë§Œ í•„ìˆ˜\n",
        "* .dep_ ë©”ì„œë“œ\n",
        "\n"
      ],
      "metadata": {
        "id": "mJLAzZnbRNlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì•ì„œ ë¬¸ì¥ì„ tokenizingí•˜ê³  POSì— ë”°ë¼ì„œ ë‚˜ëˆ„ì—ˆìŠµë‹ˆë‹¤. ì´í›„ì—ëŠ” ê°ê°ì˜ tokenë“¤ê°„ì˜ ì˜ì¡´ê´€ê³„ë¥¼ ê³ ë ¤í•˜ì—¬, ê´€ë ¨ìˆëŠ” ë‹¨ì–´ë“¤ì„ ë¬¶ì„ ìˆ˜ ìˆê² ì£ ."
      ],
      "metadata": {
        "id": "9XQdooOFih11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê·¸ëƒ¥, doc.noun_chunksë¥¼ í•˜ë©´, ì•Œì•„ì„œ dependency graphë¥¼ ê³ ë ¤í•˜ì—¬, noun phraseë¥¼ ë½‘ì•„ì¤ë‹ˆë‹¤. ì¶œë ¥ ê°’ì€ generatorì´ê³ , ì´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì„œ ëª¨ë‘ ë¶ˆëŸ¬ì˜¤ë©´ ë˜ê³ , ê°ê°ì€ token í´ë˜ìŠ¤ê°€ ì•„ë‹ˆë¼, span classì…ë‹ˆë‹¤(tokenì˜ ë³µí•©ì–´ ëŠë‚Œì´ì£ )\n",
        "doc = nlp(\"Autonomous cars shift insur"
      ],
      "metadata": {
        "id": "fCC4WPNyijkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "\n",
        "## íŠ¹ì • í…ìŠ¤íŠ¸ë¥¼ nlpì— ë„˜ê¸°ë©´ ëª¨ë‘ í•´ê²°ë˜ê¸°ëŠ” í•˜ëŠ”ë°, \n",
        "## noun_chunksì˜ ê²½ìš°ëŠ” token í´ë˜ìŠ¤ë„ ì•„ë‹ˆê³ , Doc í´ë˜ìŠ¤ë„ ì•„ë‹ˆë‹¤. \n",
        "## Spanì´ë¼ëŠ” í´ë˜ìŠ¤ëŠ” ê·¸ëƒ¥ Docì™€ ë¹„ìŠ·í•˜ë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤, ì¼ì¢…ì˜ ë³µí•©ì–´ ê°œë….\n",
        "noun_chunks = doc.noun_chunks\n",
        "print(type(noun_chunks))\n",
        "noun_chunk = list(noun_chunks)[0]\n",
        "print(type(noun_chunk))\n",
        "token = noun_chunk[0]\n",
        "print(type(token))\n",
        "\n",
        "print(\"==\"*30)\n",
        "print(\"\"\"\n",
        "Text: The original noun chunk text.\n",
        "Root text: The original text of the word connecting the noun chunk to the rest of the parse.\n",
        "Root dep: Dependency relation connecting the root to its head.\n",
        "Root head text: The text of the root token's head.\n",
        "\"\"\".strip())\n",
        "print(\"==\"*30)\n",
        "str_format = \"{:>25}\"*4\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(str_format.format(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text))\n"
      ],
      "metadata": {
        "id": "HbQEYt76bJXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b95233e-1545-4358-dd94-c88059700568"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'generator'>\n",
            "<class 'spacy.tokens.span.Span'>\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "============================================================\n",
            "Text: The original noun chunk text.\n",
            "Root text: The original text of the word connecting the noun chunk to the rest of the parse.\n",
            "Root dep: Dependency relation connecting the root to its head.\n",
            "Root head text: The text of the root token's head.\n",
            "============================================================\n",
            "          Autonomous cars                     cars                    nsubj                    shift\n",
            "      insurance liability                liability                     dobj                    shift\n",
            "            manufacturers            manufacturers                     pobj                   toward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## navigiting parse tree\n",
        "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "for tok in doc:\n",
        "    print(tok.text)\n",
        "    children = list(tok.children)\n",
        "    print('children:', children, 'head:', tok.head if tok.head != tok else \"!this is root node\")\n",
        "    print(\"==\"*16)"
      ],
      "metadata": {
        "id": "W9QAEsrLAxHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8794f5ce-a5dc-4239-dc00-525b1ec11d5b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autonomous\n",
            "children: [] head: cars\n",
            "================================\n",
            "cars\n",
            "children: [Autonomous] head: shift\n",
            "================================\n",
            "shift\n",
            "children: [cars, liability] head: !this is root node\n",
            "================================\n",
            "insurance\n",
            "children: [] head: liability\n",
            "================================\n",
            "liability\n",
            "children: [insurance, toward] head: shift\n",
            "================================\n",
            "toward\n",
            "children: [manufacturers] head: liability\n",
            "================================\n",
            "manufacturers\n",
            "children: [] head: toward\n",
            "================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ê°„ë‹¨í•œ ë„¤íŠ¸ì›Œí¬ë¡œ í‘œí˜„\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "nG = nx.Graph()\n",
        "doc[2] ## root node\n",
        "\n",
        "def add_n_to_g(inputG, tok):\n",
        "    inputG.add_node(tok)\n",
        "    children = list(tok.children)\n",
        "    if children != []:\n",
        "        inputG.add_nodes_from(children)\n",
        "        for c in children:\n",
        "            inputG.add_edges_from([(tok, c, {'dependency':c.dep_})])\n",
        "            add_n_to_g(inputG, c)\n",
        "add_n_to_g(nG, doc[2])\n",
        "print(nG.nodes(data=True))\n",
        "print(\"==\"*20)\n",
        "for e in nG.edges(data=True):\n",
        "    print(f\"{e[0]}, {e[1]}, ### dependency: {e[2]['dependency']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwjkrocPiuZb",
        "outputId": "e8fd8f9a-079e-405d-9c1d-e75d7569fc13"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(shift, {}), (cars, {}), (liability, {}), (Autonomous, {}), (insurance, {}), (toward, {}), (manufacturers, {})]\n",
            "========================================\n",
            "shift, cars, ### dependency: nsubj\n",
            "shift, liability, ### dependency: dobj\n",
            "cars, Autonomous, ### dependency: amod\n",
            "liability, insurance, ### dependency: compound\n",
            "liability, toward, ### dependency: prep\n",
            "toward, manufacturers, ### dependency: pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ **3-(2)** Spacy (kaggle) \n",
        "\n",
        "* ìºê¸€ ë…¸íŠ¸ë¶ í™˜ê²½ì—ì„œ ì‹¤ìŠµí•´ë³´ëŠ” ê²ƒì„ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤!\n",
        "\n",
        "* [kaggle_spaCy](https://www.kaggle.com/code/nirant/hitchhiker-s-guide-to-nlp-in-spacy) ğŸ‘‰ í•„ìˆ˜\n",
        "  * ë„ë‚ ë“œ íŠ¸ëŸ¼í”„ íŠ¸ìœ„í„° íŠ¸ìœ— ë‚´ìš© ë°ì´í„° ë¶„ì„\n",
        "\n",
        "\n",
        "ğŸ‘€ **ë…¸íŠ¸ë¶ í‚¤í¬ì¸íŠ¸** \n",
        "  1. spacy.display ë©”ì„œë“œë¥¼ ì‚¬ìš©í•œ NER ì‹œê°í™” \n",
        "  2. Tagging ì„ í†µí•œ íŠ¸ëŸ¼í”„ íŠ¸ìœ— ë¶„ì„ : noun_chunks ëŠ” dependency graphë¥¼ ê³ ë ¤í•˜ì—¬, noun phraseë¥¼ ë½‘ì•„ì¤€ë‹¤. \n",
        "  3. [spacy Match](https://yujuwon.tistory.com/entry/spaCy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-Rule-based-Matching) : ì§ì ‘ ë¬¸ì¥/ë‹¨ì–´ íŒ¨í„´ì„ ë“±ë¡í•˜ì—¬ parsing\n",
        "  4. Question and answering task using Dependency Parsing\n",
        "    * spacy display :  ``style = 'dep'``\n",
        "    * .dep_\n"
      ],
      "metadata": {
        "id": "XQD5oiGgRfHe"
      }
    }
  ]
}