{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week16_nlp_hw.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "📌 week16 과제는 **15주차의 Word Vector, Word Embedding, ULMfit, BERT**으로 구성되어 있습니다. 트랜스포머는 다음 주에 깊게 다루기 때문에 이번 주차에는 넣지 않았습니다.\n",
        "\n",
        "📌 위키독스의 딥러닝을 이용한 자연어 처리 입문 교재 실습, 관련 블로그 등의 문서 자료로 구성되어 있는 과제입니다. \n",
        "\n",
        "📌 안내된 링크에 맞추어 **직접 코드를 따라 치면서 (필사)** 해당 nlp task 의 기본적인 라이브러리와 메서드를 숙지해보시면 좋을 것 같습니다😊 필수라고 체크한 부분은 과제에 반드시 포함시켜주시고, 선택으로 체크한 부분은 자율적으로 스터디 하시면 됩니다.\n",
        "\n",
        "📌 궁금한 사항은 깃허브 이슈나, 카톡방, 세션 발표 시작 이전 시간 등을 활용하여 자유롭게 공유해주세요!"
      ],
      "metadata": {
        "id": "BX3ac8Ag1RPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🥰 **이하 예제를 실습하시면 됩니다.**\n",
        "\n",
        "1부터 3까지는 필수 과제입니다."
      ],
      "metadata": {
        "id": "Kq8aMYKGPQR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔹 1-(1) Pre-Trained Word Vector\n",
        "\n",
        "* 아래 아티클의 **Case Study: Learning Embeddings from Scratch vs. Pretrained Word Embeddings** 부분을 실습하세요\n",
        "\n",
        "\n",
        "📌 [An Essential Guide to Pretrained Word Embeddings for NLP Practitioners](https://www.analyticsvidhya.com/blog/2020/03/pretrained-word-embeddings-nlp/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eB5XfXsWHBHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "#preparing vocabulary\n",
        "tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#converting text into integer sequences\n",
        "x_tr_seq  = tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq = tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding to prepare sequences of same length\n",
        "x_tr_seq  = pad_sequences(x_tr_seq, maxlen=100)\n",
        "x_val_seq = pad_sequences(x_val_seq, maxlen=100)"
      ],
      "metadata": {
        "id": "dpCBqBpuHACO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#deep learning library\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.callbacks import *\n",
        "\n",
        "model=Sequential()\n",
        "\n",
        "#embedding layer\n",
        "model.add(Embedding(size_of_vocabulary,300,input_length=100,trainable=True)) \n",
        "\n",
        "#lstm layer\n",
        "model.add(LSTM(128,return_sequences=True,dropout=0.2))\n",
        "\n",
        "#Global Maxpooling\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "#Dense Layer\n",
        "model.add(Dense(64,activation='relu')) \n",
        "model.add(Dense(1,activation='sigmoid')) \n",
        "\n",
        "#Add loss function, metrics, optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=[\"acc\"]) \n",
        "\n",
        "#Adding callbacks\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
        "mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
        "\n",
        "#Print summary of model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "F3GvBzJMjEeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading best model\n",
        "from keras.models import load_model\n",
        "model = load_model('best_model.h5')\n",
        "\n",
        "#evaluation \n",
        "_,val_acc = model.evaluate(x_val_seq,y_val, batch_size=128)\n",
        "print(val_acc)"
      ],
      "metadata": {
        "id": "nZ-mXR-sjGV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('../input/glove6b/glove.6B.300d.txt')\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "H1g1WjV6jIXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((size_of_vocabulary, 300))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "ZGX8NDKkjKja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "\n",
        "#embedding layer\n",
        "model.add(Embedding(size_of_vocabulary,300,weights=[embedding_matrix],input_length=100,trainable=False)) \n",
        "\n",
        "#lstm layer\n",
        "model.add(LSTM(128,return_sequences=True,dropout=0.2))\n",
        "\n",
        "#Global Maxpooling\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "#Dense Layer\n",
        "model.add(Dense(64,activation='relu')) \n",
        "model.add(Dense(1,activation='sigmoid')) \n",
        "\n",
        "#Add loss function, metrics, optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=[\"acc\"]) \n",
        "\n",
        "#Adding callbacks\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
        "mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
        "\n",
        "#Print summary of model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "6JIwNFi2jMEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading best model\n",
        "from keras.models import load_model\n",
        "model = load_model('best_model.h5')\n",
        "\n",
        "#evaluation \n",
        "_,val_acc = model.evaluate(x_val_seq,y_val, batch_size=128)\n",
        "print(val_acc)"
      ],
      "metadata": {
        "id": "HqzdqILujOfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔹 1-(2) Word Embeddings\n",
        "\n",
        "* Keras를 이용해 Word Embeddings를 실습해 봅니다. 아래 페이지를 필사해 주시면 됩니다.\n",
        "\n",
        "📌 [Using pre-trained word embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/)"
      ],
      "metadata": {
        "id": "5rVWh1eYPs_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "HftoL18APttz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = keras.utils.get_file(\n",
        "    \"news20.tar.gz\",\n",
        "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
        "    untar=True,\n",
        ")"
      ],
      "metadata": {
        "id": "NmOKg9xCjUEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
        "dirnames = os.listdir(data_dir)\n",
        "print(\"Number of directories:\", len(dirnames))\n",
        "print(\"Directory names:\", dirnames)\n",
        "\n",
        "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
        "print(\"Number of files in comp.graphics:\", len(fnames))\n",
        "print(\"Some example filenames:\", fnames[:5])"
      ],
      "metadata": {
        "id": "-5AWy2rOjWPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(open(data_dir / \"comp.graphics\" / \"38987\").read())"
      ],
      "metadata": {
        "id": "zJi0cCPDjXOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = []\n",
        "labels = []\n",
        "class_names = []\n",
        "class_index = 0\n",
        "for dirname in sorted(os.listdir(data_dir)):\n",
        "    class_names.append(dirname)\n",
        "    dirpath = data_dir / dirname\n",
        "    fnames = os.listdir(dirpath)\n",
        "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
        "    for fname in fnames:\n",
        "        fpath = dirpath / fname\n",
        "        f = open(fpath, encoding=\"latin-1\")\n",
        "        content = f.read()\n",
        "        lines = content.split(\"\\n\")\n",
        "        lines = lines[10:]\n",
        "        content = \"\\n\".join(lines)\n",
        "        samples.append(content)\n",
        "        labels.append(class_index)\n",
        "    class_index += 1\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Number of samples:\", len(samples))"
      ],
      "metadata": {
        "id": "YvuQB45fjYvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the data\n",
        "seed = 1337\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(samples)\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "# Extract a training & validation split\n",
        "validation_split = 0.2\n",
        "num_validation_samples = int(validation_split * len(samples))\n",
        "train_samples = samples[:-num_validation_samples]\n",
        "val_samples = samples[-num_validation_samples:]\n",
        "train_labels = labels[:-num_validation_samples]\n",
        "val_labels = labels[-num_validation_samples:]"
      ],
      "metadata": {
        "id": "e-lxFFTyjaWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
        "vectorizer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "cGRJaeE5jbrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.get_vocabulary()[:5]"
      ],
      "metadata": {
        "id": "mUPP31l-jclR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = vectorizer([[\"the cat sat on the mat\"]])\n",
        "output.numpy()[0, :6]"
      ],
      "metadata": {
        "id": "9DfR5-s-jdgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "metadata": {
        "id": "JZixNo54jev8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "[word_index[w] for w in test]"
      ],
      "metadata": {
        "id": "uexO_KLjjgDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "YqxHnDS8jhMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_glove_file = os.path.join(\n",
        "    os.path.expanduser(\"~\"), \".keras/datasets/glove.6B.100d.txt\"\n",
        ")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "metadata": {
        "id": "RgOjve10jixA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "metadata": {
        "id": "lVLmNdSEjkR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "metadata": {
        "id": "ZY3WzkVFjl-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "model = keras.Model(int_sequences_input, preds)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "rjJcVsfQjns8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
        "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)"
      ],
      "metadata": {
        "id": "Ct-bbIMSjpJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "ZfEVbE25jqkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorizer(string_input)\n",
        "preds = model(x)\n",
        "end_to_end_model = keras.Model(string_input, preds)\n",
        "\n",
        "probabilities = end_to_end_model.predict(\n",
        "    [[\"this message is about computer graphics and 3D modeling\"]]\n",
        ")\n",
        "\n",
        "class_names[np.argmax(probabilities[0])]"
      ],
      "metadata": {
        "id": "AscF6ZFvjsXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔹 2. ELMo Embedding\n",
        "\n",
        "* 아래는 ELMo Embedding의 작동을 \b볼 수 있는 case study 코드입니다. 필사하며 ELMo Embedding을 이해해 보세요!\n",
        "\n",
        "\n",
        "📌 [Elmo Embeddings : A use case study with code](https://medium.com/@errabia.oussama/elmo-embeddings-a-use-case-study-with-code-part-1-e2e5eb7df85c)"
      ],
      "metadata": {
        "id": "s-FBc3jI2kLH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0sLOZxu06Eo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "from allennlp.modules.elmo import Elmo,batch_to_ids\n",
        "import spacy\n",
        "import seaborn as sns\n",
        "import re\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.simplefilter('ignore')\n",
        "\n",
        "\n",
        "# -------LOad train data ---------------------\n",
        "\n",
        "train = pd.read_csv('train.csv', nrows=10000)\n",
        "\n",
        "input_col = 'comment_text'\n",
        "target_col = 'target'\n",
        "CLEAN = True"
      ],
      "metadata": {
        "id": "FKvsDqkejyXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "misspell_dict = {\"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n",
        "                 \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
        "                 \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                 \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n",
        "                 \"i'd\": \"I had\", \"i'll\": \"I will\", \"i'm\": \"I am\", \"isn't\": \"is not\",\n",
        "                 \"it's\": \"it is\", \"it'll\": \"it will\", \"i've\": \"I have\", \"let's\": \"let us\",\n",
        "                 \"mightn't\": \"might not\", \"mustn't\": \"must not\", \"shan't\": \"shall not\",\n",
        "                 \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\",\n",
        "                 \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\",\n",
        "                 \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n",
        "                 \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\",\n",
        "                 \"weren't\": \"were not\", \"we've\": \"we have\", \"what'll\": \"what will\",\n",
        "                 \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "                 \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\",\n",
        "                 \"who're\": \"who are\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                 \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\",\n",
        "                 \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\",\n",
        "                 \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\": \" will\", \"tryin'\": \"trying\"}\n",
        "def _get_misspell(misspell_dict):\n",
        "    misspell_re = re.compile('(%s)' % '|'.join(misspell_dict.keys()))\n",
        "    return misspell_dict, misspell_re\n",
        "\n",
        "\n",
        "def replace_typical_misspell(text):\n",
        "    misspellings, misspellings_re = _get_misspell(misspell_dict)\n",
        "\n",
        "    def replace(match):\n",
        "        return misspellings[match.group(0)]\n",
        "\n",
        "    return misspellings_re.sub(replace, text)\n",
        "\n",
        "\n",
        "# deal with obvuous misspeled word:\n",
        "\n",
        "train[input_col] = train[input_col].apply(lambda x : str(x).lower())\n",
        "train[input_col] = train[input_col].apply(replace_typical_misspell)\n",
        "\n",
        "\n",
        "def clean_text(sentence):\n",
        "    sentence = str(sentence).lower()\n",
        "    sentence = re.sub(\"[^\\w']\",' ',sentence)\n",
        "    sentence = re.sub(' +',' ',sentence)\n",
        "    return sentence\n",
        "\n",
        "spell_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)\n",
        "words = spell_model.index2word\n",
        "w_rank = {}\n",
        "for i,word in enumerate(words):\n",
        "    w_rank[word] = i\n",
        "WORDS = w_rank\n",
        "# Use fast text as vocabulary\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "def P(word):\n",
        "    \"Probability of `word`.\"\n",
        "    # use inverse of rank as proxy\n",
        "    # returns 0 if the word isn't in the dictionary\n",
        "    return - WORDS.get(word, 0)\n",
        "def correction(word):\n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "def candidates(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or [word])\n",
        "def known(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "def edits2(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "def singlify(word):\n",
        "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])\n",
        "\n",
        "\n",
        "\n",
        "if not CLEAN:\n",
        "    class_toxic    = train[train.target>=0.5].comment_text.values.tolist()\n",
        "    class_positive = train[train.target <0.5].comment_text.values.tolist()[:1000]\n",
        "else:\n",
        "    class_toxic = list(map(clean_text,train[train.target >= 0.5].comment_text.values.tolist()))\n",
        "    class_positive = list(map(clean_text,train[train.target < 0.5].comment_text.values.tolist()[:1000]))"
      ],
      "metadata": {
        "id": "OnWQp7Nyj02-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_dict= {}\n",
        "word_index=1\n",
        "lemma_dict ={}\n",
        "word_sequences_toxic= []\n",
        "word_sequences_positive= []\n",
        "\n",
        "toxic_embed   = []\n",
        "positive_embed= []\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg',disable = ['parser','ner','tagger'])\n",
        "docs = nlp.pipe(class_toxic,n_threads = 6)\n",
        "for doc in docs:\n",
        "    word_seq= []\n",
        "    for token in doc:\n",
        "        word_seq.append(token.text)\n",
        "        #if token.text not in word_dict:\n",
        "        #    word_dict[token.text] = word_index\n",
        "        #    word_index+=1\n",
        "        #    lemma_dict[token.text] = token.lemma_\n",
        "        #    word_seq.append(token.text)\n",
        "    word_sequences_toxic.append(word_seq)\n",
        "\n",
        "del spell_model,words,w_rank\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "docs = nlp.pipe(class_positive,n_threads = 6)\n",
        "for doc in docs:\n",
        "    word_seq= []\n",
        "    for token in doc:\n",
        "        word_seq.append(correction(token.text))\n",
        "        #if token.text not in word_dict:\n",
        "        #    word_dict[token.text] = word_index\n",
        "        #    word_index+=1\n",
        "        #    lemma_dict[token.text] = token.lemma_\n",
        "        #    word_seq.append(token.text)\n",
        "    word_sequences_positive.append(word_seq)\n",
        "\n",
        "\n",
        "del nlp,docs"
      ],
      "metadata": {
        "id": "nlzE19vlj2jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "options_file = \"elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
        "weight_file = \"elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
        "\n",
        "elmo = Elmo(options_file,weight_file,1,dropout=0)\n",
        "def elmo_embe_avg(seq_text, top_to_use):\n",
        "    avg_embeds = []\n",
        "    for i in range(10):\n",
        "        if (i*100)>len(seq_text):\n",
        "            break\n",
        "        embds = elmo(batch_to_ids(seq_text[i*100:(i+1)*100]))\n",
        "        for embd, mask in zip(embds['elmo_representations'][0], embds['mask']):\n",
        "            avg_embeds.append( (torch.sum(embd, dim=0) / torch.sum(mask)).detach().numpy())\n",
        "    return avg_embeds\n",
        "\n",
        "class_toxic_embeddings = pd.DataFrame(elmo_embe_avg(word_sequences_toxic,500))\n",
        "class_toxic_embeddings['target'] = 1\n",
        "class_positive_embeddings = pd.DataFrame(elmo_embe_avg(word_sequences_positive,500))\n",
        "class_positive_embeddings['target'] =0\n",
        "data_all = pd.concat((class_toxic_embeddings,class_positive_embeddings))\n",
        "data_all.reset_index(inplace=True,drop=True)"
      ],
      "metadata": {
        "id": "dmogmY2Xj4FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(random_state=1991,n_iter=1500,metric='cosine',n_components=2)\n",
        "tsne.fit(data_all.drop('target',axis=1))\n",
        "\n",
        "embd_tr = tsne.fit_transform(data_all.drop('target',axis=1) )\n",
        "data_all['ts_x_axis'] = embd_tr[:,0]\n",
        "data_all['ts_y_axis'] = embd_tr[:,1]"
      ],
      "metadata": {
        "id": "e0GGrs3cj5UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot('ts_x_axis','ts_y_axis',hue='target',data=data_all)"
      ],
      "metadata": {
        "id": "3NoRmT6Kj6it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔹 3. BERT\n",
        "\n",
        "* 아래 두 예제로 BERT의 작동을 이해해 보세요!\n",
        "\n",
        "📌 [한국어 BERT의 마스크드 언어 모델(Masked Language Model) 실습](https://wikidocs.net/152922)\n",
        "\n",
        "📌  [한국어 BERT의 다음 문장 예측(Next Sentence Prediction)](https://wikidocs.net/156774)\n",
        "\n",
        "--- \n",
        "\n",
        "참고: [버트(Bidirectional Encoder Representations from Transformers, BERT)](https://wikidocs.net/115055)"
      ],
      "metadata": {
        "id": "hbsaYOqdWMFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "bMjBPhFgYZE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForMaskedLM\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "pHp0shMlYZUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForMaskedLM.from_pretrained('klue/bert-base', from_pt=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
      ],
      "metadata": {
        "id": "caEzL8KSkBDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer('축구는 정말 재미있는 [MASK]다.', return_tensors='tf')"
      ],
      "metadata": {
        "id": "GEC2g9e0kCHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs['input_ids'])"
      ],
      "metadata": {
        "id": "ET0_lCKlkC83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.Tensor([[   2 4713 2259 3944 6001 2259    4  809   18    3]], shape=(1, 10), dtype=int32)"
      ],
      "metadata": {
        "id": "iNoy_ss6kD4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs['token_type_ids'])"
      ],
      "metadata": {
        "id": "lz-F4obYkEsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.Tensor([[0 0 0 0 0 0 0 0 0 0]], shape=(1, 10), dtype=int32)"
      ],
      "metadata": {
        "id": "ZNRMZ5fokFpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs['attention_mask'])"
      ],
      "metadata": {
        "id": "jRq3AXx6kGqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.Tensor([[1 1 1 1 1 1 1 1 1 1]], shape=(1, 10), dtype=int32)"
      ],
      "metadata": {
        "id": "9fSuYPFAkHmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import FillMaskPipeline\n",
        "pip = FillMaskPipeline(model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "NsHWTAYJkIgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip('축구는 정말 재미있는 [MASK]다.')"
      ],
      "metadata": {
        "id": "m4akMZgZkJX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip('어벤져스는 정말 재미있는 [MASK]다.')"
      ],
      "metadata": {
        "id": "HuguOR8gkMQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip('나는 오늘 아침에 [MASK]에 출근을 했다.')"
      ],
      "metadata": {
        "id": "c_JaaId0kUyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "uHgVsEOwkX1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertForNextSentencePrediction\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "2Aj3ZW8SkZgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForNextSentencePrediction.from_pretrained('klue/bert-base', from_pt=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
      ],
      "metadata": {
        "id": "gEGWOS56kaim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이어지는 두 개의 문장\n",
        "prompt = \"2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다.\"\n",
        "next_sentence = \"여행을 가보니 한국의 2002년 월드컵 축구대회의 준비는 완벽했습니다.\"\n",
        "encoding = tokenizer(prompt, next_sentence, return_tensors='tf')\n",
        "\n",
        "logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]\n",
        "\n",
        "softmax = tf.keras.layers.Softmax()\n",
        "probs = softmax(logits)\n",
        "print('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())"
      ],
      "metadata": {
        "id": "F8Ba5IKIkbxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 상관없는 두 개의 문장\n",
        "prompt = \"2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다.\"\n",
        "next_sentence = \"극장가서 로맨스 영화를 보고싶어요\"\n",
        "encoding = tokenizer(prompt, next_sentence, return_tensors='tf')\n",
        "\n",
        "logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]\n",
        "\n",
        "softmax = tf.keras.layers.Softmax()\n",
        "probs = softmax(logits)\n",
        "print('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())"
      ],
      "metadata": {
        "id": "7Kb7gWn3kdK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔹 선택과제 \n",
        "\n",
        "\n",
        "📌 [구글 BERT의 마스크드 언어 모델(Masked Language Model) 실습](https://wikidocs.net/153992)\n",
        "\n",
        "📌 [구글 BERT의 다음 문장 예측(Next Sentence Prediction)](https://wikidocs.net/156767)"
      ],
      "metadata": {
        "id": "tkX9sIsE3z3N"
      }
    }
  ]
}