{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week16_nlp_hw.ipynbì˜ ì‚¬ë³¸",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“Œ week16 ê³¼ì œëŠ” **15ì£¼ì°¨ì˜ Word Vector, Word Embedding, ULMfit, BERT**ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë‹¤ìŒ ì£¼ì— ê¹Šê²Œ ë‹¤ë£¨ê¸° ë•Œë¬¸ì— ì´ë²ˆ ì£¼ì°¨ì—ëŠ” ë„£ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ìœ„í‚¤ë…ìŠ¤ì˜ ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ êµì¬ ì‹¤ìŠµ, ê´€ë ¨ ë¸”ë¡œê·¸ ë“±ì˜ ë¬¸ì„œ ìë£Œë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” ê³¼ì œì…ë‹ˆë‹¤. \n",
        "\n",
        "ğŸ“Œ ì•ˆë‚´ëœ ë§í¬ì— ë§ì¶”ì–´ **ì§ì ‘ ì½”ë“œë¥¼ ë”°ë¼ ì¹˜ë©´ì„œ (í•„ì‚¬)** í•´ë‹¹ nlp task ì˜ ê¸°ë³¸ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë©”ì„œë“œë¥¼ ìˆ™ì§€í•´ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ğŸ˜Š í•„ìˆ˜ë¼ê³  ì²´í¬í•œ ë¶€ë¶„ì€ ê³¼ì œì— ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì£¼ì‹œê³ , ì„ íƒìœ¼ë¡œ ì²´í¬í•œ ë¶€ë¶„ì€ ììœ¨ì ìœ¼ë¡œ ìŠ¤í„°ë”” í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ ê¶ê¸ˆí•œ ì‚¬í•­ì€ ê¹ƒí—ˆë¸Œ ì´ìŠˆë‚˜, ì¹´í†¡ë°©, ì„¸ì…˜ ë°œí‘œ ì‹œì‘ ì´ì „ ì‹œê°„ ë“±ì„ í™œìš©í•˜ì—¬ ììœ ë¡­ê²Œ ê³µìœ í•´ì£¼ì„¸ìš”!"
      ],
      "metadata": {
        "id": "BX3ac8Ag1RPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ¥° **ì´í•˜ ì˜ˆì œë¥¼ ì‹¤ìŠµí•˜ì‹œë©´ ë©ë‹ˆë‹¤.**\n",
        "\n",
        "1ë¶€í„° 3ê¹Œì§€ëŠ” í•„ìˆ˜ ê³¼ì œì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "Kq8aMYKGPQR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ğŸ”¹ 1-(1) Pre-Trained Word Vector\n",
        "\n",
        "* ì•„ë˜ ì•„í‹°í´ì˜ **Case Study: Learning Embeddings from Scratch vs. Pretrained Word Embeddings** ë¶€ë¶„ì„ ì‹¤ìŠµí•˜ì„¸ìš”\n",
        "\n",
        "\n",
        "ğŸ“Œ [An Essential Guide to Pretrained Word Embeddings for NLP Practitioners](https://www.analyticsvidhya.com/blog/2020/03/pretrained-word-embeddings-nlp/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eB5XfXsWHBHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "#preparing vocabulary\n",
        "tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#converting text into integer sequences\n",
        "x_tr_seq  = tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq = tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding to prepare sequences of same length\n",
        "x_tr_seq  = pad_sequences(x_tr_seq, maxlen=100)\n",
        "x_val_seq = pad_sequences(x_val_seq, maxlen=100)"
      ],
      "metadata": {
        "id": "dpCBqBpuHACO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#deep learning library\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.callbacks import *\n",
        "\n",
        "model=Sequential()\n",
        "\n",
        "#embedding layer\n",
        "model.add(Embedding(size_of_vocabulary,300,input_length=100,trainable=True)) \n",
        "\n",
        "#lstm layer\n",
        "model.add(LSTM(128,return_sequences=True,dropout=0.2))\n",
        "\n",
        "#Global Maxpooling\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "#Dense Layer\n",
        "model.add(Dense(64,activation='relu')) \n",
        "model.add(Dense(1,activation='sigmoid')) \n",
        "\n",
        "#Add loss function, metrics, optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=[\"acc\"]) \n",
        "\n",
        "#Adding callbacks\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
        "mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
        "\n",
        "#Print summary of model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "F3GvBzJMjEeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading best model\n",
        "from keras.models import load_model\n",
        "model = load_model('best_model.h5')\n",
        "\n",
        "#evaluation \n",
        "_,val_acc = model.evaluate(x_val_seq,y_val, batch_size=128)\n",
        "print(val_acc)"
      ],
      "metadata": {
        "id": "nZ-mXR-sjGV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('../input/glove6b/glove.6B.300d.txt')\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "H1g1WjV6jIXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((size_of_vocabulary, 300))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "ZGX8NDKkjKja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "\n",
        "#embedding layer\n",
        "model.add(Embedding(size_of_vocabulary,300,weights=[embedding_matrix],input_length=100,trainable=False)) \n",
        "\n",
        "#lstm layer\n",
        "model.add(LSTM(128,return_sequences=True,dropout=0.2))\n",
        "\n",
        "#Global Maxpooling\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "#Dense Layer\n",
        "model.add(Dense(64,activation='relu')) \n",
        "model.add(Dense(1,activation='sigmoid')) \n",
        "\n",
        "#Add loss function, metrics, optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=[\"acc\"]) \n",
        "\n",
        "#Adding callbacks\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
        "mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
        "\n",
        "#Print summary of model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "6JIwNFi2jMEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading best model\n",
        "from keras.models import load_model\n",
        "model = load_model('best_model.h5')\n",
        "\n",
        "#evaluation \n",
        "_,val_acc = model.evaluate(x_val_seq,y_val, batch_size=128)\n",
        "print(val_acc)"
      ],
      "metadata": {
        "id": "HqzdqILujOfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ğŸ”¹ 1-(2) Word Embeddings\n",
        "\n",
        "* Kerasë¥¼ ì´ìš©í•´ Word Embeddingsë¥¼ ì‹¤ìŠµí•´ ë´…ë‹ˆë‹¤. ì•„ë˜ í˜ì´ì§€ë¥¼ í•„ì‚¬í•´ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ [Using pre-trained word embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/)"
      ],
      "metadata": {
        "id": "5rVWh1eYPs_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "HftoL18APttz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = keras.utils.get_file(\n",
        "    \"news20.tar.gz\",\n",
        "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
        "    untar=True,\n",
        ")"
      ],
      "metadata": {
        "id": "NmOKg9xCjUEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
        "dirnames = os.listdir(data_dir)\n",
        "print(\"Number of directories:\", len(dirnames))\n",
        "print(\"Directory names:\", dirnames)\n",
        "\n",
        "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
        "print(\"Number of files in comp.graphics:\", len(fnames))\n",
        "print(\"Some example filenames:\", fnames[:5])"
      ],
      "metadata": {
        "id": "-5AWy2rOjWPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(open(data_dir / \"comp.graphics\" / \"38987\").read())"
      ],
      "metadata": {
        "id": "zJi0cCPDjXOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = []\n",
        "labels = []\n",
        "class_names = []\n",
        "class_index = 0\n",
        "for dirname in sorted(os.listdir(data_dir)):\n",
        "    class_names.append(dirname)\n",
        "    dirpath = data_dir / dirname\n",
        "    fnames = os.listdir(dirpath)\n",
        "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
        "    for fname in fnames:\n",
        "        fpath = dirpath / fname\n",
        "        f = open(fpath, encoding=\"latin-1\")\n",
        "        content = f.read()\n",
        "        lines = content.split(\"\\n\")\n",
        "        lines = lines[10:]\n",
        "        content = \"\\n\".join(lines)\n",
        "        samples.append(content)\n",
        "        labels.append(class_index)\n",
        "    class_index += 1\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Number of samples:\", len(samples))"
      ],
      "metadata": {
        "id": "YvuQB45fjYvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the data\n",
        "seed = 1337\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(samples)\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "# Extract a training & validation split\n",
        "validation_split = 0.2\n",
        "num_validation_samples = int(validation_split * len(samples))\n",
        "train_samples = samples[:-num_validation_samples]\n",
        "val_samples = samples[-num_validation_samples:]\n",
        "train_labels = labels[:-num_validation_samples]\n",
        "val_labels = labels[-num_validation_samples:]"
      ],
      "metadata": {
        "id": "e-lxFFTyjaWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
        "vectorizer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "cGRJaeE5jbrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.get_vocabulary()[:5]"
      ],
      "metadata": {
        "id": "mUPP31l-jclR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = vectorizer([[\"the cat sat on the mat\"]])\n",
        "output.numpy()[0, :6]"
      ],
      "metadata": {
        "id": "9DfR5-s-jdgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "metadata": {
        "id": "JZixNo54jev8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "[word_index[w] for w in test]"
      ],
      "metadata": {
        "id": "uexO_KLjjgDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "YqxHnDS8jhMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_glove_file = os.path.join(\n",
        "    os.path.expanduser(\"~\"), \".keras/datasets/glove.6B.100d.txt\"\n",
        ")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "metadata": {
        "id": "RgOjve10jixA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "metadata": {
        "id": "lVLmNdSEjkR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "metadata": {
        "id": "ZY3WzkVFjl-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "model = keras.Model(int_sequences_input, preds)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "rjJcVsfQjns8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
        "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)"
      ],
      "metadata": {
        "id": "Ct-bbIMSjpJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "ZfEVbE25jqkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorizer(string_input)\n",
        "preds = model(x)\n",
        "end_to_end_model = keras.Model(string_input, preds)\n",
        "\n",
        "probabilities = end_to_end_model.predict(\n",
        "    [[\"this message is about computer graphics and 3D modeling\"]]\n",
        ")\n",
        "\n",
        "class_names[np.argmax(probabilities[0])]"
      ],
      "metadata": {
        "id": "AscF6ZFvjsXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ğŸ”¹ 2. ELMo Embedding\n",
        "\n",
        "* ì•„ë˜ëŠ” ELMo Embeddingì˜ ì‘ë™ì„ \bë³¼ ìˆ˜ ìˆëŠ” case study ì½”ë“œì…ë‹ˆë‹¤. í•„ì‚¬í•˜ë©° ELMo Embeddingì„ ì´í•´í•´ ë³´ì„¸ìš”!\n",
        "\n",
        "\n",
        "ğŸ“Œ [Elmo Embeddings : A use case study with code](https://medium.com/@errabia.oussama/elmo-embeddings-a-use-case-study-with-code-part-1-e2e5eb7df85c)"
      ],
      "metadata": {
        "id": "s-FBc3jI2kLH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0sLOZxu06Eo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "from allennlp.modules.elmo import Elmo,batch_to_ids\n",
        "import spacy\n",
        "import seaborn as sns\n",
        "import re\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.simplefilter('ignore')\n",
        "\n",
        "\n",
        "# -------LOad train data ---------------------\n",
        "\n",
        "train = pd.read_csv('train.csv', nrows=10000)\n",
        "\n",
        "input_col = 'comment_text'\n",
        "target_col = 'target'\n",
        "CLEAN = True"
      ],
      "metadata": {
        "id": "FKvsDqkejyXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "misspell_dict = {\"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n",
        "                 \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
        "                 \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                 \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n",
        "                 \"i'd\": \"I had\", \"i'll\": \"I will\", \"i'm\": \"I am\", \"isn't\": \"is not\",\n",
        "                 \"it's\": \"it is\", \"it'll\": \"it will\", \"i've\": \"I have\", \"let's\": \"let us\",\n",
        "                 \"mightn't\": \"might not\", \"mustn't\": \"must not\", \"shan't\": \"shall not\",\n",
        "                 \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\",\n",
        "                 \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\",\n",
        "                 \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n",
        "                 \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\",\n",
        "                 \"weren't\": \"were not\", \"we've\": \"we have\", \"what'll\": \"what will\",\n",
        "                 \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "                 \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\",\n",
        "                 \"who're\": \"who are\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                 \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\",\n",
        "                 \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\",\n",
        "                 \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\": \" will\", \"tryin'\": \"trying\"}\n",
        "def _get_misspell(misspell_dict):\n",
        "    misspell_re = re.compile('(%s)' % '|'.join(misspell_dict.keys()))\n",
        "    return misspell_dict, misspell_re\n",
        "\n",
        "\n",
        "def replace_typical_misspell(text):\n",
        "    misspellings, misspellings_re = _get_misspell(misspell_dict)\n",
        "\n",
        "    def replace(match):\n",
        "        return misspellings[match.group(0)]\n",
        "\n",
        "    return misspellings_re.sub(replace, text)\n",
        "\n",
        "\n",
        "# deal with obvuous misspeled word:\n",
        "\n",
        "train[input_col] = train[input_col].apply(lambda x : str(x).lower())\n",
        "train[input_col] = train[input_col].apply(replace_typical_misspell)\n",
        "\n",
        "\n",
        "def clean_text(sentence):\n",
        "    sentence = str(sentence).lower()\n",
        "    sentence = re.sub(\"[^\\w']\",' ',sentence)\n",
        "    sentence = re.sub(' +',' ',sentence)\n",
        "    return sentence\n",
        "\n",
        "spell_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)\n",
        "words = spell_model.index2word\n",
        "w_rank = {}\n",
        "for i,word in enumerate(words):\n",
        "    w_rank[word] = i\n",
        "WORDS = w_rank\n",
        "# Use fast text as vocabulary\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "def P(word):\n",
        "    \"Probability of `word`.\"\n",
        "    # use inverse of rank as proxy\n",
        "    # returns 0 if the word isn't in the dictionary\n",
        "    return - WORDS.get(word, 0)\n",
        "def correction(word):\n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "def candidates(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or [word])\n",
        "def known(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "def edits2(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "def singlify(word):\n",
        "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])\n",
        "\n",
        "\n",
        "\n",
        "if not CLEAN:\n",
        "    class_toxic    = train[train.target>=0.5].comment_text.values.tolist()\n",
        "    class_positive = train[train.target <0.5].comment_text.values.tolist()[:1000]\n",
        "else:\n",
        "    class_toxic = list(map(clean_text,train[train.target >= 0.5].comment_text.values.tolist()))\n",
        "    class_positive = list(map(clean_text,train[train.target < 0.5].comment_text.values.tolist()[:1000]))"
      ],
      "metadata": {
        "id": "OnWQp7Nyj02-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_dict= {}\n",
        "word_index=1\n",
        "lemma_dict ={}\n",
        "word_sequences_toxic= []\n",
        "word_sequences_positive= []\n",
        "\n",
        "toxic_embed   = []\n",
        "positive_embed= []\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg',disable = ['parser','ner','tagger'])\n",
        "docs = nlp.pipe(class_toxic,n_threads = 6)\n",
        "for doc in docs:\n",
        "    word_seq= []\n",
        "    for token in doc:\n",
        "        word_seq.append(token.text)\n",
        "        #if token.text not in word_dict:\n",
        "        #    word_dict[token.text] = word_index\n",
        "        #    word_index+=1\n",
        "        #    lemma_dict[token.text] = token.lemma_\n",
        "        #    word_seq.append(token.text)\n",
        "    word_sequences_toxic.append(word_seq)\n",
        "\n",
        "del spell_model,words,w_rank\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "docs = nlp.pipe(class_positive,n_threads = 6)\n",
        "for doc in docs:\n",
        "    word_seq= []\n",
        "    for token in doc:\n",
        "        word_seq.append(correction(token.text))\n",
        "        #if token.text not in word_dict:\n",
        "        #    word_dict[token.text] = word_index\n",
        "        #    word_index+=1\n",
        "        #    lemma_dict[token.text] = token.lemma_\n",
        "        #    word_seq.append(token.text)\n",
        "    word_sequences_positive.append(word_seq)\n",
        "\n",
        "\n",
        "del nlp,docs"
      ],
      "metadata": {
        "id": "nlzE19vlj2jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "options_file = \"elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
        "weight_file = \"elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
        "\n",
        "elmo = Elmo(options_file,weight_file,1,dropout=0)\n",
        "def elmo_embe_avg(seq_text, top_to_use):\n",
        "    avg_embeds = []\n",
        "    for i in range(10):\n",
        "        if (i*100)>len(seq_text):\n",
        "            break\n",
        "        embds = elmo(batch_to_ids(seq_text[i*100:(i+1)*100]))\n",
        "        for embd, mask in zip(embds['elmo_representations'][0], embds['mask']):\n",
        "            avg_embeds.append( (torch.sum(embd, dim=0) / torch.sum(mask)).detach().numpy())\n",
        "    return avg_embeds\n",
        "\n",
        "class_toxic_embeddings = pd.DataFrame(elmo_embe_avg(word_sequences_toxic,500))\n",
        "class_toxic_embeddings['target'] = 1\n",
        "class_positive_embeddings = pd.DataFrame(elmo_embe_avg(word_sequences_positive,500))\n",
        "class_positive_embeddings['target'] =0\n",
        "data_all = pd.concat((class_toxic_embeddings,class_positive_embeddings))\n",
        "data_all.reset_index(inplace=True,drop=True)"
      ],
      "metadata": {
        "id": "dmogmY2Xj4FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(random_state=1991,n_iter=1500,metric='cosine',n_components=2)\n",
        "tsne.fit(data_all.drop('target',axis=1))\n",
        "\n",
        "embd_tr = tsne.fit_transform(data_all.drop('target',axis=1) )\n",
        "data_all['ts_x_axis'] = embd_tr[:,0]\n",
        "data_all['ts_y_axis'] = embd_tr[:,1]"
      ],
      "metadata": {
        "id": "e0GGrs3cj5UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot('ts_x_axis','ts_y_axis',hue='target',data=data_all)"
      ],
      "metadata": {
        "id": "3NoRmT6Kj6it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ğŸ”¹ 3. BERT\n",
        "\n",
        "* ì•„ë˜ ë‘ ì˜ˆì œë¡œ BERTì˜ ì‘ë™ì„ ì´í•´í•´ ë³´ì„¸ìš”!\n",
        "\n",
        "ğŸ“Œ [í•œêµ­ì–´ BERTì˜ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸(Masked Language Model) ì‹¤ìŠµ](https://wikidocs.net/152922)\n",
        "\n",
        "ğŸ“Œ  [í•œêµ­ì–´ BERTì˜ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡(Next Sentence Prediction)](https://wikidocs.net/156774)\n",
        "\n",
        "--- \n",
        "\n",
        "ì°¸ê³ : [ë²„íŠ¸(Bidirectional Encoder Representations from Transformers, BERT)](https://wikidocs.net/115055)"
      ],
      "metadata": {
        "id": "hbsaYOqdWMFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "bMjBPhFgYZE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForMaskedLM\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "pHp0shMlYZUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForMaskedLM.from_pretrained('klue/bert-base', from_pt=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
      ],
      "metadata": {
        "id": "caEzL8KSkBDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer('ì¶•êµ¬ëŠ” ì •ë§ ì¬ë¯¸ìˆëŠ” [MASK]ë‹¤.', return_tensors='tf')"
      ],
      "metadata": {
        "id": "GEC2g9e0kCHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs['input_ids'])"
      ],
      "metadata": {
        "id": "ET0_lCKlkC83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.Tensor([[   2 4713 2259 3944 6001 2259    4  809   18    3]], shape=(1, 10), dtype=int32)"
      ],
      "metadata": {
        "id": "iNoy_ss6kD4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs['token_type_ids'])"
      ],
      "metadata": {
        "id": "lz-F4obYkEsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.Tensor([[0 0 0 0 0 0 0 0 0 0]], shape=(1, 10), dtype=int32)"
      ],
      "metadata": {
        "id": "ZNRMZ5fokFpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs['attention_mask'])"
      ],
      "metadata": {
        "id": "jRq3AXx6kGqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.Tensor([[1 1 1 1 1 1 1 1 1 1]], shape=(1, 10), dtype=int32)"
      ],
      "metadata": {
        "id": "9fSuYPFAkHmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import FillMaskPipeline\n",
        "pip = FillMaskPipeline(model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "NsHWTAYJkIgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip('ì¶•êµ¬ëŠ” ì •ë§ ì¬ë¯¸ìˆëŠ” [MASK]ë‹¤.')"
      ],
      "metadata": {
        "id": "m4akMZgZkJX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip('ì–´ë²¤ì ¸ìŠ¤ëŠ” ì •ë§ ì¬ë¯¸ìˆëŠ” [MASK]ë‹¤.')"
      ],
      "metadata": {
        "id": "HuguOR8gkMQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip('ë‚˜ëŠ” ì˜¤ëŠ˜ ì•„ì¹¨ì— [MASK]ì— ì¶œê·¼ì„ í–ˆë‹¤.')"
      ],
      "metadata": {
        "id": "c_JaaId0kUyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "uHgVsEOwkX1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertForNextSentencePrediction\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "2Aj3ZW8SkZgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForNextSentencePrediction.from_pretrained('klue/bert-base', from_pt=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
      ],
      "metadata": {
        "id": "gEGWOS56kaim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ì–´ì§€ëŠ” ë‘ ê°œì˜ ë¬¸ì¥\n",
        "prompt = \"2002ë…„ ì›”ë“œì»µ ì¶•êµ¬ëŒ€íšŒëŠ” ì¼ë³¸ê³¼ ê³µë™ìœ¼ë¡œ ê°œìµœë˜ì—ˆë˜ ì„¸ê³„ì ì¸ í° ì”ì¹˜ì…ë‹ˆë‹¤.\"\n",
        "next_sentence = \"ì—¬í–‰ì„ ê°€ë³´ë‹ˆ í•œêµ­ì˜ 2002ë…„ ì›”ë“œì»µ ì¶•êµ¬ëŒ€íšŒì˜ ì¤€ë¹„ëŠ” ì™„ë²½í–ˆìŠµë‹ˆë‹¤.\"\n",
        "encoding = tokenizer(prompt, next_sentence, return_tensors='tf')\n",
        "\n",
        "logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]\n",
        "\n",
        "softmax = tf.keras.layers.Softmax()\n",
        "probs = softmax(logits)\n",
        "print('ìµœì¢… ì˜ˆì¸¡ ë ˆì´ë¸” :', tf.math.argmax(probs, axis=-1).numpy())"
      ],
      "metadata": {
        "id": "F8Ba5IKIkbxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ìƒê´€ì—†ëŠ” ë‘ ê°œì˜ ë¬¸ì¥\n",
        "prompt = \"2002ë…„ ì›”ë“œì»µ ì¶•êµ¬ëŒ€íšŒëŠ” ì¼ë³¸ê³¼ ê³µë™ìœ¼ë¡œ ê°œìµœë˜ì—ˆë˜ ì„¸ê³„ì ì¸ í° ì”ì¹˜ì…ë‹ˆë‹¤.\"\n",
        "next_sentence = \"ê·¹ì¥ê°€ì„œ ë¡œë§¨ìŠ¤ ì˜í™”ë¥¼ ë³´ê³ ì‹¶ì–´ìš”\"\n",
        "encoding = tokenizer(prompt, next_sentence, return_tensors='tf')\n",
        "\n",
        "logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]\n",
        "\n",
        "softmax = tf.keras.layers.Softmax()\n",
        "probs = softmax(logits)\n",
        "print('ìµœì¢… ì˜ˆì¸¡ ë ˆì´ë¸” :', tf.math.argmax(probs, axis=-1).numpy())"
      ],
      "metadata": {
        "id": "7Kb7gWn3kdK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ğŸ”¹ ì„ íƒê³¼ì œ \n",
        "\n",
        "\n",
        "ğŸ“Œ [êµ¬ê¸€ BERTì˜ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸(Masked Language Model) ì‹¤ìŠµ](https://wikidocs.net/153992)\n",
        "\n",
        "ğŸ“Œ [êµ¬ê¸€ BERTì˜ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡(Next Sentence Prediction)](https://wikidocs.net/156767)"
      ],
      "metadata": {
        "id": "tkX9sIsE3z3N"
      }
    }
  ]
}