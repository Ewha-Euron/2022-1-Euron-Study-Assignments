{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nowionlyseedaylight/2022-1-Euron-Study-Assignments/blob/Week_6/week5_nlp_hw_%EA%B9%80%EB%82%98%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhUHfXkPAORh"
      },
      "source": [
        "📌 week5 내용 주차에 해당되는 과제는 3주차의 Glove 모델 실습, 4주차의 NER task 실습, 5주차의 Dependency Parsing task 실습으로 구성되어 있습니다. (**참고** : 제출은 week6 branch 복습과제로!) \n",
        "\n",
        "📌 위키독스의 딥러닝을 이용한 자연어 처리 입문 교재 실습, 캐글 노트북 등의 자료로 구성되어있는 과제입니다. \n",
        "\n",
        "📌 안내된 링크에 맞추어 **직접 코드를 따라 치면서 (필사)** 해당 nlp task 의 기본적인 라이브러리와 메서드를 숙지해보시면 좋을 것 같습니다😊 필수라고 체크한 부분은 과제에 반드시 포함시켜주시고, 선택으로 체크한 부분은 자율적으로 스터디 하시면 됩니다.\n",
        "\n",
        "📌 궁금한 사항은 깃허브 이슈나, 카톡방, 세션 발표 시작 이전 시간 등을 활용하여 자유롭게 공유해주세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XjTSbcxBB6o",
        "outputId": "b6010e82-c453-46fd-cc27-00e817287a52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "# nltk colab 환경에서 실행시 필요한 코드입니다. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vPZn15zBHIv"
      },
      "source": [
        "### 1️⃣ **Glove**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P11biHcUuBaH"
      },
      "source": [
        "👀 **내용 복습** \n",
        "* 스탠포드 대학에서 개발한 카운트 기반과 예측 기반을 모두 사용하는 단어 임베딩 방법론 \n",
        "* word2vec 의 단점을 보완해서 나온 모델 \n",
        "* glove model 의 **input 은 반드시 동시등장행렬 형태**여야 한다 ⭐\n",
        "\n",
        "![1](https://www.dropbox.com/s/nz0ji4yzre56ifv/word_presentation.png?raw=1) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "🤔 한국어 예제는 없는 것 같습니다. 논문에서는 k-Glove 로 소개되는 연구가 있긴 한데, 좀 더 알아봐야 할 것 같아요!\n",
        "\n",
        "➕ [논문1](https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=NPAP13255003&dbt=NPAP)\n",
        "\n",
        "\n",
        "➕[논문2](https://scienceon.kisti.re.kr/commons/util/originalView.do?cn=CFKO201832073078664&oCn=NPAP13255064&dbt=CFKO&journal=NPRO00383361&keyword=%ED%95%9C%EA%B5%AD%EC%96%B4%20%EB%8C%80%ED%99%94%20%EC%97%94%EC%A7%84%EC%97%90%EC%84%9C%EC%9D%98%20%EB%AC%B8%EC%9E%A5%EB%B6%84%EB%A5%98)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asGcGy6fBM1E"
      },
      "source": [
        "🔹 **1-(1)** glove python\n",
        "\n",
        "* [실습 : basic code](https://wikidocs.net/22885) 👉 필수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V31NoJdu5t3p",
        "outputId": "e7b0b776-e486-40ac-c3a3-18b6d8ad0d76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting glove_python_binary\n",
            "  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 33.4 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51 kB 27.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 61 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 92 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 102 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 112 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 122 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 133 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 143 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 153 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 163 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 174 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 184 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 194 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 204 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 215 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 225 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 235 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 245 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 256 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 266 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 276 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 286 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 296 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 307 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 317 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 327 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 337 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 348 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 358 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 368 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 378 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 389 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 399 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 409 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 419 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 430 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 440 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 450 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 460 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 471 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 481 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 491 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 501 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 512 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 522 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 532 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 542 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 552 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 563 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 573 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 583 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 593 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 604 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 614 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 624 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 634 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 645 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 655 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 665 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 675 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 686 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 696 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 706 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 716 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 727 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 737 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 747 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 757 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 768 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 778 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 788 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 798 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 808 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 819 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 829 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 839 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 849 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 860 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 870 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 880 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 890 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 901 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 911 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 921 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 931 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 942 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 948 kB 25.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.21.5)\n",
            "Installing collected packages: glove-python-binary\n",
            "Successfully installed glove-python-binary-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install glove_python_binary\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from lxml import etree\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "QPO6M4vQ3ddt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 다운로드\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRjk-wy-3vNp",
        "outputId": "c9026f2c-e687-424a-f97c-1638f8b64b95"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x7f3383548050>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "target_text = etree.parse(targetXML)\n",
        "\n",
        "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
        "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
        "sent_text = sent_tokenize(content_text)\n",
        "\n",
        "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]"
      ],
      "metadata": {
        "id": "YKqiLpzm3yut"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ta6QgoKO5uXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "668c2833-2759-448c-fc83-c87babd75a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing 20 training epochs with 4 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n"
          ]
        }
      ],
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "corpus = Corpus() \n",
        "\n",
        "# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\n",
        "corpus.fit(result, window=5)\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "\n",
        "# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(glove.most_similar(\"man\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etrrv4zq4UGt",
        "outputId": "e046382f-b288-49e1-dc1f-063539bde452"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.9581033418704747), ('guy', 0.8818607431986919), ('young', 0.8444182670659356), ('girl', 0.8442028051297421)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(glove.most_similar(\"boy\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_PFrKAL4Uir",
        "outputId": "72ab2df7-818f-4bc4-d162-f160bde5dd1f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('girl', 0.9368075825658816), ('kid', 0.8377182024137665), ('lady', 0.8342948176270489), ('woman', 0.8107560914816183)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(glove.most_similar(\"university\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP25wle24WSL",
        "outputId": "aed465fe-1598-4478-83b2-556601330750"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('harvard', 0.8802987471886451), ('mit', 0.8503521960472746), ('cambridge', 0.8394960743801057), ('stanford', 0.8351756258102443)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(glove.most_similar(\"water\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UBs0Itc4X01",
        "outputId": "6e816393-790b-4570-f3ab-6ab5d1852496"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('fresh', 0.8298173482630411), ('clean', 0.8232735212597403), ('air', 0.821438756628592), ('electricity', 0.8172628506518693)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(glove.most_similar(\"physics\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgMUFwGK4ZUb",
        "outputId": "8c3c19ce-ce3b-483a-9e0a-311fc9fb30eb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('economics', 0.8965688631239828), ('beauty', 0.8901367203054608), ('chemistry', 0.8848410860087903), ('mathematics', 0.8567844440815366)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(glove.most_similar(\"muscle\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EKRoFxK4a75",
        "outputId": "59f3a374-42bf-4fd7-9fcc-41358417efd4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('tissue', 0.8377493144796648), ('nerve', 0.8166217591937872), ('bone', 0.7764990615554421), ('channel', 0.75458661962369)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(glove.most_similar(\"clean\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFDmJwYD4cZZ",
        "outputId": "0d90057b-918b-45e5-befe-01ab8535e6b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('water', 0.8232735212597403), ('wind', 0.8127588239904695), ('fresh', 0.8070736049466734), ('heat', 0.7967777191492716)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ADfVM9lO9NE"
      },
      "source": [
        "🔹 **1-(2)** pre-trained glove \n",
        "\n",
        "* **사전학습모델** : 임의의 값으로 초기화하던 모델의 가중치들을 다른 문제에 학습시킨 가중치들로 초기화하는 방법이다.사전 학습한 가중치를 활용해 학습하고자 하는 본래 문제를 하위문제라고 한다. \n",
        "\n",
        "* [실습 : 문장의 긍부정을 판단하는 감성 분류 모델 만들기](https://wikidocs.net/33793) 👉 필수\n",
        "  * [설명참고](https://omicro03.medium.com/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-16%EC%9D%BC%EC%B0%A8-pre-trained-word-embedding-bb30db424a35)\n",
        "* pre-trained data 를 가져오는데 시간이 오래걸림\n",
        "* kaggle 대회에서 주로 이 방식을 많이 사용함\n",
        "  * [참고](https://lsjsj92.tistory.com/455)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
        "y_train = [1, 0, 0, 1, 1, 0, 1]"
      ],
      "metadata": {
        "id": "IDIP3hPk4pw8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1 # 패딩을 고려하여 +1\n",
        "print('단어 집합 :',vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__e-0Llg4rqq",
        "outputId": "a033d524-62fc-4663-a365-2332c98757cc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 : 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print('정수 인코딩 결과 :',X_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KlCPYUR4ur7",
        "outputId": "5cba45ee-9a0d-482e-cc7f-3613a00b81a2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 결과 : [[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(l) for l in X_encoded)\n",
        "print('최대 길이 :',max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUvgtLON4wlb",
        "outputId": "a1e1dcbb-024f-4462-cac1-2f82d7c2061f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최대 길이 : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
        "y_train = np.array(y_train)\n",
        "print('패딩 결과 :')\n",
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzEMm4MR4yJM",
        "outputId": "89401ecc-ec3f-4c80-c0a2-9f0469e3c643"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "패딩 결과 :\n",
            "[[ 1  2  3  4]\n",
            " [ 5  6  0  0]\n",
            " [ 7  8  0  0]\n",
            " [ 9 10  0  0]\n",
            " [11 12  0  0]\n",
            " [13  0  0  0]\n",
            " [14 15  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "embedding_dim = 4\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGPCilK24z19",
        "outputId": "844ca141-f8cd-46b0-db42-59f3c74276b3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 1s - loss: 0.6885 - acc: 0.5714 - 703ms/epoch - 703ms/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.6871 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.6858 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.6845 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.6831 - acc: 0.7143 - 5ms/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.6818 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.6804 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.6791 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.6778 - acc: 0.7143 - 5ms/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.6764 - acc: 0.7143 - 3ms/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.6751 - acc: 0.7143 - 5ms/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.6737 - acc: 0.7143 - 3ms/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.6724 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.6710 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.6697 - acc: 0.7143 - 5ms/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.6683 - acc: 0.8571 - 3ms/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.6670 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.6656 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.6643 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.6629 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.6615 - acc: 0.8571 - 3ms/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.6601 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.6588 - acc: 0.8571 - 3ms/epoch - 3ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.6574 - acc: 0.8571 - 3ms/epoch - 3ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.6560 - acc: 0.8571 - 3ms/epoch - 3ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.6546 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.6532 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.6517 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.6503 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.6489 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.6474 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.6460 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.6445 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.6431 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.6416 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.6401 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.6386 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.6371 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.6356 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.6341 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.6325 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.6310 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.6294 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.6279 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.6263 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.6247 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.6231 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.6215 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.6199 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.6183 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.6166 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.6150 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.6133 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.6117 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.6100 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.6083 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.6066 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.6049 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.6032 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.6014 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.5997 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.5979 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.5962 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.5944 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.5926 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.5908 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.5890 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.5872 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.5854 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.5836 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.5817 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.5799 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.5780 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.5762 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.5743 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.5724 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.5705 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.5686 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.5667 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.5648 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.5629 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.5609 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.5590 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.5570 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.5551 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.5531 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.5511 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.5491 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.5471 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.5451 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.5431 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.5411 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.5391 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.5371 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.5351 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.5330 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.5310 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.5289 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.5269 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.5248 - acc: 1.0000 - 3ms/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f32f1b1f350>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnaSi31y41lw",
        "outputId": "8ecbe84f-8ccd-4c13-8ebb-a6bc0aa77464"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  2  3  4]\n",
            " [ 5  6  0  0]\n",
            " [ 7  8  0  0]\n",
            " [ 9 10  0  0]\n",
            " [11 12  0  0]\n",
            " [13  0  0  0]\n",
            " [14 15  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxsCLcdE425I",
        "outputId": "3f6789b2-2683-49cf-cf5c-7b10d23901fc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve, urlopen\n",
        "import gzip\n",
        "import zipfile\n",
        "\n",
        "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
        "zf = zipfile.ZipFile('glove.6B.zip')\n",
        "zf.extractall() \n",
        "zf.close()"
      ],
      "metadata": {
        "id": "v6lDXi2N44Pk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dict = dict()\n",
        "\n",
        "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "\n",
        "    # 100개의 값을 가지는 array로 변환\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "\n",
        "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1oLreMj46KO",
        "outputId": "e5e62492-cb1b-4377-c755-a4f262548e66"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400000개의 Embedding vector가 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_dict['respectable'])\n",
        "print('벡터의 차원 수 :',len(embedding_dict['respectable']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dcI29tj4716",
        "outputId": "65e99d84-3058-4f3e-e3f0-d9af4fd77d57"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.049773   0.19903    0.10585    0.1391    -0.32395    0.44053\n",
            "  0.3947    -0.22805   -0.25793    0.49768    0.15384   -0.08831\n",
            "  0.0782    -0.8299    -0.037788   0.16772   -0.45197   -0.17085\n",
            "  0.74756    0.98256    0.81872    0.28507    0.16178   -0.48626\n",
            " -0.006265  -0.92469   -0.30625   -0.067318  -0.046762  -0.76291\n",
            " -0.0025264 -0.018795   0.12882   -0.52457    0.3586     0.43119\n",
            " -0.89477   -0.057421  -0.53724    0.25587    0.55195    0.44698\n",
            " -0.24252    0.29946    0.25776   -0.8717     0.68426   -0.05688\n",
            " -0.1848    -0.59352   -0.11227   -0.57692   -0.013593   0.18488\n",
            " -0.32507   -0.90171    0.17672    0.075601   0.54896   -0.21488\n",
            " -0.54018   -0.45882   -0.79536    0.26331    0.18879   -0.16363\n",
            "  0.3975     0.1099     0.1164    -0.083499   0.50159    0.35802\n",
            "  0.25677    0.088546   0.42108    0.28674   -0.71285   -0.82915\n",
            "  0.15297   -0.82712    0.022112   1.067     -0.31776    0.1211\n",
            " -0.069755  -0.61327    0.27308   -0.42638   -0.085084  -0.17694\n",
            " -0.0090944  0.1109     0.62543   -0.23682   -0.44928   -0.3667\n",
            " -0.21616   -0.19187   -0.032502   0.38025  ]\n",
            "벡터의 차원 수 : 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "print('임베딩 행렬의 크기(shape) :',np.shape(embedding_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgsGNU-f49WO",
        "outputId": "2bb105ab-9172-42d0-8c58-96aead9d8dae"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩 행렬의 크기(shape) : (16, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6Jht8-L4-jY",
        "outputId": "16a2806b-04ca-48de-a150-ee7e8547bb15"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('nice', 1), ('great', 2), ('best', 3), ('amazing', 4), ('stop', 5), ('lies', 6), ('pitiful', 7), ('nerd', 8), ('excellent', 9), ('work', 10), ('supreme', 11), ('quality', 12), ('bad', 13), ('highly', 14), ('respectable', 15)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 great의 맵핑된 정수 :',tokenizer.word_index['great'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAw1FKiv4_6t",
        "outputId": "a79ee93a-c05d-4751-8b31-21a419c245f2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 great의 맵핑된 정수 : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_dict['great'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO-04D025BTF",
        "outputId": "3c6c430e-4a96-407a-ee06-e26145226393"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.013786   0.38216    0.53236    0.15261   -0.29694   -0.20558\n",
            " -0.41846   -0.58437   -0.77355   -0.87866   -0.37858   -0.18516\n",
            " -0.128     -0.20584   -0.22925   -0.42599    0.3725     0.26077\n",
            " -1.0702     0.62916   -0.091469   0.70348   -0.4973    -0.77691\n",
            "  0.66045    0.09465   -0.44893    0.018917   0.33146   -0.35022\n",
            " -0.35789    0.030313   0.22253   -0.23236   -0.19719   -0.0053125\n",
            " -0.25848    0.58081   -0.10705   -0.17845   -0.16206    0.087086\n",
            "  0.63029   -0.76649    0.51619    0.14073    1.019     -0.43136\n",
            "  0.46138   -0.43585   -0.47568    0.19226    0.36065    0.78987\n",
            "  0.088945  -2.7814    -0.15366    0.01015    1.1798     0.15168\n",
            " -0.050112   1.2626    -0.77527    0.36031    0.95761   -0.11385\n",
            "  0.28035   -0.02591    0.31246   -0.15424    0.3778    -0.13599\n",
            "  0.2946    -0.31579    0.42943    0.086969   0.019169  -0.27242\n",
            " -0.31696    0.37327    0.61997    0.13889    0.17188    0.30363\n",
            " -1.2776     0.044423  -0.52736   -0.88536   -0.19428   -0.61947\n",
            " -0.10146   -0.26301   -0.061707   0.36627   -0.95223   -0.39346\n",
            " -0.69183   -1.0426     0.28855    0.63056  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in tokenizer.word_index.items():\n",
        "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
        "    vector_value = embedding_dict.get(word)\n",
        "    if vector_value is not None:\n",
        "        embedding_matrix[index] = vector_value"
      ],
      "metadata": {
        "id": "20AjaqLi5C07"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHWBfh0i5EOB",
        "outputId": "3ff10e22-c800-4d47-dd7e-2e1b5a2317dc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.013786  ,  0.38216001,  0.53236002,  0.15261   , -0.29694   ,\n",
              "       -0.20558   , -0.41846001, -0.58437002, -0.77354997, -0.87866002,\n",
              "       -0.37858   , -0.18516   , -0.12800001, -0.20584001, -0.22925   ,\n",
              "       -0.42598999,  0.3725    ,  0.26076999, -1.07019997,  0.62915999,\n",
              "       -0.091469  ,  0.70348001, -0.4973    , -0.77691001,  0.66044998,\n",
              "        0.09465   , -0.44893   ,  0.018917  ,  0.33146   , -0.35021999,\n",
              "       -0.35789001,  0.030313  ,  0.22253001, -0.23236001, -0.19719   ,\n",
              "       -0.0053125 , -0.25848001,  0.58081001, -0.10705   , -0.17845   ,\n",
              "       -0.16205999,  0.087086  ,  0.63028997, -0.76648998,  0.51618999,\n",
              "        0.14072999,  1.01900005, -0.43136001,  0.46138   , -0.43584999,\n",
              "       -0.47567999,  0.19226   ,  0.36065   ,  0.78987002,  0.088945  ,\n",
              "       -2.78139997, -0.15366   ,  0.01015   ,  1.17980003,  0.15167999,\n",
              "       -0.050112  ,  1.26259995, -0.77526999,  0.36030999,  0.95761001,\n",
              "       -0.11385   ,  0.28035   , -0.02591   ,  0.31246001, -0.15424   ,\n",
              "        0.37779999, -0.13598999,  0.29460001, -0.31579   ,  0.42943001,\n",
              "        0.086969  ,  0.019169  , -0.27241999, -0.31696001,  0.37327   ,\n",
              "        0.61997002,  0.13889   ,  0.17188001,  0.30362999, -1.27760005,\n",
              "        0.044423  , -0.52736002, -0.88536   , -0.19428   , -0.61947   ,\n",
              "       -0.10146   , -0.26301   , -0.061707  ,  0.36627001, -0.95222998,\n",
              "       -0.39346001, -0.69182998, -1.04260004,  0.28854999,  0.63055998])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "output_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, output_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryB3Yenr5F1H",
        "outputId": "47f97a57-57df-43a5-8487-3b66cf32406f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 0s - loss: 0.7064 - acc: 0.4286 - 472ms/epoch - 472ms/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.6875 - acc: 0.5714 - 8ms/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.6692 - acc: 0.7143 - 7ms/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.6516 - acc: 0.7143 - 5ms/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.6346 - acc: 0.7143 - 6ms/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.6181 - acc: 0.7143 - 5ms/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.6022 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.5869 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.5721 - acc: 0.7143 - 5ms/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.5577 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.5439 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.5304 - acc: 0.8571 - 6ms/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.5174 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.5048 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.4925 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.4806 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.4691 - acc: 0.8571 - 3ms/epoch - 3ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.4578 - acc: 0.8571 - 7ms/epoch - 7ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.4469 - acc: 0.8571 - 7ms/epoch - 7ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.4363 - acc: 0.8571 - 6ms/epoch - 6ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.4260 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.4160 - acc: 0.8571 - 3ms/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.4063 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.3968 - acc: 0.8571 - 6ms/epoch - 6ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.3876 - acc: 0.8571 - 7ms/epoch - 7ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.3787 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.3700 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.3615 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.3533 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.3453 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.3375 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.3300 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.3226 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.3155 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.3086 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.3018 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.2953 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.2889 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.2827 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.2767 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.2709 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.2652 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.2597 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.2544 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.2492 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.2441 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.2392 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.2345 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.2298 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.2253 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.2210 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.2167 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.2126 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.2086 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.2047 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.2009 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.1972 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.1936 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.1901 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.1867 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.1834 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.1801 - acc: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.1770 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.1740 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.1710 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.1681 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.1653 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.1625 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.1598 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.1572 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.1547 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.1522 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.1498 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.1474 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.1451 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.1429 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.1407 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.1385 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.1364 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.1344 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.1324 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.1305 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.1286 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.1267 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.1249 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.1231 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.1214 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.1197 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.1180 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.1164 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.1148 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.1133 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.1118 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.1103 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.1088 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.1074 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.1060 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.1046 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.1033 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.1020 - acc: 1.0000 - 7ms/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f32eb1c7d50>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\n",
        "                           filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "print('모델의 크기(shape) :',word2vec_model.vectors.shape) # 모델의 크기 확인"
      ],
      "metadata": {
        "id": "sgV9OdPB5HQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "print('임베딩 행렬의 크기(shape) :',np.shape(embedding_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aCTWKiX5I7k",
        "outputId": "9b952dba-64ca-46d0-b792-a93896f5550d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩 행렬의 크기(shape) : (16, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector(word):\n",
        "    if word in word2vec_model:\n",
        "        return word2vec_model[word]\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "ip1Z4WvH5LIf"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in tokenizer.word_index.items():\n",
        "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
        "    vector_value = get_vector(word)\n",
        "    if vector_value is not None:\n",
        "        embedding_matrix[index] = vector_value"
      ],
      "metadata": {
        "id": "yoidY3035L3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2vec_model['nice'])"
      ],
      "metadata": {
        "id": "V_9kn0eB5Oei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 nice의 맵핑된 정수 :', tokenizer.word_index['nice'])"
      ],
      "metadata": {
        "id": "CemcC53Z5O7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_matrix[1])"
      ],
      "metadata": {
        "id": "7pCJadPG5R6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, Input\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(max_len,), dtype='int32'))\n",
        "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWZsBlpI5UUv",
        "outputId": "93b97a56-962c-4fde-f7c0-b2f530a4d0ab"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 0s - loss: 0.6931 - acc: 0.4286 - 430ms/epoch - 430ms/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.6931 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.6930 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.6929 - acc: 0.5714 - 7ms/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.6929 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.6928 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.6927 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.6927 - acc: 0.5714 - 9ms/epoch - 9ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.6926 - acc: 0.5714 - 8ms/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.6925 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.6924 - acc: 0.5714 - 9ms/epoch - 9ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.6924 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.6923 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.6922 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.6922 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.6921 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.6920 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.6920 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.6919 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.6918 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.6918 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.6917 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.6916 - acc: 0.5714 - 3ms/epoch - 3ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.6916 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.6915 - acc: 0.5714 - 3ms/epoch - 3ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.6915 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.6914 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.6913 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.6913 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.6912 - acc: 0.5714 - 9ms/epoch - 9ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.6911 - acc: 0.5714 - 3ms/epoch - 3ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.6911 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.6910 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.6909 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.6909 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.6908 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.6908 - acc: 0.5714 - 9ms/epoch - 9ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.6907 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.6906 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.6906 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.6905 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.6905 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.6904 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.6904 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.6903 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.6902 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.6902 - acc: 0.5714 - 7ms/epoch - 7ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.6901 - acc: 0.5714 - 12ms/epoch - 12ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.6901 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.6900 - acc: 0.5714 - 8ms/epoch - 8ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.6900 - acc: 0.5714 - 8ms/epoch - 8ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.6899 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.6898 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.6898 - acc: 0.5714 - 13ms/epoch - 13ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.6897 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.6897 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.6896 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.6896 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.6895 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.6895 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.6894 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.6894 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.6893 - acc: 0.5714 - 7ms/epoch - 7ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.6893 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.6892 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.6892 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.6891 - acc: 0.5714 - 9ms/epoch - 9ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.6891 - acc: 0.5714 - 8ms/epoch - 8ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.6890 - acc: 0.5714 - 7ms/epoch - 7ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.6890 - acc: 0.5714 - 7ms/epoch - 7ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.6889 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.6889 - acc: 0.5714 - 7ms/epoch - 7ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.6888 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.6888 - acc: 0.5714 - 10ms/epoch - 10ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.6887 - acc: 0.5714 - 7ms/epoch - 7ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.6887 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.6886 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.6886 - acc: 0.5714 - 7ms/epoch - 7ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.6885 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.6885 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.6884 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.6884 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.6883 - acc: 0.5714 - 9ms/epoch - 9ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.6883 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.6882 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.6882 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.6882 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.6881 - acc: 0.5714 - 10ms/epoch - 10ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.6881 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.6880 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.6880 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.6879 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.6879 - acc: 0.5714 - 8ms/epoch - 8ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.6879 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.6878 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.6878 - acc: 0.5714 - 7ms/epoch - 7ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.6877 - acc: 0.5714 - 8ms/epoch - 8ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.6877 - acc: 0.5714 - 6ms/epoch - 6ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.6876 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.6876 - acc: 0.5714 - 5ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f32e8b4e910>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_wcrE5PtLMI"
      },
      "source": [
        "🔹 **1-(3)** fine tuning glove\n",
        "* 미세조정 : 사전 학습한 모든 가중치와 더불어 하위 문제를 위한 최소한의 가중치를 추가해 모델을 추가로 학습하는 방법이다. \n",
        "\n",
        "* fine tuning 이 필요한 경우 \n",
        "  * pretrained model 에 데이터셋에 있는 단어가 포함되지 않은 경우 \n",
        "  * 데이터 집합이 너무 작아서 전체 모델을 훈련시키기 어려운 경우 \n",
        "\n",
        "* [Mittens 라이브러리로 fine tuning](https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39) 👉 필수\n",
        "  *  GloVe 임베딩을 fine-tuning 하기 위한 파이썬 라이브러리\n",
        "  * [github](https://github.com/roamanalytics/mittens)\n",
        "\n",
        "* [한국어 소설 텍스트 데이터 미세조정 모델 학습 - GPT2](https://m.blog.naver.com/PostView.nhn?isHttpsRedirect=true&blogId=horajjan&logNo=222104684132&categoryNo=120&proxyReferer=) 👉 선택 (glove 모델 예제는 아닙니다. fine-tuning 에 초점을 두어서 참고해주시면 좋을 것 같습니다.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mittens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x848mRYG9G9Y",
        "outputId": "42e567b2-0e15-421b-aa24-5fe72be66fe7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mittens\n",
            "  Downloading mittens-0.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mittens) (1.21.5)\n",
            "Installing collected packages: mittens\n",
            "Successfully installed mittens-0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYPbQWzO9jqT",
        "outputId": "5c7f7bdd-9566-4cd3-a382-db8ee6dfdf89"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk.corpus import brown\n",
        "from mittens import GloVe, Mittens\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "6S7H7P4C8j7w"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "xDBDn64S58U5"
      },
      "outputs": [],
      "source": [
        "def glove2dict(glove_filename):\n",
        "    with open(glove_filename, encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter=' ',quoting=csv.QUOTE_NONE)\n",
        "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
        "                for line in reader}\n",
        "    return embed\n",
        "glove_path = \"glove.6B.50d.txt\"\n",
        "pre_glove = glove2dict(glove_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "sw = list(_stop_words.ENGLISH_STOP_WORDS)\n",
        "brown_data = brown.words()[:200000]\n",
        "brown_nonstop = [token.lower() for token in brown_data if (token.lower() not in sw)]\n",
        "oov = [token for token in brown_nonstop if token not in pre_glove.keys()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgB79bP68VgZ",
        "outputId": "6303fab6-7437-4889-9a1f-d8c664afcbe2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "FHiR5mN4577l"
      },
      "outputs": [],
      "source": [
        "def get_rareoov(xdict, val):\n",
        "    return [k for (k,v) in Counter(xdict).items() if v<=val]\n",
        "oov_rare = get_rareoov(oov, 1)\n",
        "corp_vocab = list(set(oov) - set(oov_rare))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brown_tokens = [token for token in brown_nonstop if token not in oov_rare]\n",
        "brown_doc = [' '.join(brown_tokens)]\n",
        "corp_vocab = list(set(oov))"
      ],
      "metadata": {
        "id": "bpDAp3jq8YYj"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
        "X = cv.fit_transform(brown_doc)\n",
        "Xc = (X.T * X)\n",
        "Xc.setdiag(0)\n",
        "coocc_ar = Xc.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pzGDS_x8ZcT",
        "outputId": "0bdf4bb6-e483-41f1-92f1-49215a98b954"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mittens_model = Mittens(n=50, max_iter=1000)\n",
        "new_embeddings = mittens_model.fit(\n",
        "    coocc_ar,\n",
        "    vocab=corp_vocab,\n",
        "    initial_embedding_dict= pre_glove)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_5uzVot8cB5",
        "outputId": "c9b64055-15d9-45c3-94cf-d48a87d0aeae"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/adagrad.py:139: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 1000: loss: 0.004322037100791931"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "GI6SuLRj-nxZ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newglove = dict(zip(corp_vocab, new_embeddings))\n",
        "f = open(\"repo_glove.pkl\",\"wb\")\n",
        "pickle.dump(newglove, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "vq6XqEiS8gLB"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_-OB9Siga3G"
      },
      "source": [
        "* (참고) word2vec pretrained example\n",
        "\n",
        "➕ [word2vec 사전학습 모델 -한국어1](http://doc.mindscale.kr/km/unstructured/11.html)\n",
        "\n",
        "➕ [word2vec 사전학습 - 한국어2](https://monetd.github.io/python/nlp/Word-Embedding-Word2Vec-%EC%8B%A4%EC%8A%B5/#%ED%95%9C%EA%B5%AD%EC%96%B4-word2vec-%EB%A7%8C%EB%93%A4%EA%B8%B0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUWWDwdiPLS9"
      },
      "source": [
        "### **2️⃣ NER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N0B4VknPkTk"
      },
      "source": [
        "👀 **내용 복습** \n",
        "* 개체명 인식을 사용하면 코퍼스로부터 어떤 단어가 사람, 장소, 조직 등을 의미하는 단어인지를 찾을 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWgla1BuPRqJ"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "🔹 **2-(1)** NER task by nltk library\n",
        "\n",
        "\n",
        "* nltk 에서는 개체명 인식기 (NER chunker) 를 지원하고 있다. \n",
        "* ne_chunk 는 개체명을 태깅하기 위해서 앞서 품사 태깅 pos_tag 가 수행되어야 한다. \n",
        "\n",
        "\n",
        "📌 [basic code](https://wikidocs.net/30682) 👉 필수 \n",
        "\n",
        "📌 [BIO 표현, LSTM을 활용한 NER 실습](https://wikidocs.net/24682) 👉 선택\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "diaZweMyAxJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e964452-0040-4603-e702-86451ca01e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\n"
          ]
        }
      ],
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "sentence = \"James is working at Disney in London\"\n",
        "# 토큰화 후 품사 태깅\n",
        "tokenized_sentence = pos_tag(word_tokenize(sentence))\n",
        "print(tokenized_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "1k09tKha3Lgi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d01dd1-0e69-448d-d32b-3abbed2186aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON James/NNP)\n",
            "  is/VBZ\n",
            "  working/VBG\n",
            "  at/IN\n",
            "  (ORGANIZATION Disney/NNP)\n",
            "  in/IN\n",
            "  (GPE London/NNP))\n"
          ]
        }
      ],
      "source": [
        "# 개체명 인식\n",
        "ner_sentence = ne_chunk(tokenized_sentence)\n",
        "print(ner_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPX-WtSvPmm6"
      },
      "source": [
        "🔹 **2-(2)** NER task by spacy library\n",
        "\n",
        "\n",
        "* spaCy 는 자연어처리를 위한 파이썬 기반의 오픈 소스 라이브러리로 다음과 같은 기능을 제공한다. \n",
        "  * Tokenization \n",
        "  * POS tagging \n",
        "  * Lemmatization \n",
        "  * Sentence Boundary Detection (SBD)\n",
        "  * Named Entity Recognition (NER)\n",
        "  * Similarity\n",
        "  * Text Classification\n",
        "  * Rule-based Matching\n",
        "  * Training\n",
        "  * Serialization\n",
        "\n",
        "* spaCy 와 NER\n",
        "  * .ents → .label_\n",
        "\n",
        "\n",
        "📌 [basic code](https://frhyme.github.io/python-lib/nlp_spacy_1/) 👉 필수 (NER 부분만)\n",
        "\n",
        "📌 [kaggle_Custom NER using SpaCy](https://www.kaggle.com/code/amarsharma768/custom-ner-using-spacy/notebook) 👉 선택\n",
        "\n",
        "  * 훈련되지 않은 데이터 세트에 명명된 엔티티를 학습하는 방법 : 이력서 pdf 데이터 활용 \n",
        "  * manually labelled \n",
        "\n",
        "📌 [한국어 NER](https://github.com/monologg/KoBERT-NER) 👉 참고하면 좋을 자료\n",
        "\n",
        "➕ [참고](http://aispiration.com/nlp2/nlp-ner-python.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "WXjRfz-qP0Xx"
      },
      "outputs": [],
      "source": [
        "#conda install -c conda-forge spacy\n",
        "#python -m spacy download en\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp('Apple is looking at buying U.K. startup for $1 billion')\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaNkTMlL_Vku",
        "outputId": "cf8684e1-7a55-43a2-ceba-a7d7397646a0"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "U.K. GPE\n",
            "$1 billion MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"\"\"But Google is starting from behind. The company made a late push\n",
        "into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa\n",
        "software, which runs on its Echo and Dot devices, have clear leads in\n",
        "consumer adoption.\"\"\".replace(\"\\n\", \" \").strip())\n",
        "\n",
        "## 아래처럼 무엇이 organization이고, 무엇이 product인지, 꽤 잘 구별해주지만, \n",
        "## echo, dot 등에 대해서는 정확하지 못하다. \n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNAhr6ba_tbz",
        "outputId": "0d7fae2c-c53d-4274-b5ef-310d4162795d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google ORG\n",
            "Apple ORG\n",
            "Siri PRODUCT\n",
            "Amazon ORG\n",
            "Alexa ORG\n",
            "Echo PRODUCT\n",
            "Dot PRODUCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "008-V5QsQG25"
      },
      "source": [
        "###**3️⃣ Dependency Parsing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQfcodHQQPlt"
      },
      "source": [
        "👀 **내용 복습** \n",
        "* 문장의 전체적인 구성/구조 보다는 각 개별단어 간의 '의존관계' 또는 '수식관계' 와 같은 단어간 관계를 파악하는 것이 목적인 NLP Task\n",
        "* 문장 해석의 모호성을 없애기 위해 Parsing 을 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJLAzZnbRNlL"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "🔹 **3-(1)** Dependency Parsing by spacy library\n",
        "\n",
        "\n",
        "* [basic](https://frhyme.github.io/python-lib/nlp_spacy_1/#navigating-parse-tree) 👉 dependecy parsing 부분만 필수\n",
        "* .dep_ 메서드\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "HbQEYt76bJXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0c8c5ba-dc13-4bc9-89d0-9cdeb7ae48ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'generator'>\n",
            "<class 'spacy.tokens.span.Span'>\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "============================================================\n",
            "Text: The original noun chunk text.\n",
            "Root text: The original text of the word connecting the noun chunk to the rest of the parse.\n",
            "Root dep: Dependency relation connecting the root to its head.\n",
            "Root head text: The text of the root token's head.\n",
            "============================================================\n",
            "          Autonomous cars                     cars                    nsubj                    shift\n",
            "      insurance liability                liability                     dobj                    shift\n",
            "            manufacturers            manufacturers                     pobj                   toward\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "\n",
        "## 특정 텍스트를 nlp에 넘기면 모두 해결되기는 하는데, \n",
        "## noun_chunks의 경우는 token 클래스도 아니고, Doc 클래스도 아니다. \n",
        "## Span이라는 클래스는 그냥 Doc와 비슷하다고 생각하면 된다, 일종의 복합어 개념.\n",
        "noun_chunks = doc.noun_chunks\n",
        "print(type(noun_chunks))\n",
        "noun_chunk = list(noun_chunks)[0]\n",
        "print(type(noun_chunk))\n",
        "token = noun_chunk[0]\n",
        "print(type(token))\n",
        "\n",
        "print(\"==\"*30)\n",
        "print(\"\"\"\n",
        "Text: The original noun chunk text.\n",
        "Root text: The original text of the word connecting the noun chunk to the rest of the parse.\n",
        "Root dep: Dependency relation connecting the root to its head.\n",
        "Root head text: The text of the root token's head.\n",
        "\"\"\".strip())\n",
        "print(\"==\"*30)\n",
        "str_format = \"{:>25}\"*4\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(str_format.format(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "W9QAEsrLAxHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42557b83-df5d-43b1-894b-796506e4247d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autonomous\n",
            "children: [] head: cars\n",
            "================================\n",
            "cars\n",
            "children: [Autonomous] head: shift\n",
            "================================\n",
            "shift\n",
            "children: [cars, liability] head: !this is root node\n",
            "================================\n",
            "insurance\n",
            "children: [] head: liability\n",
            "================================\n",
            "liability\n",
            "children: [insurance, toward] head: shift\n",
            "================================\n",
            "toward\n",
            "children: [manufacturers] head: liability\n",
            "================================\n",
            "manufacturers\n",
            "children: [] head: toward\n",
            "================================\n"
          ]
        }
      ],
      "source": [
        "## navigiting parse tree\n",
        "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "for tok in doc:\n",
        "    print(tok.text)\n",
        "    children = list(tok.children)\n",
        "    print('children:', children, 'head:', tok.head if tok.head != tok else \"!this is root node\")\n",
        "    print(\"==\"*16)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "nG = nx.Graph()\n",
        "doc[2] ## root node\n",
        "\n",
        "def add_n_to_g(inputG, tok):\n",
        "    inputG.add_node(tok)\n",
        "    children = list(tok.children)\n",
        "    if children != []:\n",
        "        inputG.add_nodes_from(children)\n",
        "        for c in children:\n",
        "            inputG.add_edges_from([(tok, c, {'dependency':c.dep_})])\n",
        "            add_n_to_g(inputG, c)\n",
        "add_n_to_g(nG, doc[2])\n",
        "print(nG.nodes(data=True))\n",
        "print(\"==\"*20)\n",
        "for e in nG.edges(data=True):\n",
        "    print(f\"{e[0]}, {e[1]}, ### dependency: {e[2]['dependency']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoyBGoBq_-50",
        "outputId": "829e9cf1-2367-4353-a327-8ff2cf333b33"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(shift, {}), (cars, {}), (liability, {}), (Autonomous, {}), (insurance, {}), (toward, {}), (manufacturers, {})]\n",
            "========================================\n",
            "shift, cars, ### dependency: nsubj\n",
            "shift, liability, ### dependency: dobj\n",
            "cars, Autonomous, ### dependency: amod\n",
            "liability, insurance, ### dependency: compound\n",
            "liability, toward, ### dependency: prep\n",
            "toward, manufacturers, ### dependency: pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQD5oiGgRfHe"
      },
      "source": [
        "🔹 **3-(2)** Spacy (kaggle) \n",
        "\n",
        "* 캐글 노트북 환경에서 실습해보는 것을 권장드립니다!\n",
        "\n",
        "* [kaggle_spaCy](https://www.kaggle.com/code/nirant/hitchhiker-s-guide-to-nlp-in-spacy) 👉 필수\n",
        "  * 도날드 트럼프 트위터 트윗 내용 데이터 분석\n",
        "\n",
        "\n",
        "👀 **노트북 키포인트** \n",
        "  1. spacy.display 메서드를 사용한 NER 시각화 \n",
        "  2. Tagging 을 통한 트럼프 트윗 분석 : noun_chunks 는 dependency graph를 고려하여, noun phrase를 뽑아준다. \n",
        "  3. [spacy Match](https://yujuwon.tistory.com/entry/spaCy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-Rule-based-Matching) : 직접 문장/단어 패턴을 등록하여 parsing\n",
        "  4. Question and answering task using Dependency Parsing\n",
        "    * spacy display :  ``style = 'dep'``\n",
        "    * .dep_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuHGKITRbKYq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMArOUrxAJ8K"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xUWWDwdiPLS9",
        "008-V5QsQG25"
      ],
      "name": "week5_nlp_hw_김나현.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}