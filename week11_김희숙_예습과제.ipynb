{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week11_ê¹€í¬ìˆ™_ì˜ˆìŠµê³¼ì œ.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# í•œêµ­ì–´ ë¬¸ìž¥ ê´€ê³„ ë¶„ë¥˜ ê²½ì§„ëŒ€íšŒ í•„ì‚¬"
      ],
      "metadata": {
        "id": "gFTktAuI4E7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Œ ì „ì œ (premise) ë¬¸ìž¥ì„ ì°¸ê³ í•´ ê°€ì„¤ (hypothesis) ì´ ì°¸/ê±°ì§“/ì•Œìˆ˜ ì—†ëŠ” ë¬¸ìž¥ì¸ì§€ íŒë³„í•˜ëŠ” ë¬¸ì œ â†’ í•œêµ­ì–´ ë¬¸ìž¥ ìŒ ë¶„ì„ ëª¨ë¸ ê°œë°œ (Natural language inference) ì— ê´€í•œ ëŒ€íšŒ\n",
        "- BERT ëª¨ë¸ì—ì„œ ë‚˜ì•„ê°€, ì—°ì‚°ëŸ‰ ì¸¡ë©´ì—ì„œ ê°œì„ ì„ ë³´ì¸ ELECTRA ëª¨ë¸ê³¼ RoBERT ëª¨ë¸ì„ í™œìš©í•œ ë…¸íŠ¸ë¶ì´ ìƒìœ„ê¶Œ ìˆ˜ìƒì„ ì°¨ì§€\n"
      ],
      "metadata": {
        "id": "ribzIUip5zLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1ë“± ë…¸íŠ¸ë¶"
      ],
      "metadata": {
        "id": "nhD1vbG74H3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- PublicLB | 0.896 | Finetuning Electra with Arcface\n",
        "- https://dacon.io/competitions/official/235875/codeshare/4589?page=1&dtype=recent"
      ],
      "metadata": {
        "id": "Onpns_1Y4KDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keypoint**\n",
        "-  2ê°œì˜ ëª¨ë¸ ì‚¬ìš©; ë‘ ëª¨ë¸ ê²°ê³¼ê°’ì„ Softvoting Ensemble\n",
        "    - (1) Tunib's KoElectra-base finetuned with Arcface Head(Public LB 0.896)\n",
        "    - (2) KLUE Roberta-large finetuned with sentence pooling embeddings and special token embeddings(Public LB 0.902)\n",
        "- KoElectra-baseëŠ” #paramsìœ¼ë¡œ ë”°ì§€ë©´ Roberta-largeì˜ 1/3 ê·œëª¨ì˜ ìž‘ì€ ëª¨ë¸ë¡œ, Attention Layers ìˆ˜ë¡œ ë”°ì§€ë©´ Roberta-largeì˜ ì ˆë°˜ ì •ë„ ê¹Šì´ ëª¨ë¸\n"
      ],
      "metadata": {
        "id": "xQh_WoBp6AtC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8-iMUFamg4k"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install adamp\n",
        "!pip install wandb\n",
        "!pip install transformers\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" define train data and test data path \"\"\"\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# environment variable settings\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "# path definitions\n",
        "ROOT_PATH = os.path.abspath(\".\")\n",
        "TRAIN_FILE_PATH = os.path.join(ROOT_PATH, \"train_dataset_v6_sentiment_and_score_pseudo_labeled_pororo_added.csv\")\n",
        "TEST_FILE_PATH = os.path.join(ROOT_PATH, 'test_dataset_v6_sentiment_and_score_pseudo_labeled_pororo_added.csv')\n",
        "SAMPLE_SUBMISSION_PATH = os.path.join(ROOT_PATH, 'sample_submission.csv')"
      ],
      "metadata": {
        "id": "qsTcORpJlBam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Set configuration as dictionary format \"\"\"\n",
        "\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "from easydict import EasyDict\n",
        "\n",
        "# login wandb and get today's date until hour and minute\n",
        "wandb.login()\n",
        "\n",
        "# CFG Configuration\n",
        "CFG = wandb.config # wandb.config provides functionality of easydict.EasyDict\n",
        "CFG.DEBUG = False\n",
        "\n",
        "# Dataset Config as constants\n",
        "CFG.num_labels = 3\n",
        "CFG.num_workers = 2\n",
        "\n",
        "# Train configuration\n",
        "CFG.user_name = \"snoop2head\"\n",
        "CFG.model_name = \"tunib/electra-ko-base\"\n",
        "\n",
        "# Electra Paper's Hyperparameter for Finetuning GLUE details at Table 7: https://arxiv.org/pdf/2003.10555.pdf\n",
        "CFG.learning_rate = 1e-4\n",
        "CFG.adam_epsilon = 1e-6\n",
        "CFG.weight_decay = 0\n",
        "CFG.num_epochs = 10 \n",
        "CFG.train_batch_size = 32\n",
        "CFG.val_batch_size = CFG.train_batch_size\n",
        "CFG.dropout_rate = 0.1\n",
        "\n",
        "# However, while observing evaluation loss and f1 score, the following parameter resulted in better performance for KLUE NLI dataset.\n",
        "# Partially referenced from KRElectra's KorNLI finetuning Hyperparams: https://github.com/snunlp/KR-ELECTRA/blob/master/finetune/config/kornli/kr-electra.json#L15-L31\n",
        "CFG.learning_rate = 4e-5\n",
        "CFG.adam_epsilon = 1e-8\n",
        "CFG.weight_decay = 1e-2\n",
        "CFG.num_folds = 6 # 1st fold: Movie Review, 2nd fold: Airbnb Review, 3 ~ 6th fold: Other datasets\n",
        "CFG.num_epochs = 20\n",
        "CFG.train_batch_size = 3 * 40\n",
        "CFG.val_batch_size = CFG.train_batch_size"
      ],
      "metadata": {
        "id": "F7P2GMftlBYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def seed_everything(seed) :\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "EFicj8y-lBVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_to_num(label):\n",
        "    label_dict = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2}\n",
        "    num_label = []\n",
        "\n",
        "    for v in label:\n",
        "        num_label.append(label_dict[v])\n",
        "    \n",
        "    return num_label\n"
      ],
      "metadata": {
        "id": "Z76fQUw-lBRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# get tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
        "\n",
        "added_special_tokens = [\"[PREMISE]\", \"[HYPOTHESIS]\"]\n",
        "\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\":added_special_tokens})\n",
        "\n",
        "len(tokenizer) # 32002"
      ],
      "metadata": {
        "id": "mffdHfbnlBOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "\n",
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def literal_eval_without_error(input_string):\n",
        "    # string [0.2765555, 0.7233189, 6.404926e-05, 6.1491264e-05] into list [0.2765555, 0.7233189, 6.404926e-05, 6.1491264e-05]\n",
        "    input_string = input_string[1:-1]\n",
        "    input_list = input_string.split(\",\")\n",
        "    output_list = [item.strip() for item in input_list]\n",
        "    output_list = [float(item) for item in input_list]\n",
        "    return output_list\n",
        "\n",
        "# read pororo dataset\n",
        "df_pororo_dataset = pd.read_csv(TRAIN_FILE_PATH) \n",
        "display(df_pororo_dataset.head(3))\n",
        "\n",
        "df_pororo_dataset[\"electra_premise_probability\"] = df_pororo_dataset[\"electra_premise_probability\"].apply(literal_eval_without_error)\n",
        "df_pororo_dataset[\"electra_hypothesis_probability\"] = df_pororo_dataset[\"electra_hypothesis_probability\"].apply(literal_eval_without_error)\n",
        "df_pororo_dataset[\"electra_premise_score_probability\"] = df_pororo_dataset[\"electra_premise_score_probability\"].apply(literal_eval_without_error)\n",
        "df_pororo_dataset[\"electra_hypothesis_score_probability\"] = df_pororo_dataset[\"electra_hypothesis_score_probability\"].apply(literal_eval_without_error)"
      ],
      "metadata": {
        "id": "OSgBIcE2lBMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_augmentation_electra(\n",
        "    df:pd.DataFrame, \n",
        "    str_premise_start_special_token = \"^\", \n",
        "    str_premise_end_special_token = \"^\", \n",
        "    str_hypothesis_start_special_token = \"*\",\n",
        "    str_hypothesis_end_special_token = \"*\",\n",
        "    is_test=False\n",
        "    ):\n",
        "  CLS_TOKEN_LEN = 1\n",
        "  SEP_TOKEN_LEN = 1\n",
        "\n",
        "  premise_list = []\n",
        "  hypothesis_list = []\n",
        "  label_list = []\n",
        "  index_list = []\n",
        "\n",
        "  # wrap premise sentence with additional hintings and special tokens\n",
        "  df_return = pd.DataFrame({})\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    if is_test:\n",
        "      index = row['index']\n",
        "    else:\n",
        "      pass\n",
        "    premise = row['premise']\n",
        "    hypothesis = row['hypothesis']\n",
        "    label = row['label']\n",
        "    \n",
        "    # source(or categorical) information\n",
        "    source_label = row['source_label']\n",
        "    source_prefix = row['source_label'].split(\"/\")[0]\n",
        "    source_subfix = row['source_label'].split(\"/\")[1]\n",
        "    \n",
        "    if source_label == \"ì˜í™”í›„ê¸°/ì˜í™”í‰ê°€\" or source_label == \"ì—¬í–‰í›„ê¸°/ì—¬í–‰í‰ê°€\":\n",
        "      \n",
        "      # make pororo sentiment labels (ê¸ì •ì  vs ë¶€ì •ì )\n",
        "      premise_sentiment = row[\"premise_sentiment\"] # pororo label\n",
        "      hypothesis_sentiment = row[\"hypothesis_sentiment\"]\n",
        "\n",
        "      electra_premise_sentiment = row[\"electra_premise_label\"] # electra label\n",
        "      electra_hypothesis_sentiment = row[\"electra_hypothesis_label\"]\n",
        "\n",
        "      # if two sentiment labels don't agree then use neutral sentiment label, \n",
        "      if premise_sentiment == electra_premise_sentiment:\n",
        "        pass\n",
        "      else:\n",
        "        premise_sentiment = \"ì¤‘ë¦½ì \"\n",
        "      \n",
        "      if hypothesis_sentiment == electra_hypothesis_sentiment:\n",
        "        pass\n",
        "      else:\n",
        "        hypothesis_sentiment = \"ì¤‘ë¦½ì \"\n",
        "      \n",
        "\n",
        "      # use naver shopping based scoreing\n",
        "      pororo_premise_score = row[\"premise_score\"]\n",
        "      electra_premise_score = np.dot([1,2,4,5], row[\"electra_premise_score_probability\"]) # score weighted sum\n",
        "      premise_score = pororo_premise_score * 0.5 + electra_premise_score * 0.5\n",
        "      premise_score =  '{:.2f}'.format(round(premise_score, 2))\n",
        "      \n",
        "      pororo_hypothesis_score = row[\"hypothesis_score\"]\n",
        "      electra_hypothesis_score = np.dot([1,2,4,5], row[\"electra_hypothesis_score_probability\"]) # score weighted sum\n",
        "      hypothesis_score = pororo_hypothesis_score * 0.5 + electra_hypothesis_score * 0.5\n",
        "      hypothesis_score = '{:.2f}'.format(round(hypothesis_score, 2))\n",
        "      \n",
        "      # create span + sentence\n",
        "      premise = str_premise_start_special_token + \\\n",
        "                str(premise_score) + \"ì  ë§Œí¼ \" + premise_sentiment + \"ì¸ \" + \\\n",
        "                source_prefix + \":\" + premise + \\\n",
        "                str_premise_end_special_token\n",
        "      hypothesis = str_hypothesis_start_special_token + \\\n",
        "                str(hypothesis_score) + \"ì  ë§Œí¼ \" + hypothesis_sentiment + \"ì¸ \" + \\\n",
        "                source_subfix + \":\" + hypothesis + \\\n",
        "                str_hypothesis_end_special_token\n",
        "\n",
        "    else:\n",
        "      premise = str_premise_start_special_token + \\\n",
        "                source_prefix + \":\" + premise + \\\n",
        "                str_premise_end_special_token\n",
        "      hypothesis = str_hypothesis_start_special_token + \\\n",
        "                source_subfix + \":\" + hypothesis + \\\n",
        "                str_hypothesis_end_special_token\n",
        "    premise_list.append(premise)\n",
        "    hypothesis_list.append(hypothesis)\n",
        "    label_list.append(label)\n",
        "    if is_test:\n",
        "      index_list.append(index)\n",
        "    else:\n",
        "      pass    \n",
        "\n",
        "  df_return[\"premise\"] = premise_list\n",
        "  df_return[\"hypothesis\"] = hypothesis_list\n",
        "  df_return[\"label\"] = label_list \n",
        "  if is_test:\n",
        "    df_return[\"index\"] = index_list\n",
        "  # apply tokenizer for premise\n",
        "  df_return['premise_len'] = df_return['premise'].apply(\n",
        "      lambda x: len(tokenizer.encode(x, add_special_tokens=False))\n",
        "  )\n",
        "  df_return['hypothesis_len'] = df_return['hypothesis'].apply(\n",
        "      lambda x: len(tokenizer.encode(x, add_special_tokens=False))\n",
        "  )\n",
        "\n",
        "  df_return['premise_index'] = CLS_TOKEN_LEN # ^ is next to [CLS] token\n",
        "  df_return['hypothesis_index'] = CLS_TOKEN_LEN + df_return['premise_len'] + SEP_TOKEN_LEN # * is next to [SEP] token\n",
        "  df_return['total_length'] = CLS_TOKEN_LEN + df_return['premise_len'] \\\n",
        "      + SEP_TOKEN_LEN + df_return['hypothesis_len'] + SEP_TOKEN_LEN\n",
        "  return df_return"
      ],
      "metadata": {
        "id": "bYMQOWnylBKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set display options\n",
        "pd.options.display.max_colwidth = None\n",
        "\n",
        "df_pororo_dataset_preprocessed = preprocess_augmentation_electra(\n",
        "    df_pororo_dataset, \n",
        "    str_premise_start_special_token = \"[PREMISE]\", \n",
        "    str_premise_end_special_token = \"[PREMISE]\", \n",
        "    str_hypothesis_start_special_token = \"[HYPOTHESIS]\",\n",
        "    str_hypothesis_end_special_token = \"[HYPOTHESIS]\",\n",
        ")\n",
        "\n",
        "df_pororo_dataset_preprocessed.head(3)"
      ],
      "metadata": {
        "id": "VueR-MEblBHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show distribution of total_length\n",
        "df_pororo_dataset_preprocessed['total_length'].hist(bins=100)\n",
        "print(max(df_pororo_dataset_preprocessed['total_length'])) # tunib electra max token length: 97\n",
        "CFG.max_token_length = max(df_pororo_dataset_preprocessed['total_length'])"
      ],
      "metadata": {
        "id": "28DJzOy1lBFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class RBERTDataset(Dataset):\n",
        "    def __init__(self, dataset, is_training:bool=True):\n",
        "        \n",
        "        # object values with string items\n",
        "        self.dataset = dataset\n",
        "        self.premise = self.dataset['premise']\n",
        "        self.hypothesis = self.dataset['hypothesis']\n",
        "        self.sentence = self.dataset['premise'] + ' [SEP] ' + self.dataset['hypothesis']\n",
        "        \n",
        "        # torch values with integer values\n",
        "        if is_training:\n",
        "            self.train_label = label_to_num(self.dataset['label'].values)\n",
        "        if not is_training:\n",
        "            self.train_label = self.dataset['label'].values\n",
        "        self.label = torch.tensor(self.train_label)\n",
        "        self.premise_start_index = torch.tensor(self.dataset['premise_index'].values)\n",
        "        self.premise_len = torch.tensor(self.dataset['premise_len'].values)\n",
        "        self.hypothesis_start_index = torch.tensor(self.dataset['hypothesis_index'].values)\n",
        "        self.hypothesis_len = torch.tensor(self.dataset['hypothesis_len'].values)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # object values with string items\n",
        "        sentence = self.sentence[idx]\n",
        "        \n",
        "        # torch values with integer values\n",
        "        premise_start_index = self.premise_start_index[idx]\n",
        "        premise_len = self.premise_len[idx]\n",
        "        hypothesis_start_index = self.hypothesis_start_index[idx]\n",
        "        hypothesis_len = self.hypothesis_len[idx]\n",
        "        label = self.label[idx]\n",
        "        \n",
        "        # tokenize\n",
        "        item = tokenizer(\n",
        "            sentence,\n",
        "            return_tensors=\"pt\",\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=CFG.max_token_length,\n",
        "            add_special_tokens=True,\n",
        "            return_token_type_ids=False, # for RoBERTa\n",
        "            )\n",
        "        \n",
        "        # RoBERTa's provided masks (do not include token_type_ids for RoBERTa)\n",
        "        item['input_ids'] = item['input_ids'].squeeze(0)\n",
        "        item['attention_mask'] = item['attention_mask'].squeeze(0)\n",
        "        \n",
        "        # add subject and object entity masks where masks notate where the entity is\n",
        "        premise_mask, hypothesis_mask = self.add_entity_mask(\n",
        "            premise_start_index,\n",
        "            hypothesis_start_index,\n",
        "            premise_len, \n",
        "            hypothesis_len,\n",
        "        )\n",
        "        item['premise_mask'] = torch.tensor(premise_mask)\n",
        "        item['hypothesis_mask'] = torch.tensor(hypothesis_mask)\n",
        "\n",
        "        # fill label\n",
        "        item['label'] = label\n",
        "\n",
        "        item['premise_special_token_index'] = premise_start_index\n",
        "        item['hypothesis_special_token_index'] = hypothesis_start_index\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def add_entity_mask(\n",
        "            self,\n",
        "            premise_start_index,\n",
        "            hypothesis_start_index,\n",
        "            premise_len, \n",
        "            hypothesis_len\n",
        "        ):\n",
        "        \"\"\" add entity token to input_ids \"\"\"\n",
        "        # print(\"tokenized input ids: \\n\",item['input_ids'])\n",
        "\n",
        "        # initialize entity masks\n",
        "        premise_mask = np.zeros(CFG.max_token_length, dtype=int)\n",
        "        hypothesis_mask = np.zeros(CFG.max_token_length, dtype=int)\n",
        "\n",
        "        premise_mask[\n",
        "          premise_start_index : premise_start_index + premise_len\n",
        "        ] = 1\n",
        "        \n",
        "        hypothesis_mask[\n",
        "          hypothesis_start_index : hypothesis_start_index + hypothesis_len\n",
        "        ] = 1\n",
        "        \n",
        "        return premise_mask, hypothesis_mask"
      ],
      "metadata": {
        "id": "dNDQehTFlBCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_1 = RBERTDataset(df_pororo_dataset_preprocessed)[0]\n",
        "sample_2 = RBERTDataset(df_pororo_dataset_preprocessed)[1]\n",
        "sample_3 = RBERTDataset(df_pororo_dataset_preprocessed)[2]\n",
        "\n",
        "print(sample_1)"
      ],
      "metadata": {
        "id": "uEFCXAzNlU55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging add_entity_mask_sample function with sample code\n",
        "\n",
        "decoded_item_1 = tokenizer.decode(sample_1['input_ids'])\n",
        "decoded_item_2 = tokenizer.decode(sample_2['input_ids'])\n",
        "decoded_item_3 = tokenizer.decode(sample_3['input_ids'])\n",
        "\n",
        "print(decoded_item_1)\n",
        "print(decoded_item_2)\n",
        "print(decoded_item_3)"
      ],
      "metadata": {
        "id": "Yscgc7V2lU3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nn from torch\n",
        "from torch import nn\n",
        "from transformers import AutoConfig, AutoModel\n",
        "\n",
        "\"\"\" R-BERT: https://github.com/monologg/R-BERT \"\"\"\n",
        "class FCLayer(nn.Module):\n",
        "    # both attention dropout and fc dropout is 0.1 on Roberta: https://arxiv.org/pdf/1907.11692.pdf\n",
        "    def __init__(self, input_dim, output_dim, dropout_rate=0.1, use_activation=True):\n",
        "        super(FCLayer, self).__init__()\n",
        "        self.use_activation = use_activation\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "        self.activation = nn.GELU() # electra uses gelu whereas BERT or Roberta used tanh in fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        return self.linear(x)\n",
        "\n",
        "class RBERT(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        model_name:str=CFG.model_name,\n",
        "        num_labels:int=3,\n",
        "        dropout_rate:float=0.1,\n",
        "        special_tokens_dict:dict=None,\n",
        "        is_train:bool=True,\n",
        "        embedding_resizing_length = len(tokenizer),\n",
        "        ):\n",
        "        super(RBERT, self).__init__()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        config = AutoConfig.from_pretrained(model_name)\n",
        "        config.num_labels = num_labels\n",
        "        self.backbone_model = AutoModel.from_pretrained(model_name, config=config)\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_labels = num_labels\n",
        "        \n",
        "        # add special tokens\n",
        "        self.special_tokens_dict = special_tokens_dict\n",
        "        if embedding_resizing_length != 32000:\n",
        "            self.backbone_model.resize_token_embeddings(embedding_resizing_length)\n",
        "\n",
        "        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, self.dropout_rate)\n",
        "        self.entity_fc_layer = FCLayer(config.hidden_size, config.hidden_size, self.dropout_rate)\n",
        "        self.label_classifier = FCLayer(\n",
        "            config.hidden_size * 3,\n",
        "            self.num_labels,\n",
        "            self.dropout_rate,\n",
        "            use_activation=False,\n",
        "        )\n",
        "\n",
        "    def entity_average(self, hidden_output, e_mask):\n",
        "        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]\n",
        "        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [batch_size, 1]\n",
        "\n",
        "        # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\n",
        "        sum_vector = torch.bmm(e_mask_unsqueeze.float(), hidden_output).squeeze(1)\n",
        "        avg_vector = sum_vector.float() / length_tensor.float()  # broadcasting\n",
        "        return avg_vector\n",
        "        \n",
        "\n",
        "    def forward(self, input_ids, attention_mask, premise_mask=None, hypothesis_mask=None, labels=None):\n",
        "        \n",
        "        discriminator_hidden_states = self.backbone_model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "                \n",
        "        # https://github.com/huggingface/transformers/blob/db7d6a80e82d66127b2a44b6e3382969fdc8b207/src/transformers/models/electra/modeling_electra.py#L932-L951\n",
        "        sequence_output = discriminator_hidden_states[0]\n",
        "        pooled_output = sequence_output[:, 0, :]  # [CLS] token's hidden featrues(hidden state)\n",
        "\n",
        "        # hidden state's average in between entities\n",
        "        # print(sequence_output.shape, premise_mask.shape)\n",
        "        e1_h = self.entity_average(sequence_output, premise_mask) # token in between subject entities -> \n",
        "        e2_h = self.entity_average(sequence_output, hypothesis_mask) # token in between object entities\n",
        "\n",
        "        # Dropout -> gelu -> fc_layer (Share FC layer for e1 and e2)\n",
        "        pooled_output = self.cls_fc_layer(pooled_output) # [CLS] token -> hidden state | green on diagram\n",
        "        e1_h = self.entity_fc_layer(e1_h) # subject entity's fully connected layer | yellow on diagram\n",
        "        e2_h = self.entity_fc_layer(e2_h) # object entity's fully connected layer | red on diagram\n",
        "\n",
        "        # Concat -> fc_layer / [CLS], subject_average, object_average\n",
        "        concat_h = torch.cat([pooled_output, e1_h, e2_h], dim=-1)\n",
        "        logits = self.label_classifier(concat_h)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "i656xj4RlU1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IBDataset(Dataset):\n",
        "    def __init__(self, dataset, is_training:bool=True):\n",
        "        \n",
        "        # pandas.Dataframe dataset\n",
        "        self.dataset = dataset\n",
        "        self.premise = self.dataset['premise']\n",
        "        self.hypothesis = self.dataset['hypothesis']\n",
        "        self.sentence = self.dataset['premise'] + ' [SEP] ' + self.dataset['hypothesis']\n",
        "        if is_training:\n",
        "            self.train_label = label_to_num(self.dataset['label'].values)\n",
        "        if not is_training:\n",
        "            self.train_label = self.dataset['label'].values\n",
        "        self.label = torch.tensor(self.train_label)\n",
        "        self.premise_special_token_index = torch.tensor(self.dataset['premise_index'].values)\n",
        "        self.hypothesis_special_token_index = torch.tensor(self.dataset['hypothesis_index'].values)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentence[idx]\n",
        "        label = self.label[idx]\n",
        "        premise_special_token_index = self.premise_special_token_index[idx]\n",
        "        hypothesis_special_token_index = self.hypothesis_special_token_index[idx]\n",
        "\n",
        "        # tokenize\n",
        "        item = tokenizer(\n",
        "            sentence,\n",
        "            return_tensors=\"pt\",\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=CFG.max_token_length,\n",
        "            add_special_tokens=True,\n",
        "            return_token_type_ids=False, # for RoBERTa\n",
        "            )\n",
        "        \n",
        "        # RoBERTa's provided masks (do not include token_type_ids for RoBERTa)\n",
        "        item['input_ids'] = item['input_ids'].squeeze(0)\n",
        "        item['attention_mask'] = item['attention_mask'].squeeze(0)\n",
        "\n",
        "        # fill label\n",
        "        item['label'] = label\n",
        "        item['premise_special_token_index'] = premise_special_token_index\n",
        "        item['hypothesis_special_token_index'] = hypothesis_special_token_index\n",
        "        return item"
      ],
      "metadata": {
        "id": "HekQ1UBplUyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_1 = IBDataset(df_pororo_dataset_preprocessed)[0]\n",
        "sample_2 = IBDataset(df_pororo_dataset_preprocessed)[1]\n",
        "sample_3 = IBDataset(df_pororo_dataset_preprocessed)[2]\n",
        "\n",
        "print(sample_1)"
      ],
      "metadata": {
        "id": "JKjqm6TclUvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging add_entity_mask_sample function with sample code\n",
        "\n",
        "decoded_item_1 = tokenizer.decode(sample_1['input_ids'])\n",
        "decoded_item_2 = tokenizer.decode(sample_2['input_ids'])\n",
        "decoded_item_3 = tokenizer.decode(sample_3['input_ids'])\n",
        "\n",
        "print(decoded_item_1)\n",
        "print(decoded_item_2)\n",
        "print(decoded_item_3)"
      ],
      "metadata": {
        "id": "RQ4MpaablUsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nn from torch\n",
        "from torch import nn\n",
        "from transformers import AutoConfig, AutoModel\n",
        "\n",
        "class IBModel(nn.Module):\n",
        "    \"\"\"\n",
        "    'An Improved Baseline for Sentence-level Relation Extraction'\n",
        "    https://github.com/wzhouad/RE_improved_baseline/blob/main/model.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "        model_name = \"tunib/electra-ko-base\", \n",
        "        dropout_rate = CFG.dropout_rate,\n",
        "        use_arcface = False,\n",
        "        embedding_resizing_length = len(tokenizer),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        config = AutoConfig.from_pretrained(CFG.model_name)\n",
        "        config.num_labels = CFG.num_labels\n",
        "        self.backbone_model = AutoModel.from_pretrained(model_name, config=config)\n",
        "        if embedding_resizing_length != 32000:\n",
        "          self.backbone_model.resize_token_embeddings(embedding_resizing_length)\n",
        "        hidden_size = config.hidden_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(hidden_size * 3, hidden_size), # change to hidden_size * 3 if you want to add pooler_output\n",
        "            nn.GELU(), \n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(hidden_size, config.num_labels),\n",
        "        )\n",
        "    \n",
        "    def forward(\n",
        "            self, \n",
        "            input_ids, \n",
        "            attention_mask,\n",
        "            premise_special_token_index,\n",
        "            hypothesis_special_token_index,\n",
        "            labels=None):\n",
        "        outputs = self.backbone_model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[\"last_hidden_state\"] # sequence output of the last layer\n",
        "        cls_output = sequence_output[:, 0, :]  # [CLS] token's hidden features(different from pooler output)\n",
        "        # pooler_output = outputs[\"pooler_output\"]  # [CLS] hidden state passed through pooler: https://github.com/huggingface/transformers/blob/2c2a31ffbcfe03339b1721348781aac4fc05bc5e/src/transformers/models/roberta/modeling_roberta.py#L569-L581\n",
        "\n",
        "        # extract embedding for special tokens for each premise and hypothesis\n",
        "        idx_seq = torch.arange(input_ids.size(0)).to(input_ids.device) \n",
        "        premise_emb = sequence_output[idx_seq, premise_special_token_index]\n",
        "        hypothesis_emb = sequence_output[idx_seq, hypothesis_special_token_index]\n",
        "\n",
        "        # concat [CLS], ^, *\n",
        "        concat_hidden_states = torch.cat((cls_output, premise_emb, hypothesis_emb), dim=-1)\n",
        "      \n",
        "        return self.classifier(concat_hidden_states)"
      ],
      "metadata": {
        "id": "wuNsoAKIlUqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nn from torch\n",
        "from torch import nn\n",
        "from torch.cuda.amp import autocast\n",
        "from transformers import AutoConfig, AutoModel\n",
        "\n",
        "class IBConcatModel(nn.Module):\n",
        "    def __init__(self, \n",
        "        model_name = \"tunib/electra-ko-base\",\n",
        "        dropout_rate = CFG.dropout_rate,\n",
        "        embedding_resizing_length = len(tokenizer),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        config = AutoConfig.from_pretrained(CFG.model_name)\n",
        "        config.num_labels = CFG.num_labels\n",
        "        self.backbone_model = AutoModel.from_pretrained(model_name, config=config)\n",
        "        if embedding_resizing_length != 32000:\n",
        "          self.backbone_model.resize_token_embeddings(embedding_resizing_length)\n",
        "        hidden_size = config.hidden_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(hidden_size * 12, hidden_size * 3), # change to hidden_size * 3 if you want to add pooler_output\n",
        "            # nn.GELU(), \n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(hidden_size * 3, config.num_labels),\n",
        "        )\n",
        "\n",
        "    # @autocast() # autocast for FP16\n",
        "    def forward(\n",
        "            self, \n",
        "            input_ids, \n",
        "            attention_mask,\n",
        "            premise_special_token_index,\n",
        "            hypothesis_special_token_index,\n",
        "            labels=None):\n",
        "        \n",
        "        outputs = self.backbone_model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "        )\n",
        "        \n",
        "        # sequence output of the last layer\n",
        "        # https://github.com/huggingface/transformers/blob/db7d6a80e82d66127b2a44b6e3382969fdc8b207/src/transformers/models/electra/modeling_electra.py#L961\n",
        "        hidden_states = outputs[\"hidden_states\"]\n",
        "        \n",
        "        # extract embedding for [CLS] token\n",
        "        # concating last four hidden states are mentioned in BERT paper: https://github.com/huggingface/transformers/issues/1328\n",
        "        cls_concat = torch.cat(tuple([hidden_states[i][:, 0, :] for i in [-4, -3, -2, -1]]), dim=-1)\n",
        "\n",
        "        # extract embedding for special tokens for each premise and hypothesis\n",
        "        idx_seq = torch.arange(input_ids.size(0)).to(input_ids.device) \n",
        "        \n",
        "        # premise special token embedding\n",
        "        # concating last four hidden states are mentioned in BERT paper: https://github.com/huggingface/transformers/issues/1328\n",
        "        premise_concat = torch.cat(tuple([hidden_states[i][idx_seq, premise_special_token_index] for i in [-4, -3, -2, -1]]), dim=-1)\n",
        "        \n",
        "        # hypothesis special token embedding\n",
        "        # concating last four hidden states are mentioned in BERT paper: https://github.com/huggingface/transformers/issues/1328\n",
        "        hypothesis_concat = torch.cat(tuple([hidden_states[i][idx_seq, hypothesis_special_token_index] for i in [-4, -3, -2, -1]]), dim=-1)\n",
        "\n",
        "        # concat embeddings for [CLS], ^, *\n",
        "        knit_hidden_states = torch.cat((\n",
        "            cls_concat,\n",
        "            premise_concat,\n",
        "            hypothesis_concat,\n",
        "        ), dim=-1)\n",
        "        \n",
        "        return self.classifier(knit_hidden_states)"
      ],
      "metadata": {
        "id": "gqJuNnPElUnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from torch.cuda.amp import autocast\n",
        "from torch.nn import Parameter\n",
        "import math\n",
        "\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    \"\"\"  \n",
        "    Reference: https://github.com/ronghuaiyang/arcface-pytorch/blob/master/models/metrics.py\n",
        "    Implement of large margin arc distance: :\n",
        "    Args:\n",
        "        in_features: size of each input sample\n",
        "        out_features: size of each output sample\n",
        "        s: norm of input feature: scaling ratio from a small number to a final logit\n",
        "        m: the additional angular margin\n",
        "        cos(theta + m)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = math.cos(m)\n",
        "        self.sin_m = math.sin(m)\n",
        "        self.th = math.cos(math.pi - m)\n",
        "        self.mm = math.sin(math.pi - m) * m\n",
        "    \n",
        "    # @autocast() for FP16 (Automatic Mixed Precision)\n",
        "    def forward(self, input, label=None):\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "    \n",
        "        if label is not None:\n",
        "          one_hot = torch.zeros(cosine.size(), device='cuda')\n",
        "          one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
        "          output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        else:\n",
        "          output = cosine\n",
        "        \n",
        "        output *= self.s\n",
        "        return output"
      ],
      "metadata": {
        "id": "KNbZckSKlh5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nn from torch\n",
        "from torch import nn\n",
        "from torch.cuda.amp import autocast\n",
        "from transformers import AutoConfig, AutoModel\n",
        "\n",
        "class ArcFaceIBModel(nn.Module):\n",
        "    \"\"\" Modified IBmodel with Arcmarginproduct head \"\"\"\n",
        "    def __init__(\n",
        "        self, \n",
        "        model_name, \n",
        "        dropout_rate = CFG.dropout_rate,\n",
        "        embedding_resizing_length = len(tokenizer)\n",
        "    ):\n",
        "        super().__init__()\n",
        "        config = AutoConfig.from_pretrained(CFG.model_name)\n",
        "        config.num_labels = CFG.num_labels\n",
        "        self.backbone_model = AutoModel.from_pretrained(model_name, config=config)\n",
        "        if embedding_resizing_length != 32000:\n",
        "          self.backbone_model.resize_token_embeddings(embedding_resizing_length)\n",
        "        hidden_size = config.hidden_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.arcface = ArcMarginProduct(config.hidden_size, config.num_labels)\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(hidden_size * 3, hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "        )\n",
        "        self.proj_fc_layer = FCLayer(config.hidden_size * 4, config.hidden_size, self.dropout_rate)\n",
        "\n",
        "    # @autocast()\n",
        "    def forward(\n",
        "            self, \n",
        "            input_ids, \n",
        "            attention_mask,\n",
        "            premise_special_token_index,\n",
        "            hypothesis_special_token_index,\n",
        "            labels=None):\n",
        "        outputs = self.backbone_model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "        )\n",
        "        hidden_states = outputs[\"hidden_states\"]\n",
        "        cls_concat = torch.cat(tuple([hidden_states[i][:, 0, :] for i in [-4, -3, -2, -1]]), dim=-1)\n",
        "        cls_output = self.proj_fc_layer(cls_concat)\n",
        "        \n",
        "        # extract embedding for special tokens for each premise and hypothesis\n",
        "        idx_seq = torch.arange(input_ids.size(0)).to(input_ids.device) \n",
        "        premise_concat = torch.cat(tuple([hidden_states[i][idx_seq, premise_special_token_index] for i in [-4, -3, -2, -1]]), dim=-1)\n",
        "        premise_output = self.proj_fc_layer(premise_concat)\n",
        "        hypothesis_concat = torch.cat(tuple([hidden_states[i][idx_seq, hypothesis_special_token_index] for i in [-4, -3, -2, -1]]), dim=-1)\n",
        "        hypothesis_output = self.proj_fc_layer(hypothesis_concat)\n",
        "\n",
        "        # concat [CLS], ^, *\n",
        "        concat_hidden_states = torch.cat((cls_output, premise_output, hypothesis_output), dim=-1)\n",
        "      \n",
        "        # goes through neck for projection\n",
        "        out_proj = self.neck(concat_hidden_states)\n",
        "        \n",
        "        # input to the arcface head for logit\n",
        "        if labels is not None:\n",
        "            logits = self.arcface(out_proj, labels)\n",
        "        else:\n",
        "            logits = self.arcface(out_proj)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "t5mr2rGdlh3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() and CFG.DEBUG == False else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "i7xiG5u6lh1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Metrics(object):\n",
        "    \"\"\" \n",
        "    Averaging metrics collected across batches such as loss, f1 and accuracy\n",
        "    Reference: https://github.com/pytorch/examples/blob/21c240b814658e590b4fa9d4682d39831060c5b9/imagenet/main.py#L367-L385    \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        \n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "tShRv3jdlhyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_name = f\"ArcfaceIBModel-Electra-4-concat-Augmentation-v7-not-shuffled-CrossEntropyLoss-{CFG.train_batch_size}\""
      ],
      "metadata": {
        "id": "BjiWEbBOlhv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scikit-learn metrics and folds\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# torch data utils\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "\n",
        "# optimizers and schedulers\n",
        "from adamp import AdamP\n",
        "from transformers import AdamW # or from torch.optim import AdamW\n",
        "from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
        "\n",
        "# tqdm progress bar repetition error fixed on jupyter notebook\n",
        "from tqdm.notebook import tqdm \n",
        "\n",
        "wandb.init(\n",
        "    project='KLUE-NLI', \n",
        "    name=run_name,\n",
        "    config=CFG\n",
        "  )\n",
        "\n",
        "train_data = IBDataset(df_pororo_dataset_preprocessed)\n",
        "dev_data = IBDataset(df_pororo_dataset_preprocessed)\n",
        "\n",
        "kfd = KFold(n_splits = CFG.num_folds, shuffle=False)\n",
        "\n",
        "for fold_num, (train_idx, dev_idx) in enumerate(kfd.split(df_pororo_dataset_preprocessed)):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print(\"using CE loss function\")\n",
        "\n",
        "    print(f\"#################### Fold: {fold_num + 1} ######################\")\n",
        "\n",
        "    RESULT_PATH = f\"./results\"\n",
        "    if not os.path.exists(RESULT_PATH):\n",
        "      os.mkdir(RESULT_PATH)\n",
        "    \n",
        "    SAVE_PATH = f\"./results/{run_name}\"\n",
        "    if not os.path.exists(SAVE_PATH):\n",
        "      os.mkdir(SAVE_PATH)\n",
        "\n",
        "    train_set = Subset(train_data, train_idx)\n",
        "    dev_set = Subset(dev_data, dev_idx)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_set, \n",
        "        batch_size=CFG.train_batch_size, \n",
        "        shuffle=False, \n",
        "        num_workers=CFG.num_workers,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    dev_loader = DataLoader(\n",
        "        dev_set, \n",
        "        batch_size=CFG.val_batch_size, \n",
        "        shuffle=False, \n",
        "        num_workers=CFG.num_workers,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    # fetch model\n",
        "    model = ArcFaceIBModel(\n",
        "        CFG.model_name, \n",
        "        dropout_rate = CFG.dropout_rate,\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # fetch loss function, optimizer, scheduler outside of torch library\n",
        "    # https://github.com/clovaai/AdamP\n",
        "    optimizer = AdamP(\n",
        "        model.parameters(), # training all params\n",
        "        lr=CFG.learning_rate,\n",
        "        weight_decay=CFG.weight_decay,\n",
        "        eps=CFG.adam_epsilon,\n",
        "        betas=(0.9, 0.999),\n",
        "    )\n",
        "\n",
        "    CFG.logging_steps = len(train_loader) // 3 # set logging steps according to the length of train_loader\n",
        "    CFG.warmup_steps = CFG.logging_steps # warmup steps as 1/3 of first epoch\n",
        "    \n",
        "    # https://huggingface.co/transformers/main_classes/optimizer_schedules.html\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=CFG.warmup_steps, num_training_steps=len(train_loader)*CFG.num_epochs)\n",
        "\n",
        "    # class for accumulative metrics calculation over batches iteration\n",
        "    train_acc = Metrics()\n",
        "    train_loss = Metrics()\n",
        "    dev_acc = Metrics()\n",
        "    dev_loss = Metrics()\n",
        "    train_f1 = Metrics()\n",
        "    dev_f1 = Metrics()\n",
        "\n",
        "    best_eval_loss = 5.0\n",
        "    best_eval_accuracy = 0.80\n",
        "    best_f1_score = 0.80\n",
        "    \n",
        "    steps = 0\n",
        "    \n",
        "    # fetch training loop\n",
        "    for epoch in range(CFG.num_epochs):\n",
        "        for _ , item in enumerate(tqdm(train_loader)):\n",
        "            \n",
        "            # model to training mode\n",
        "            model.train()\n",
        "            \n",
        "            input_ids = item['input_ids'].to(device)\n",
        "            attention_mask = item['attention_mask'].to(device)\n",
        "            premise_special_token_index = item['premise_special_token_index'].to(device)\n",
        "            hypothesis_special_token_index = item['hypothesis_special_token_index'].to(device)\n",
        "            label = item['label'].to(device)\n",
        "            \n",
        "            # assign forward() arguments to the device\n",
        "            logits = model(\n",
        "                input_ids, \n",
        "                attention_mask,\n",
        "                premise_special_token_index=premise_special_token_index, \n",
        "                hypothesis_special_token_index=hypothesis_special_token_index, \n",
        "                labels=label\n",
        "            )\n",
        "            \n",
        "            loss = criterion(logits, label)\n",
        "\n",
        "            optimizer.zero_grad() # replacing model.zero_grad() with optimizer_zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # update loss\n",
        "            train_loss.update(loss.item(), len(input_ids))\n",
        "            \n",
        "            # accuracy\n",
        "            predict = logits.argmax(-1)\t\n",
        "            train_accuracy_score = accuracy_score(label.detach().cpu().numpy(), predict.detach().cpu().numpy())\n",
        "            train_acc.update(train_accuracy_score, len(input_ids))\n",
        "            \n",
        "            # f1 score\n",
        "            train_f1_score = f1_score(label.detach().cpu().numpy(), predict.detach().cpu().numpy(), average=\"macro\") # macro for even classes: https://www.baeldung.com/cs/multi-class-f1-score\n",
        "            train_f1.update(train_f1_score, len(input_ids))\n",
        "            \n",
        "            steps += 1\n",
        "\n",
        "            if steps % CFG.logging_steps == 0: # batch\n",
        "                print('Epoch: {}/{}'.format(epoch+1, CFG.num_epochs), 'Step: {}'.format(steps), 'Train Loss: {:.4f}'.format(train_loss.avg), 'Train Acc: {:.4f}'.format(train_acc.avg))\n",
        "                for _, dev_item in enumerate(tqdm(dev_loader)):\n",
        "                    dev_input_ids = dev_item['input_ids'].to(device)\n",
        "                    dev_attention_mask = dev_item['attention_mask'].to(device)\n",
        "                    dev_premise_special_token_index = dev_item['premise_special_token_index'].to(device)\n",
        "                    dev_hypothesis_special_token_index = dev_item['hypothesis_special_token_index'].to(device)\n",
        "                    dev_label = dev_item['label'].to(device)\n",
        "                    \n",
        "                    # switch model to eval mode\n",
        "                    model.eval()\n",
        "                    dev_logits = model(\n",
        "                        dev_input_ids, \n",
        "                        dev_attention_mask,\n",
        "                        premise_special_token_index=dev_premise_special_token_index, \n",
        "                        hypothesis_special_token_index=dev_hypothesis_special_token_index, \n",
        "                        labels=dev_label\n",
        "                    )\n",
        "\n",
        "                    # update loss\n",
        "                    loss = criterion(dev_logits, dev_label)\n",
        "                    dev_loss.update(loss.item(), len(dev_input_ids))\n",
        "                    \n",
        "                    # accuracy\n",
        "                    dev_predict = dev_logits.argmax(-1)\n",
        "                    dev_accuracy_score = accuracy_score(dev_label.detach().cpu().numpy(), dev_predict.detach().cpu().numpy())\n",
        "                    dev_acc.update(dev_accuracy_score, len(dev_input_ids))\n",
        "\n",
        "                    # f1 score\n",
        "                    dev_f1_score = f1_score(dev_label.detach().cpu().numpy(), dev_predict.detach().cpu().numpy(), average=\"macro\") # macro for even classes: https://www.baeldung.com/cs/multi-class-f1-score\n",
        "                    dev_f1.update(dev_f1_score, len(input_ids))\n",
        "\n",
        "                # print metrics\n",
        "                print('Epoch: {}/{}'.format(epoch+1, CFG.num_epochs), \n",
        "                      'Step: {}'.format(steps), \n",
        "                      'Dev Loss: {:.4f}'.format(dev_loss.avg), \n",
        "                      'Dev Acc: {:.4f}'.format(dev_acc.avg), \n",
        "                      'Dev f1: {:.4f}'.format(dev_f1.avg)\n",
        "                )\n",
        "                wandb.log(\n",
        "                    {\n",
        "                        'train/loss':train_loss.avg, \n",
        "                        'train/accuracy':train_acc.avg, \n",
        "                        'train/f1': train_f1.avg,\n",
        "                        'train/learning_rate':optimizer.param_groups[0]['lr'], \n",
        "                        'eval/loss':dev_loss.avg,\n",
        "                        'eval/accuracy':dev_acc.avg,\n",
        "                        'eval/f1':dev_f1.avg,\n",
        "                        'Step':steps\n",
        "                    }\n",
        "                )\n",
        "                    \n",
        "                if best_eval_loss > dev_loss.avg:\n",
        "                    best_eval_loss = dev_loss.avg\n",
        "                    torch.save(model.state_dict(), f'{SAVE_PATH}/{fold_num+1}-fold-{CFG.num_folds}-best-eval-loss-model.pt')\n",
        "                    print('Saved model with lowest validation loss: {:.4f}'.format(best_eval_loss))\n",
        "                    wandb.log({'best_eval_loss':best_eval_loss})\n",
        "                  \n",
        "                if best_eval_accuracy < dev_acc.avg:\n",
        "                    best_eval_accuracy = dev_acc.avg\n",
        "                    torch.save(model.state_dict(), f'{SAVE_PATH}/{fold_num+1}-fold-{CFG.num_folds}-best-eval-accuracy-model.pt')\n",
        "                    print('Saved model with highest validation accuracy: {:.4f}'.format(best_eval_accuracy))\n",
        "                    wandb.log({'best_eval_accuracy':best_eval_accuracy})\n",
        "                \n",
        "                if best_f1_score < dev_f1.avg:\n",
        "                    best_f1_score = dev_f1.avg\n",
        "                    torch.save(model.state_dict(), f'{SAVE_PATH}/{fold_num+1}-fold-{CFG.num_folds}-best-eval-f1-model.pt')\n",
        "                    print('Saved model with highest validation f1: {:.4f}'.format(best_f1_score))\n",
        "                    wandb.log({'best_f1_score':best_f1_score})\n",
        "\n",
        "                # reset metrics\n",
        "                dev_loss.reset()\n",
        "                dev_acc.reset()\n",
        "                dev_f1.reset()\n",
        "                \n",
        "                train_acc.reset()\n",
        "                train_loss.reset()\n",
        "                train_f1.reset()\n",
        "                \n",
        "\n",
        "    # Prevent OOM error\n",
        "    model.cpu()\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    clear_output()"
      ],
      "metadata": {
        "id": "6quLf6ysloL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_to_label(label):\n",
        "    label_dict = {0: \"entailment\", 1: \"contradiction\", 2: \"neutral\"}\n",
        "    list_str_label = []\n",
        "\n",
        "    for i, v in enumerate(label):\n",
        "        list_str_label.append(label_dict[v])\n",
        "    \n",
        "    return list_str_label"
      ],
      "metadata": {
        "id": "wpKufvfEloJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "\n",
        "test_dataset = pd.read_csv(TEST_FILE_PATH)\n",
        "display(test_dataset.head(2))"
      ],
      "metadata": {
        "id": "2WpSsF32loHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "test_dataset_shuffled = test_dataset.sort_values(by=[\"source_label\"]).iloc[::-1] # order by source label: ì˜í™” -> ì—¬í–‰ -> ì¼ë°˜\n",
        "test_dataset_shuffled.head(3)"
      ],
      "metadata": {
        "id": "rqldTzAFloEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "\n",
        "def literal_eval_without_error(input_string):\n",
        "    # string [0.2765555, 0.7233189, 6.404926e-05, 6.1491264e-05] into list [0.2765555, 0.7233189, 6.404926e-05, 6.1491264e-05]\n",
        "    input_string = input_string[1:-1]\n",
        "    input_list = input_string.split(\",\")\n",
        "    output_list = [item.strip() for item in input_list]\n",
        "    output_list = [float(item) for item in input_list]\n",
        "    return output_list\n",
        "\n",
        "\n",
        "test_dataset_shuffled[\"electra_premise_probability\"] = test_dataset_shuffled[\"electra_premise_probability\"].apply(literal_eval_without_error)\n",
        "test_dataset_shuffled[\"electra_hypothesis_probability\"] = test_dataset_shuffled[\"electra_hypothesis_probability\"].apply(literal_eval_without_error)\n",
        "test_dataset_shuffled[\"electra_premise_score_probability\"] = test_dataset_shuffled[\"electra_premise_score_probability\"].apply(literal_eval_without_error)\n",
        "test_dataset_shuffled[\"electra_hypothesis_score_probability\"] = test_dataset_shuffled[\"electra_hypothesis_score_probability\"].apply(literal_eval_without_error)\n",
        "\n",
        "display(test_dataset_shuffled.head(6))\n",
        "display(test_dataset_shuffled.tail(6))"
      ],
      "metadata": {
        "id": "OraMSpM8lBAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_dataset = preprocess_augmentation_electra(\n",
        "    test_dataset_shuffled, \n",
        "    str_premise_start_special_token = \"[PREMISE]\", \n",
        "    str_premise_end_special_token = \"[PREMISE]\", \n",
        "    str_hypothesis_start_special_token = \"[HYPOTHESIS]\",\n",
        "    str_hypothesis_end_special_token = \"[HYPOTHESIS]\",\n",
        "    is_test = True\n",
        ")\n",
        "\n",
        "# set label as 100 \n",
        "df_test_dataset['label'] = 100"
      ],
      "metadata": {
        "id": "d1NWHoL3lvw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = IBDataset(df_test_dataset, is_training=False)\n",
        "print(len(test_set))\n",
        "test_data_loader = DataLoader(\n",
        "    test_set, \n",
        "    batch_size=CFG.val_batch_size, \n",
        "    num_workers=CFG.num_workers, \n",
        "    shuffle=False,\n",
        "    drop_last=False\n",
        ")\n",
        "len(test_data_loader)"
      ],
      "metadata": {
        "id": "laDefPoglvuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.nn import functional as F\n",
        "import shutil\n",
        "\n",
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "\n",
        "CRITERIA_LIST = [\"accuracy\", \"f1\", \"loss\" ]\n",
        "CRITERIA =\"f1\"\n",
        "DIR_PATH = f\"/content/results/{run_name}\" # or \"/content/\"\n",
        "\n",
        "oof_pred = [] # out of fold prediction list\n",
        "for i in range(CFG.num_folds) :\n",
        "    model_path = f'{DIR_PATH}/{i+1}-fold-{CFG.num_folds}-best-eval-{CRITERIA}-model.pt'\n",
        "    model = ArcFaceIBModel(\n",
        "        CFG.model_name, \n",
        "        dropout_rate = CFG.dropout_rate,\n",
        "    )\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    list_logits = []\n",
        "    for i, data in enumerate(tqdm(test_data_loader)) :\n",
        "        with torch.no_grad():\n",
        "            logit_output = model(\n",
        "              input_ids = data['input_ids'].to(device),\n",
        "              attention_mask = data['attention_mask'].to(device),\n",
        "              premise_special_token_index = data['premise_special_token_index'].to(device),\n",
        "              hypothesis_special_token_index = data['hypothesis_special_token_index'].to(device),\n",
        "              labels=None\n",
        "            )\n",
        "        list_logits.extend(logit_output.cpu().detach().numpy())\n",
        "    output_probs = F.softmax(torch.Tensor(list_logits), dim=1)\n",
        "    oof_pred.append(np.array(output_probs)[:,np.newaxis])\n",
        "\n",
        "    # Prevent OOM error\n",
        "    model.cpu()\n",
        "    del model\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "SyLQA2T_lvrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean probability of fold predictions\n",
        "oof_pred_mean = np.mean(oof_pred, axis=0)\n",
        "print(oof_pred_mean.shape) # (1666, 1, 3)\n",
        "print(oof_pred_mean[:10])"
      ],
      "metadata": {
        "id": "0BeSNCO-l0qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_label = []\n",
        "all_probability = []\n",
        "for pred in oof_pred_mean:\n",
        "  probability = flatten(pred)\n",
        "  all_probability.append(probability)\n",
        "  single_label = np.argmax(probability)\n",
        "  all_label.append(single_label)\n",
        "\n",
        "df_oof = df_test_dataset.copy()\n",
        "df_oof[\"label\"] = num_to_label(all_label)\n",
        "df_oof[\"label\"] = df_oof[\"label\"].str.strip()\n",
        "df_oof[\"probability\"] = all_probability\n",
        "df_oof = df_oof.sort_values(by=['index']).reset_index(drop=True)\n",
        "df_oof.head(24)"
      ],
      "metadata": {
        "id": "5axpERGjl0oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_oof['label'].value_counts()"
      ],
      "metadata": {
        "id": "dK1xqlABl0lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model_name = DIR_PATH.split(\"/\")[-1]\n",
        "print(save_model_name)\n",
        "\n",
        "result_save_path = f'./result-{save_model_name}-{CFG.num_epochs}-epochs-best-{CRITERIA}.csv'\n",
        "df_oof.to_csv(result_save_path, index=False)\n",
        "\n",
        "df_submission = df_oof.copy()\n",
        "df_submission = df_submission.sort_values(by=['index'])\n",
        "df_submission.reset_index(drop=True, inplace=True)\n",
        "df_submission = df_submission[[\"index\", \"label\"]]\n",
        "display(df_submission.head(6))\n",
        "display(df_submission.tail(6))\n",
        "\n",
        "submission_save_path = f'./submission-{save_model_name}-{CFG.num_epochs}-epochs-best-{CRITERIA}.csv'\n",
        "df_submission.to_csv(submission_save_path, index=False)"
      ],
      "metadata": {
        "id": "_cEoJdsWl0iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kglZIbR5lvon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2ë“± ë…¸íŠ¸ë¶"
      ],
      "metadata": {
        "id": "MZrbF6BN4Oe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Private 3ë“± | 0.89915 | 2.Custom Model R-Roberta\n",
        "- https://dacon.io/competitions/official/235875/codeshare/4631?page=1&dtype=recent"
      ],
      "metadata": {
        "id": "7QJt3Ey04Z2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keypoint**\n",
        "- í•™ìŠµ KLUE Official Dev Dataë¥¼ ì¶”ê°€ ì‚¬ìš©\n",
        "- R-BERT ëª¨ë¸ êµ¬ì¡° ì°¨ìš©í•˜ì—¬ ëª¨ë¸ ì•„í‚¤í…ì²˜ ìˆ˜ì •\n",
        "-  5-Fold Soft Ensemble ì„ ì‹œë„í•˜ë ¤ í–ˆì§€ë§Œ 4ë²ˆì§¸ì™€ 5ë²ˆì§¸ Foldë§Œ í•™ìŠµì™„ë£Œ\n",
        "- ì´ë“¤ ì¤‘ Public Score 0.897ì„ ê¸°ë¡í•œ 4ë²ˆì§¸ Fold ëª¨ë¸ë§Œì„ Inferenceì— ì‚¬ìš©"
      ],
      "metadata": {
        "id": "p31NnPWY46SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers wandb"
      ],
      "metadata": {
        "id": "bmPzeqnw4ucH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle as pickle\n",
        "import os\n",
        "import math\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "tqdm.pandas()\n",
        "# Step 4. í•œê¸€ ê¸€ê¼´ ì„¤ì •\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pprint import  pprint\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from collections import defaultdict, Counter\n",
        "from itertools import chain\n",
        "from pprint import pprint\n",
        "import wandb\n",
        "# from pycaret.classification import *\n",
        "# from pycaret.regression import *\n",
        "# from pycaret.utils import check_metric\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification, AutoConfig\n",
        "from transformers import BertConfig, BertForSequenceClassification, Trainer, TrainingArguments, BertModel, ElectraModel, RobertaModel\n",
        "from importlib import import_module\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "def seed_everything(seed: int = 42, contain_cuda: bool = False):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    print(f\"Seed set as {seed}\")\n",
        "\n",
        "seed=42\n",
        "seed_everything(seed)\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/\"\n",
        "project_folder = \"Konli\"\n",
        "os.chdir(os.path.join(root_dir,project_folder))\n"
      ],
      "metadata": {
        "id": "_0cBrBqSmAhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wrong_batch_for_wandb(tokenizer,\n",
        "                          wrong_sample_index,\n",
        "                          input_ids,\n",
        "                          valid_labels,\n",
        "                          valid_predict,\n",
        "                          valid_output,\n",
        "                          ):\n",
        "    num_to_label_dict = {0:'entailment',1:'contradiction',2:'neutral',}\n",
        "\n",
        "    wrong_sample_index = np.where(valid_labels!=valid_predict)[0]\n",
        "    wrong_sample_text = [tokenizer.decode(element, skip_special_tokens=False) for element in input_ids[wrong_sample_index]]\n",
        "    wrong_sample_label = [num_to_label_dict[lab] for lab in list(valid_labels[wrong_sample_index])]\n",
        "    wrong_sample_pred = [num_to_label_dict[pred] for pred in list(valid_predict[wrong_sample_index])]\n",
        "    wrong_sample_output = valid_output[wrong_sample_index].tolist()\n",
        "\n",
        "    entailment_prob, contradiction_prob, neutral_prob = [], [], []\n",
        "    for element in wrong_sample_output:\n",
        "        entailment_prob.append(element[0])\n",
        "        contradiction_prob.append(element[1])\n",
        "        neutral_prob.append(element[2])\n",
        "\n",
        "    return wrong_sample_text, wrong_sample_label, wrong_sample_pred, entailment_prob, contradiction_prob, neutral_prob\n"
      ],
      "metadata": {
        "id": "Oi6VzVWwmAfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset êµ¬ì„±.\n",
        "class NLI_Dataset(Dataset):\n",
        "    def __init__(self, tokenized_dataset, labels):\n",
        "        self.tokenized_dataset = tokenized_dataset\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.tokenized_dataset.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "6zUCmKmdmAdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_dataset(args, dataset):\n",
        "\n",
        "    if 'answer' not in dataset.label: # Train Dataì¼ ë•Œì—ë§Œ\n",
        "        dataset = dataset.loc[dataset.label.isnull()==False,:] # label ê¸°ì¤€ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ìžˆëŠ” í–‰ ì œê±°\n",
        "        dataset = dataset.drop_duplicates(['premise','hypothesis','label']) # ì¤‘ë³µë˜ëŠ” ë°ì´í„°ê°€ ìžˆì—ˆë„¤.. ì¤‘ë³µì œê±°\n",
        "\n",
        "        # # Test Setì—ëŠ” ì—†ëŠ” íŠ¹ìˆ˜ë¬¸ìž í¬í•¨í•œ ë¬¸ìž¥ë“¤ì´ Train Setì— ì¡´ìž¬.\n",
        "        # # ì˜¤ížˆë ¤ í•™ìŠµì— í˜¼ëž€ì„ ì£¼ëŠ” ê²ƒ ê°™ì€ë°.. ì œê±°í• ì§€ ë§ì§€ ê³ ë¯¼ì¤‘\n",
        "        # dataset = dataset.loc[dataset.premise.apply(lambda x: '\"' in x)==False]\n",
        "        # dataset = dataset.loc[dataset.premise.apply(lambda x: '\"' in x)==False]\n",
        "        # dataset = dataset.loc[dataset.premise.apply(lambda x: '%' in x)==False] # ì œê±°í•´ì•¼í•˜ë‚˜..\n",
        "        # dataset = dataset.loc[dataset.premise.apply(lambda x: '~' in x)==False] # ì œê±°í•´ì•¼í•˜ë‚˜..\n",
        "\n",
        "        # Label Encoding\n",
        "        label_to_num_dict = {'entailment':0,'contradiction':1,'neutral':2,}\n",
        "        dataset['labels'] = dataset.label.map(label_to_num_dict)\n",
        "\n",
        "    return dataset.reset_index(drop=True)\n",
        "\n",
        "def load_data(args, dataset_dir):\n",
        "    print(\"===================loading data=====================\")\n",
        "    # load dataset\n",
        "    dataset = pd.read_csv(dataset_dir)\n",
        "    \n",
        "    # preprecessing dataset\n",
        "    dataset = preprocessing_dataset(args, dataset)\n",
        "    # print(dataset)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# bert inputì„ ìœ„í•œ tokenizing.\n",
        "def tokenized_dataset(args, dataset, tokenizer):\n",
        "    lst_premise = dataset['premise'].tolist()\n",
        "    lst_hypothesis = dataset['hypothesis'].tolist()\n",
        "\n",
        "    tokenized_sentences = tokenizer(\n",
        "        lst_premise,\n",
        "        lst_hypothesis,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=args.seq_max_len,\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return tokenized_sentences\n",
        "\n",
        "    all_dataset = load_data(args, dataset_dir = f'./data/{args.train_file}')\n",
        "\n",
        "def add_extra_data(args, dataset, label):\n",
        "    if args.add_klue_data==True:\n",
        "        klue_extra_data = load_data(args, dataset_dir = f'./data/klue_extra_data.csv')\n",
        "        klue_extra_label = klue_extra_data['labels'].values\n",
        "        dataset = pd.concat([dataset, klue_extra_data],axis=0)\n",
        "        label = np.hstack([label, klue_extra_label])\n",
        "    if args.add_nikl_data==True:\n",
        "        nikl_extra_data = load_data(args, dataset_dir = f'./data/nikl_extra_data.csv')\n",
        "        nikl_extra_label = nikl_extra_data['labels'].values\n",
        "        dataset = pd.concat([dataset, nikl_extra_data],axis=0)\n",
        "        label = np.hstack([label, nikl_extra_label])\n",
        "    dataset.reset_index(drop=True, inplace=True)\n",
        "    return dataset, label"
      ],
      "metadata": {
        "id": "Jqbr5eYMmAa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trainLoader(args, train_data, valid_data, train_label, valid_label, tokenizer):\n",
        "\n",
        "    # entity_between = '</s></s>' if args.model == 'r_roberta' or args.model == 'roberta' else '[SEP]'\n",
        "    tokenized_train = tokenized_dataset(args, train_data, tokenizer)\n",
        "    tokenized_valid = tokenized_dataset(args, valid_data, tokenizer)\n",
        "\n",
        "    # make dataset for pytorch.\n",
        "    NLI_train_dataset = NLI_Dataset(tokenized_train, train_label)\n",
        "    NLI_valid_dataset = NLI_Dataset(tokenized_valid, valid_label)\n",
        "\n",
        "    trainloader = DataLoader(NLI_train_dataset,\n",
        "                             batch_size=args.batch_size,\n",
        "                             shuffle=True,\n",
        "                             num_workers=args.num_workers,\n",
        "                             )\n",
        "\n",
        "    validloader = DataLoader(NLI_valid_dataset,\n",
        "                             batch_size=args.batch_size,\n",
        "                             shuffle=False,\n",
        "                             num_workers=args.num_workers,\n",
        "                             )\n",
        "\n",
        "    return trainloader, validloader\n"
      ],
      "metadata": {
        "id": "Zqa9e7OfmAYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam, AdamW\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import math\n",
        "\n",
        "class AdamP(Optimizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params,\n",
        "        lr=1e-3,\n",
        "        betas=(0.9, 0.999),\n",
        "        eps=1e-8,\n",
        "        weight_decay=0,\n",
        "        delta=0.1,\n",
        "        wd_ratio=0.1,\n",
        "        nesterov=False,\n",
        "    ):\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            betas=betas,\n",
        "            eps=eps,\n",
        "            weight_decay=weight_decay,\n",
        "            delta=delta,\n",
        "            wd_ratio=wd_ratio,\n",
        "            nesterov=nesterov,\n",
        "        )\n",
        "        super(AdamP, self).__init__(params, defaults)\n",
        "\n",
        "    def _channel_view(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "    def _layer_view(self, x):\n",
        "        return x.view(1, -1)\n",
        "\n",
        "    def _cosine_similarity(self, x, y, eps, view_func):\n",
        "        x = view_func(x)\n",
        "        y = view_func(y)\n",
        "\n",
        "        return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\n",
        "\n",
        "    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n",
        "        wd = 1\n",
        "        expand_size = [-1] + [1] * (len(p.shape) - 1)\n",
        "        for view_func in [self._channel_view, self._layer_view]:\n",
        "\n",
        "            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n",
        "\n",
        "            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n",
        "                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n",
        "                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n",
        "                wd = wd_ratio\n",
        "\n",
        "                return perturb, wd\n",
        "\n",
        "        return perturb, wd\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad.data\n",
        "                beta1, beta2 = group[\"betas\"]\n",
        "                nesterov = group[\"nesterov\"]\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state[\"step\"] = 0\n",
        "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
        "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
        "\n",
        "                # Adam\n",
        "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "                bias_correction1 = 1 - beta1 ** state[\"step\"]\n",
        "                bias_correction2 = 1 - beta2 ** state[\"step\"]\n",
        "\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "\n",
        "                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(\n",
        "                    group[\"eps\"]\n",
        "                )\n",
        "                step_size = group[\"lr\"] / bias_correction1\n",
        "\n",
        "                if nesterov:\n",
        "                    perturb = (beta1 * exp_avg + (1 - beta1) * grad) / denom\n",
        "                else:\n",
        "                    perturb = exp_avg / denom\n",
        "\n",
        "                # Projection\n",
        "                wd_ratio = 1\n",
        "                if len(p.shape) > 1:\n",
        "                    perturb, wd_ratio = self._projection(\n",
        "                        p,\n",
        "                        grad,\n",
        "                        perturb,\n",
        "                        group[\"delta\"],\n",
        "                        group[\"wd_ratio\"],\n",
        "                        group[\"eps\"],\n",
        "                    )\n",
        "\n",
        "                # Weight decay\n",
        "                if group[\"weight_decay\"] > 0:\n",
        "                    p.data.mul_(1 - group[\"lr\"] * group[\"weight_decay\"] * wd_ratio)\n",
        "\n",
        "                # Step\n",
        "                p.data.add_(perturb, alpha=-step_size)\n",
        "\n",
        "        return loss\n",
        "\n",
        "def get_optimizer(model, args):\n",
        "    if args.optimizer == \"Adam\":\n",
        "        optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=0.01)\n",
        "    elif args.optimizer == \"AdamW\":\n",
        "        optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=0.01)\n",
        "    elif args.optimizer == \"AdamP\":\n",
        "        optimizer = AdamP(\n",
        "            model.parameters(),\n",
        "            lr=args.lr,\n",
        "            betas=(0.9, 0.999),\n",
        "            weight_decay=0.01,\n",
        "            delta=0.1,\n",
        "            wd_ratio=0.1,\n",
        "            nesterov=False,\n",
        "        )\n",
        "\n",
        "\n",
        "    # ëª¨ë“  parameterë“¤ì˜ gradê°’ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    return optimizer\n"
      ],
      "metadata": {
        "id": "dlp60e5vmAV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, _LRScheduler\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup/blob/master/cosine_annearing_with_warmup.py\n",
        "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
        "    \"\"\"\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        first_cycle_steps (int): First cycle step size.\n",
        "        cycle_mult(float): Cycle steps magnification. Default: -1.\n",
        "        max_lr(float): First cycle's max learning rate. Default: 0.1.\n",
        "        min_lr(float): Min learning rate. Default: 0.001.\n",
        "        warmup_steps(int): Linear warmup step size. Default: 0.\n",
        "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n",
        "        last_epoch (int): The index of last epoch. Default: -1.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,\n",
        "                 optimizer : torch.optim.Optimizer,\n",
        "                 first_cycle_steps : int,\n",
        "                 cycle_mult : float = 1.,\n",
        "                 max_lr : float = 0.1,\n",
        "                 min_lr : float = 0.001,\n",
        "                 warmup_steps : int = 0,\n",
        "                 gamma : float = 1.,\n",
        "                 last_epoch : int = -1\n",
        "        ):\n",
        "        assert warmup_steps < first_cycle_steps\n",
        "        \n",
        "        self.first_cycle_steps = first_cycle_steps # first cycle step size\n",
        "        self.cycle_mult = cycle_mult # cycle steps magnification\n",
        "        self.base_max_lr = max_lr # first max learning rate\n",
        "        self.max_lr = max_lr # max learning rate in the current cycle\n",
        "        self.min_lr = min_lr # min learning rate\n",
        "        self.warmup_steps = warmup_steps # warmup step size\n",
        "        self.gamma = gamma # decrease rate of max learning rate by cycle\n",
        "        \n",
        "        self.cur_cycle_steps = first_cycle_steps # first cycle step size\n",
        "        self.cycle = 0 # cycle count\n",
        "        self.step_in_cycle = last_epoch # step size of the current cycle\n",
        "        \n",
        "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
        "        \n",
        "        # set learning rate min_lr\n",
        "        self.init_lr()\n",
        "    \n",
        "    def init_lr(self):\n",
        "        self.base_lrs = []\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.min_lr\n",
        "            self.base_lrs.append(self.min_lr)\n",
        "    \n",
        "    def get_lr(self):\n",
        "        if self.step_in_cycle == -1:\n",
        "            return self.base_lrs\n",
        "        elif self.step_in_cycle < self.warmup_steps:\n",
        "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr + (self.max_lr - base_lr) \\\n",
        "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\n",
        "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
        "                    for base_lr in self.base_lrs]\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "            self.step_in_cycle = self.step_in_cycle + 1\n",
        "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
        "                self.cycle += 1\n",
        "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
        "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
        "        else:\n",
        "            if epoch >= self.first_cycle_steps:\n",
        "                if self.cycle_mult == 1.:\n",
        "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
        "                    self.cycle = epoch // self.first_cycle_steps\n",
        "                else:\n",
        "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
        "                    self.cycle = n\n",
        "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
        "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
        "            else:\n",
        "                self.cur_cycle_steps = self.first_cycle_steps\n",
        "                self.step_in_cycle = epoch\n",
        "                \n",
        "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n",
        "        self.last_epoch = math.floor(epoch)\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def get_scheduler(optimizer, args, total_batch_):\n",
        "    if args.scheduler == \"plateau\":\n",
        "        scheduler = ReduceLROnPlateau(\n",
        "            optimizer, patience=2, factor=0.85, mode=\"max\", verbose=True\n",
        "        )\n",
        "    elif args.scheduler == \"linear\":\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            # num_warmup_steps=int(total_batch_*args.epochs*0.1),\n",
        "            num_warmup_steps=args.warmup_steps,\n",
        "            num_training_steps=int(total_batch_*args.epochs),\n",
        "        )\n",
        "    elif args.scheduler == \"cosine\":\n",
        "        scheduler = CosineAnnealingWarmupRestarts( # ver1: first_cycle=20, warmup_steps=5, cycle_mult=1.0, max_lr=args.lr, min_lr=args.lr/100, gamma=0.8, patience=7, \n",
        "            optimizer,                             # ver2: first_cycle=30, warmup_steps=5, cycle_mult=0.8, max_lr=args.lr, min_lr=args.lr/100, gamma=0.8, patience=5\n",
        "            first_cycle_steps=300,                  # ver3: first_cycle=50, warmup_steps=10, cycle_mult=1.0, max_lr=args.lr, min_lr=args.lr/100, gamma=0.8, patience=7\n",
        "            warmup_steps=args.warmup_steps,\n",
        "            cycle_mult=args.cycle_mult,\n",
        "            max_lr=args.lr,\n",
        "            min_lr=args.lr * 0.01,\n",
        "            gamma=0.8,\n",
        "        )\n",
        "\n",
        "    return scheduler\n",
        "\n"
      ],
      "metadata": {
        "id": "cqBRSSUUmMCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class kobert_Classifier(nn.Module):\n",
        "    def __init__(self, bert, hidden_size=768, num_classes=3, dr_rate=0.0):\n",
        "        super(kobert_Classifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.pooler = nn.Linear(hidden_size, hidden_size)\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "        torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
        "        self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "    def forward(self, token_ids, attention_mask, segment_ids):\n",
        "        # TypeError: dropout(): argument 'input' (position 1) must be Tensor, not tuple\n",
        "        out = self.bert(input_ids=token_ids, attention_mask=attention_mask, token_type_ids=segment_ids)[0]\n",
        "        out = out[:, 0, :]\n",
        "        out = self.pooler(out)\n",
        "        out = torch.nn.functional.tanh(out)  # although BERT uses tanh here, it seems Electra authors used gelu here\n",
        "\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(out)\n",
        "        \n",
        "        return self.classifier(out)\n",
        "    \n",
        "\n",
        "class roberta_large_Classifier(nn.Module):\n",
        "    def __init__(self, roberta, hidden_size=1024, num_classes=3, dr_rate=0.0):\n",
        "        super(roberta_large_Classifier, self).__init__()\n",
        "        self.roberta = roberta\n",
        "        self.dr_rate = dr_rate\n",
        "        \n",
        "        self.pooler = FCLayer(hidden_size, hidden_size//2, self.dr_rate)\n",
        "        self.classifier = FCLayer(hidden_size//2, num_classes, self.dr_rate, False)\n",
        "\n",
        "    def forward(self, token_ids, attention_mask, segment_ids=None):\n",
        "        out = self.roberta(input_ids=token_ids, attention_mask=attention_mask)[0]\n",
        "        \n",
        "        out = out[:, 0, :] # take <s> token (equiv. to [CLS])\n",
        "        out = self.pooler(out)\n",
        "\n",
        "        return self.classifier(out)\n",
        "\n",
        "\n",
        "# Relation Extraction R-BERT ì•„ì´ë””ì–´ ì°¨ìš©\n",
        "# https://github.com/monologg/R-BERT/blob/master/model.py#L21\n",
        "class r_roberta_Classifier(nn.Module):\n",
        "    def __init__(self, roberta, hidden_size=1024, num_classes=3, dr_rate=0.0):\n",
        "        super(r_roberta_Classifier, self).__init__()\n",
        "        self.roberta = roberta\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.cls_fc = FCLayer(hidden_size, hidden_size//2, self.dr_rate)\n",
        "        self.sentence_fc = FCLayer(hidden_size, hidden_size//2, self.dr_rate)\n",
        "        self.label_classifier = FCLayer(hidden_size//2 * 3, num_classes, self.dr_rate, False)\n",
        "\n",
        "    def forward(self, token_ids, attention_mask, segment_ids=None):\n",
        "        out = self.roberta(input_ids=token_ids, attention_mask=attention_mask)[0]\n",
        "        \n",
        "        sentence_end_position = torch.where(token_ids == 2)[1]\n",
        "        sent1_end, sent2_end = sentence_end_position[0], sentence_end_position[1]\n",
        "        \n",
        "        cls_vector = out[:, 0, :] # take <s> token (equiv. to [CLS])\n",
        "        prem_vector = out[:,1:sent1_end]              # Get Premise vector\n",
        "        hypo_vector = out[:,sent1_end+1:sent2_end]    # Get Hypothesis vector\n",
        "\n",
        "        prem_vector = torch.mean(prem_vector, dim=1) # Average\n",
        "        hypo_vector = torch.mean(hypo_vector, dim=1)\n",
        "\n",
        "        \n",
        "        # Dropout -> tanh -> fc_layer (Share FC layer for premise and hypothesis)\n",
        "        cls_embedding = self.cls_fc(cls_vector)\n",
        "        prem_embedding = self.sentence_fc(prem_vector)\n",
        "        hypo_embedding = self.sentence_fc(hypo_vector)\n",
        "        \n",
        "        # Concat -> fc_layer\n",
        "        concat_embedding = torch.cat([cls_embedding, prem_embedding, hypo_embedding], dim=-1)\n",
        "        \n",
        "        return self.label_classifier(concat_embedding)\n",
        "\n",
        "\n",
        "class FCLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n",
        "        super(FCLayer, self).__init__()\n",
        "        self.use_activation = use_activation\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        \n",
        "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        if self.use_activation:\n",
        "            x = self.tanh(x)\n",
        "        return self.linear(x)"
      ],
      "metadata": {
        "id": "Mp_xnuqbmL_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(args):\n",
        "    if args.model == 'kobert':\n",
        "        # tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"kykim/bert-kor-base\")\n",
        "        \n",
        "    elif args.model == 'klue_roberta_large':\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
        "\n",
        "    elif args.model == 'r_klue_roberta':\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
        "\n",
        "    elif args.model == 'r_roberta':\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
        "\n",
        "    else:\n",
        "    \traise NotImplementedError('Tokenizer & Model not available')\n",
        "\n",
        "    return tokenizer\n"
      ],
      "metadata": {
        "id": "EOHSFmTXmL9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(args):\n",
        "    if args.model == 'kobert':\n",
        "    \t# feature_model = BertModel.from_pretrained(\"monologg/kobert\")\n",
        "    \tfeature_model = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
        "    \tmodel = kobert_Classifier(feature_model, dr_rate=args.dp)\n",
        "\n",
        "    elif args.model == 'klue_roberta_large':\t# 1024\n",
        "        feature_model = RobertaModel.from_pretrained(\"klue/roberta-large\", add_pooling_layer=False)\n",
        "        model = roberta_large_Classifier(feature_model, dr_rate=args.dp)\n",
        "\n",
        "    elif args.model == 'r_roberta':\n",
        "        feature_model = RobertaModel.from_pretrained(\"xlm-roberta-large\", add_pooling_layer=False)\n",
        "        model = r_roberta_Classifier(feature_model, dr_rate=args.dp)\n",
        "    \n",
        "    elif args.model == 'r_klue_roberta':\n",
        "        feature_model = RobertaModel.from_pretrained(\"klue/roberta-large\", add_pooling_layer=False)\n",
        "        model = r_roberta_Classifier(feature_model, dr_rate=args.dp)\n",
        "\n",
        "    else:\n",
        "    \traise NotImplementedError('Tokenizer & Model not available')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "JDSZe0MgmL61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/8\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, weight=None,\n",
        "                 gamma=2., reduction='mean'):\n",
        "        nn.Module.__init__(self)\n",
        "        self.weight = weight\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, input_tensor, target_tensor):\n",
        "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
        "        prob = torch.exp(log_prob)\n",
        "        return F.nll_loss(\n",
        "            ((1 - prob) ** self.gamma) * log_prob,\n",
        "            target_tensor,\n",
        "            weight=self.weight,\n",
        "            reduction=self.reduction\n",
        "        )\n",
        "\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes=3, smoothing=0.0, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
        "\n",
        "\n",
        "def get_criterion(args):\n",
        "    if args.smoothing!=0 and args.criterion == 'smoothing':\n",
        "        criterion = LabelSmoothingLoss(smoothing=args.smoothing)\n",
        "    elif args.criterion == 'cross':\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "    elif args.criterion == 'focal':\n",
        "        criterion = FocalLoss(gamma=2.0)\n",
        "    else:\n",
        "        raise NotImplementedError('Criterion not available')\n",
        "    return criterion"
      ],
      "metadata": {
        "id": "79onVVu4mL3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args, wandb, fold_lst=[4,5]):\n",
        "    criterion = get_criterion(args)\n",
        "    tokenizer = get_tokenizer(args)\n",
        "    all_dataset = load_data(args, dataset_dir = f'./data/{args.train_file}')\n",
        "    all_label = all_dataset['labels'].values\n",
        "\n",
        "    kf = StratifiedKFold(n_splits=args.n_splits, random_state=42, shuffle=True)\n",
        "    fold_idx = 1\n",
        "    best_val_acc_list = []\n",
        "    for train_index, test_index in kf.split(all_dataset, all_label):\n",
        "        if fold_idx not in fold_lst:\n",
        "            fold_idx+=1\n",
        "            continue\n",
        "\n",
        "        run = wandb.init(project=args.project_name)\n",
        "        wandb.run.name = f'{args.model_name}/{fold_idx}-fold'\n",
        "        wandb.config.update(args)\n",
        "\n",
        "        os.makedirs(f'./models/{args.model_name}/{fold_idx}-fold', exist_ok=True)\n",
        "        ### Model Select\n",
        "        model = get_model(args)\n",
        "        print('===================get model===================')\n",
        "        model.to(device)\n",
        "\n",
        "\n",
        "        train_data, valid_data = all_dataset.iloc[train_index], all_dataset.iloc[test_index]\n",
        "        train_label, valid_label = all_label[train_index], all_label[test_index]\n",
        "        \n",
        "        print(f\"len(train_label) : {len(train_label)}\")\n",
        "        print(f\"len(train_data) : {len(train_data)}\")\n",
        "        if args.add_klue_data or args.add_nikl_data: # ì™¸ë¶€ ë°ì´í„° í™œìš©\n",
        "            train_data, train_label = add_extra_data(args, train_data, train_label)\n",
        "            print('='*15,'Extra Data Added','='*15)\n",
        "        print(f\"len(train_label) : {len(train_label)}\")\n",
        "        print(f\"len(train_data) : {len(train_data)}\")\n",
        "\n",
        "        trainloader, validloader = get_trainLoader(args, train_data, valid_data, train_label, valid_label, tokenizer)\t\n",
        "        total_batch_ = len(trainloader)\n",
        "        valid_batch_ = len(validloader)\n",
        "\n",
        "        ### Optimizer\n",
        "        optimizer = get_optimizer(model, args)\n",
        "\n",
        "        ### Scheduler\n",
        "        scheduler = get_scheduler(optimizer, args, total_batch_)\n",
        "\n",
        "        best_val_loss, best_val_acc, = np.inf, 0\n",
        "        early_stopping_counter = 0\n",
        "\n",
        "        print(f\"---------------------------------- {fold_idx} fold----------------------------------\")\t\n",
        "        for i in tqdm(range(1, args.epochs+1)):\n",
        "            model.train()\n",
        "            epoch_perform, batch_perform = np.zeros(2), np.zeros(2)\t\n",
        "            print()\n",
        "            progress_bar = tqdm(enumerate(trainloader), total=len(trainloader), leave=True, position=0,)\n",
        "            for j, v in progress_bar:\n",
        "                input_ids, attention_mask, labels = v['input_ids'].to(device), v['attention_mask'].to(device), v['labels'].to(device)\n",
        "\n",
        "                if 'roberta' in args.model:\n",
        "                    token_type_ids = None\n",
        "                else:\n",
        "                    token_type_ids = v['token_type_ids'].to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                output = model(input_ids, attention_mask, token_type_ids) ## labelì„ ì•ˆ ë„£ì–´ì„œ logitsê°’ë§Œ ì¶œë ¥\t\n",
        "\n",
        "                loss = criterion(output, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                for learning_rate in scheduler.get_lr():\n",
        "                    wandb.log({\"learning_rate\": learning_rate})\n",
        "\n",
        "\n",
        "                predict = output.argmax(dim=-1)\n",
        "                predict = predict.detach().cpu().numpy()\n",
        "                labels = labels.detach().cpu().numpy()\t\n",
        "                acc = accuracy_score(labels, predict)\n",
        "\n",
        "                batch_perform += np.array([loss.item(), acc])\n",
        "                epoch_perform += np.array([loss.item(), acc])\n",
        "\n",
        "                if (j + 1) % 50 == 0:\n",
        "                    print(\n",
        "                        f\"Epoch {i:#04d} #{j + 1:#03d} -- loss: {batch_perform[0] / 50:#.5f}, acc: {batch_perform[1] / 50:#.4f}\"\n",
        "                        )\n",
        "                    batch_perform = np.zeros(2)\n",
        "            print()\n",
        "            print(\n",
        "                f\"Epoch {i:#04d} loss: {epoch_perform[0] / total_batch_:#.5f}, acc: {epoch_perform[1] / total_batch_:#.2f}\"\n",
        "                )\n",
        "            wandb.log({\n",
        "                \"epoch\": i,\n",
        "                \"Train epoch Loss\": epoch_perform[0] / total_batch_,\n",
        "                \"Train epoch Acc\": epoch_perform[1] / total_batch_}\n",
        "                )\n",
        "            \n",
        "            ###### Validation\t\n",
        "            model.eval()\n",
        "            valid_perform = np.zeros(2)\n",
        "\n",
        "            all_valid_predict_lst = []\n",
        "            all_valid_labels_lst = []\n",
        "\n",
        "            # í‹€ë¦° ë°ì´í„°ë“¤ì„ wandb ê¸°ë¡í•˜ê¸° ìœ„í•¨.\n",
        "            wrong_sample_dict = defaultdict(list)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for v in validloader:\n",
        "                    input_ids, attention_mask, valid_labels = v['input_ids'].to(device), v['attention_mask'].to(device), v['labels'].to(device)\n",
        "\n",
        "                    if 'roberta' in args.model:\n",
        "                        token_type_ids = None\n",
        "                    else:\n",
        "                        token_type_ids = v['token_type_ids'].to(device)\n",
        "\n",
        "                    valid_output = model(input_ids, attention_mask, token_type_ids)\n",
        "                    valid_loss = criterion(valid_output, valid_labels)\t\n",
        "\n",
        "                    valid_predict = valid_output.argmax(dim=-1)\n",
        "                    valid_predict = valid_predict.detach().cpu().numpy()\n",
        "                    valid_labels = valid_labels.detach().cpu().numpy()\t\n",
        "\n",
        "                    ###########################\n",
        "                    # valid eval ê²°ê³¼, í‹€ë¦° ë°ì´í„°ë“¤ì€ wandbì— Logging\n",
        "                    if args.logging_wrong_samples:\n",
        "                        wrong_sample_index = np.where(valid_labels!=valid_predict)[0]\n",
        "                        if len(wrong_sample_index)>0:\n",
        "                            wrong_sample_text, wrong_sample_label, wrong_sample_pred, entailment_prob, contradiction_prob, neutral_prob = wrong_batch_for_wandb(tokenizer, wrong_sample_index, input_ids, valid_labels, valid_predict, valid_output)\n",
        "\n",
        "                            wrong_sample_dict['ìž…ë ¥ ë¬¸ìž¥ Pair'] += wrong_sample_text\n",
        "                            wrong_sample_dict['ì‹¤ì œê°’'] += wrong_sample_label\n",
        "                            wrong_sample_dict['ì˜ˆì¸¡ê°’'] += wrong_sample_pred\n",
        "                            wrong_sample_dict['entailment_logit'] += entailment_prob\n",
        "                            wrong_sample_dict['contradiction_logit'] += contradiction_prob\n",
        "                            wrong_sample_dict['neutral_logit'] += neutral_prob\n",
        "                    ###########################\n",
        "\n",
        "                    valid_acc = accuracy_score(valid_labels, valid_predict)\t\n",
        "                    valid_perform += np.array([valid_loss.item(), valid_acc])\n",
        "\n",
        "                    all_valid_predict_lst += list(valid_predict)\n",
        "                    all_valid_labels_lst += list(valid_labels)\n",
        "            \n",
        "            ###### Model save\n",
        "            val_total_loss = valid_perform[0] / valid_batch_\n",
        "            val_total_acc = valid_perform[1] / valid_batch_\n",
        "            best_val_loss = min(best_val_loss, val_total_loss)\n",
        "        \n",
        "            if val_total_acc > best_val_acc:    #  and val_total_acc >= 0.25\n",
        "                print(f\"New best model for val accuracy : {val_total_acc:#.4f}! saving the best model..\")\n",
        "                torch.save(model.state_dict(), f\"./models/{args.model_name}/{fold_idx}-fold/best.pt\")\n",
        "                \n",
        "                # ì°¸ê³  : Model ì¶”ê°€ ìž¬í•™ìŠµì„ ìœ„í•œ ëª¨ë¸ì„ ì €ìž¥í•˜ëŠ” ì½”ë“œ\n",
        "                # https://tutorials.pytorch.kr/beginner/saving_loading_models.html#checkpoint\n",
        "\n",
        "                best_val_acc = val_total_acc\n",
        "                early_stopping_counter = 0\n",
        "\n",
        "                ### Confusion Matrix\n",
        "                class_names = ['entailment','contradiction','neutral'] # (0,1,2)\n",
        "                # https://wandb.ai/wandb/plots/reports/Confusion-Matrix--VmlldzozMDg1NTM\n",
        "                wandb.log({f\"{i}th_epoch_conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
        "                            y_true=all_valid_labels_lst, preds=all_valid_predict_lst,\n",
        "                            class_names=class_names)})\n",
        "                \n",
        "                if args.logging_wrong_samples and val_total_acc > 0.91:\n",
        "                    ########### Logging Wrong Samples ##########\n",
        "                    # Save Wrong DataFrame\n",
        "                    wrong_sample_df = pd.DataFrame(wrong_sample_dict)\n",
        "                    wrong_sample_df.to_csv(f\"./models/{args.model_name}/{fold_idx}-fold/wrong_df.csv\",index=False)\n",
        "                    print('='*15,f'{fold_idx}-Fold Wrong DataFrame Saved','='*15)\n",
        "                    # Loggin Wandb\n",
        "                    text_table = wandb.Table(data = wrong_sample_df)\n",
        "                    run.log({f\"{fold_idx}th_fold_wrong_samples\" : text_table})\n",
        "                    ###########################\n",
        "            \n",
        "            else: # bestë³´ë‹¤ scoreê°€ ì•ˆ ì¢‹ì„ ë•Œ, early stopping check\n",
        "                early_stopping_counter += 1\n",
        "                if early_stopping_counter >= args.patience:\n",
        "                    print(\n",
        "                        f\"EarlyStopping counter: {early_stopping_counter} out of {args.patience}\"\n",
        "                    )\n",
        "                    break\n",
        "\n",
        "            print()\n",
        "            print(\n",
        "                f\">>>> Validation loss: {val_total_loss:#.5f}, Acc: {val_total_acc:#.4f}\"\n",
        "                )\n",
        "            print()\n",
        "            wandb.log({\n",
        "                \"epoch\": i,\n",
        "                \"Valid Loss\": val_total_loss,\n",
        "                \"Valid Acc\": val_total_acc}\n",
        "                )\n",
        "\n",
        "        best_val_acc_list.append(best_val_acc)\n",
        "        fold_idx +=1\n",
        "    print('='*50)\n",
        "    print(f\"{args.n_splits}-fold best_val_acc_list : {best_val_acc_list}\")\n",
        "    print('='*15, f'{args.n_splits}-fold Final Score(ACC) : {np.mean(best_val_acc_list)}', '='*15)\n",
        "    wandb.log({\n",
        "    f\"Total Mean ACC ({args.n_splits}-fold)\": np.mean(best_val_acc_list)}\n",
        "    )"
      ],
      "metadata": {
        "id": "Cd3QnXRimATK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import easydict\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'current device : {device}')\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "        \"seed\":42,\n",
        "        \"optimizer\":\"AdamW\",    # help = (AdamW, Adam, AdamP)\n",
        "        \"scheduler\":\"linear\",     # help= (linear, cosine, plateau ...)\n",
        "        \"warmup_steps\":500,\n",
        "        \"cycle_mult\":1.2,\n",
        "        \"seq_max_len\":128,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 20,\n",
        "        \"patience\":5,\n",
        "        \"n_splits\" : 5,\n",
        "        \"lr\": 1e-05,\n",
        "        \"num_workers\":2,\n",
        "        \"criterion\":'cross', # 'smoothing','focal','cross'\n",
        "        \"smoothing\": 0.0,\n",
        "        \"dp\": 0.0,\n",
        "        \"model\": \"r_klue_roberta\",  # help='model type (kobert, koelectra, mbert, \n",
        "                                                        # roberta_base, roberta_large, \n",
        "                                                        # klue_roberta_small, klue_roberta_base, klue_roberta_large, klue_roberta_base_nli\n",
        "                                                        # r_roberta, r_klue_roberta)'\n",
        "\n",
        "        \"logging_wrong_samples\":True,\n",
        "        \"train_file\":'train_data.csv',\n",
        "        \"test_file\":'test_data.csv',\n",
        "        \"add_klue_data\":True,\n",
        "        'add_nikl_data':False,\n",
        "    })\n",
        "\n",
        "project_name = f\"{args.model}_Scdu{args.scheduler}_Dp{args.dp}_add_klue_data{args.add_klue_data}_{args.n_splits}Fd_Sm{args.smoothing}_Bs{args.batch_size}_Lr{args.lr}_Ep{args.epochs}_Cy{args.cycle_mult}\"\n",
        "args.update(\n",
        "            {\n",
        "                \"project_name\":project_name,\n",
        "                \"model_name\":project_name,\n",
        "             }\n",
        "            )\n",
        "\n",
        "seed_everything(args.seed)\n"
      ],
      "metadata": {
        "id": "GxMvtU_jmWrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args"
      ],
      "metadata": {
        "id": "7YKv1sJBmWm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb logging\n",
        "wandb.login()\n"
      ],
      "metadata": {
        "id": "c4HEZM1vmWkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(args, wandb, fold_lst=[4,5])"
      ],
      "metadata": {
        "id": "fCAGDz-8mWhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_test_dataset(args, tokenizer):\n",
        "    test_dataset = load_data(args, dataset_dir = f\"./data/{args.test_file}\")\n",
        "    test_label = test_dataset['labels'].values\n",
        "\n",
        "    # # tokenizing dataset\n",
        "    # entity_between = '</s></s>' if 'roberta' in args.model else '[SEP]'\n",
        "\n",
        "    tokenized_test = tokenized_dataset(args, test_dataset, tokenizer)\n",
        "    return tokenized_test, test_label\n",
        "\n",
        "def test_single_main(args, idx):\n",
        "    model = get_model(args)\n",
        "    tokenizer = get_tokenizer(args)\n",
        "    # load test datset\n",
        "    test_dataset, test_label = load_test_dataset(args, tokenizer)\n",
        "    test_dataset = NLI_Dataset(test_dataset, test_label)\n",
        "    testloader = DataLoader(test_dataset,\n",
        "                    shuffle=False,\n",
        "                    batch_size=args.batch_size,\n",
        "                    num_workers=args.num_workers,\n",
        "                    )\n",
        "\n",
        "    load_path = f'./models/{args.model_name}/{idx}-fold/best.pt'\n",
        "    model.load_state_dict(torch.load(load_path,map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    progress_bar = tqdm(enumerate(testloader), total=len(testloader), leave=True, position=0,)\n",
        "    for i, data in progress_bar:\n",
        "        with torch.no_grad():\n",
        "            logits = model(\n",
        "                data['input_ids'].to(device),\n",
        "                data['attention_mask'].to(device),\n",
        "                data['token_type_ids'].to(device)\n",
        "            )\n",
        "        if i==0:\n",
        "            one_fold_logits = logits\n",
        "        else:\n",
        "            one_fold_logits = torch.cat([one_fold_logits,logits],dim=0) # (batchsize,3) + (batchsize,3) -> (batchsize+batchsize,3)\n",
        "\n",
        "    # torch tensorë¥¼ ì €ìž¥í•˜ê¸° ìœ„í•œ numpy ë³€í™˜\n",
        "    one_fold_logits = one_fold_logits.squeeze(0).detach().cpu().numpy()\n",
        "    # numpy array ì €ìž¥\n",
        "    np.save(f'./models/{args.model_name}/{idx}-fold/numpy_logits', one_fold_logits)\n",
        "    # np_load = np.load(f'./models/{args.model_name}/{idx}-fold/numpy_logits.npy')\n",
        "        \n",
        "    return np.argmax(one_fold_logits, axis=1)"
      ],
      "metadata": {
        "id": "U1Cg1t7FmWen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_path = \"./data/sample_submission.csv\"\n",
        "submission = pd.read_csv(submission_path)"
      ],
      "metadata": {
        "id": "pekaTKermWbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 4\n",
        "single_pred = test_single_main(args, idx)\n",
        "\n",
        "label_to_num_dict = {'entailment':0,'contradiction':1,'neutral':2,}\n",
        "num_to_label_dict = {v:k for k,v in label_to_num_dict.items()}\n",
        "print(f\"len(single_pred) : {len(single_pred)}\")\n",
        "print('='*50)\n",
        "submission['label'] = single_pred\n",
        "# submission['label'] = hard_output\n",
        "submission['label'] = submission['label'].map(num_to_label_dict)\n",
        "submission['label'].value_counts()"
      ],
      "metadata": {
        "id": "z29tNGMgmqYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4ky5ub-bmqVj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}