# 예습과제_resampling

# 불균형 데이터

데이터 중에서 정상 범주의 관측치 수와 이상 범주의 관측치 수가 현저히 차이나는 데이터

일반적으로 이상 데이터를 이상으로 정확히 분류하는 것이 더 중요한데, 데이터가 불균형하면 이상 데이터를 정확히 찾아내지 못한다. 

![image](https://user-images.githubusercontent.com/61305409/176619515-05ce6008-3c56-4c4b-8d96-8f422591449c.png)

# 해결 기법

다수 범주의 데이터를 소수 범주의 데이터 수만큼 줄여 둘의 균형을 맞추는 Under Sampling과 소수 범주의 데이터를 다수 범주의 데이터 수만큼 늘여 균형을 맞추는 Over Sampling으로 인해 데이터 불균형을 해결할 수 있다. 이 두 방법을 합쳐 resampling이라 한다.
![image](https://user-images.githubusercontent.com/61305409/176619580-593f1859-4b6f-492d-aab0-58b924217e38.png)

## Under Sampling

다수 범주의 데이터를 소수 범주의 데이터 수에 맞게 줄이는 샘플링 방식(데이터 많은 애를 데이터 적은 애에 맞게 수를 줄인다)

계산 시간이 감소하고 클래스 오버랩을 줄이지만 데이터 제거로 인해 정보 손실이 발생할 수 있다.

많은 데이터를 줄이는 방법에 따라 다음과 같은 방법들이 존재한다.

### Random Sampling

다수 범주의 데이터를 A그룹, 소수 범주의 데이터를 B그룹이라고 하면,

B의 데이터 개수와 거의 동일한 만큼의 데이터를 A데이터들 중 무작위로 선택하여 남긴다.

무작위로 샘플링하기 때문에 샘플링 할 때마다 다른 결과를 얻는다.

resampling을 파이썬에서 `imblearn` 라이브러리를 통해 지원해준다.

```python
from imblearn.under_sampling import RandomUnderSampler

ran = RandomUnderSampler(return_indices=True) ##intialize to return indices of dropped rows
X_rs,y_rs,dropped = ran.fit_sample(X,y)

print("The number of removed indices are ",len(dropped))
plot_2d_space(X_rs,y_rs,X,y,'Random under sampling')
```

![image](https://user-images.githubusercontent.com/61305409/176619669-5ad0363f-8065-485f-8ef2-eca0803ad0eb.png)

### Tomek Links

무작위로 선택하는 것보다 더 좋은 효과를 얻기 위해 고안해낸 방법으로,

다수 범주의 데이터(B의 데이터) 중 Tomek Link를 만드는 데이터를 제거하는 방법이다.

A의 데이터의 특정 데이터와 B의 데이터 중 가장 가까운 거리의 데이터는 Tomek Link를 이룬다.

![image](https://user-images.githubusercontent.com/61305409/176619693-08aaaf87-bad8-4900-9f3e-b52598d39daf.png)

위의 그림에서 왼쪽의 그래프에서 x_j와 x_i는 tomek link를 이룬다고 할 수 없고, 오른쪽의 그래프에서는 tomek link를 이룬다고 할 수 있다.

![image](https://user-images.githubusercontent.com/61305409/176619730-4215cac3-101d-4552-8f6f-577250e26022.png)

![image](https://user-images.githubusercontent.com/61305409/176619752-bf4a9214-7b1d-4c51-8952-8a98020f96a1.png)

위의 그래프의 첫번째 그림에서 점선 원이 tomek link를 형성한 것을 나타내고, 이후에 다수 범주에 속한 데이터 중 tomek link에 연결된 데이터를 제거하여 언더 샘플링한 결과는 두번째 그림과 같다.

```python
from imblearn.under_sampling import TomekLinks

tl = TomekLinks(return_indices=True, ratio='majority')
X_tl, y_tl, id_tl = tl.fit_sample(X, y)

#print('Removed indexes:', id_tl)

plot_2d_space(X_tl, y_tl,X,y, 'Tomek links under-sampling')
```
![image](https://user-images.githubusercontent.com/61305409/176619807-5cbd635d-c01f-4fd8-acdd-6ffe542a2cc1.png)

### CNN(Condensed Nearest Neighbor)

다수 범주의 데이터 중 소수 범주와 가까운 데이터만 남기고 나머지의 다수 범주의 데이터들을 제거한다. 이 방법의 언더 샘플링을 적용하면 밑의 그림처럼 된다.

![image](https://user-images.githubusercontent.com/61305409/176619853-59740aba-aa1c-41a8-925b-d32e8532e7e3.png)

### OSS(One Sided Selection)

Tomek Link 방법과 CNN방법을 합쳐 언더 샘플링하는 기법이다.

OSS를 적용한 결과는 다음과 같다.

![image](https://user-images.githubusercontent.com/61305409/176619891-066857f7-83ad-4b8e-a325-12561afaf6d3.png)

## Over Sampling

언더 샘플링이랑 반대로 소수 범주의 데이터를 다수 범주의 데이터 수에 맞게 늘리는 방법이다.

특징 또한 언더 샘플링과 반대다.

데이터를 증가시키기 때문에 정보 손실이 없고, 따라서 언더 샘플링보다 높은 분류 정확도를 보이지만, 데이터가 많아 계산 시간이 증가할 수 있으며 과적합 가능성이 존재하고 노이즈 혹은 이상치에 민감하다.

이때 데이터를 증가시키는 다양한 방법이 존재하는데, 다음과 같다.

### Random Over Sampling

소수 범주의 데이터를 단순 복제하여 다수 범주의 데이터 수와 비슷해지도록 증가시키는 방법이다. 

똑같은 데이터를 증가시키다 보니 과적합이 발생한다는 단점이 있다.

![image](https://user-images.githubusercontent.com/61305409/176619934-ce19da79-663f-4241-b242-509204881ef4.png)

위의 그림에서 (n)은 해당 데이터가 n개 존재한다는 의미이다.

```  
python
from imblearn.over_sampling import RandomOverSampler

ran=RandomOverSampler()
X_ran,y_ran= ran.fit_resample(X,y)

print('The new data contains {} rows '.format(X_ran.shape[0]))

plot_2d_space(X_ran,y_ran,X,y,'over-sampled')
```

![image](https://user-images.githubusercontent.com/61305409/176620029-fbf878a6-4e09-40d4-b573-62036b6181e2.png)

### SMOTE(Sythetic Minority Over-Sampling Technique)

무작위의 방법의 단점을 보완하기 위해 고안되었다. 소수 범주의 동일한 데이터를 복제하여 수를 증가시키지 않고 가상의 데이터를 생성하여 수를 증가시킨다. 가상의 데이터는 소수 범주의 유사한 두 데이터를 선택해 합성하여 생성한다. 

![image](https://user-images.githubusercontent.com/61305409/176620063-d82bbfed-cab8-410f-9816-37665c1e7e02.png)

```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(ratio='minority')
X_sm, y_sm = smote.fit_sample(X, y)

plot_2d_space(X_sm, y_sm,X,y, 'SMOTE over-sampling')
```

![image](https://user-images.githubusercontent.com/61305409/176620082-aad853e1-cef4-475c-8a71-e5c4c5395b52.png)

### Borderline SMOTE

Borderline(다수 범주의 데이터와 소수 범주의 데이터가 인접한 경계선)부분에서만 SMOTE방식을 적용하는 방법이다. 경계선 부분에 위치한 소수 범주의 데이터 중 주변에 다수 범주의 데이터가 많은 데이터에 SMOTE를 적용하여 소수 범주의 데이터를 늘인다. 다시 말하면, 다른 클래스가 인접한 부분의 소수 범주의 데이터 수를 늘인다. 이때 주변에 다수 범주의 데이터가 K개 있다면 많은 것이고, K/2보다 작거나 같다면 적다고 판단한다. K는 따로 지정해주는 값이다.

![image](https://user-images.githubusercontent.com/61305409/176620112-e9992b05-9ab8-4c2d-a569-9e6af3ca448f.png)

### ADASYN(Adaptive Synthetic Sampling)

borderline SMOTE와 비슷한 방법인데, borderline SMOTE에서는 K값을 사용하여 SMOTE를 진행한 반면, ADASYN은 소수 범주의 데이터 주변의 다수 범주의 데이터 개수에 따라 가중치를 부여하여 SMOTE를 적용한다. 이 가중치를 통해 주변에 다수 범주의 데이터가 많은 소수 범주의 데이터일 수록 많은 수가 증가된다. 

이 방법은 borderline에 위치하는 소수 범주의 데이터의 수만 증가시키는 것이 아니라, 다수 범주의 데이터와 섞여있는 모든 소수 범주의 데이터의 수가 증가된다. SMOTE를 적용한 세 방법의 오버 샘플링 결과는 다음과 같이 비교해볼 수 있다. 

![image](https://user-images.githubusercontent.com/61305409/176620139-c129269c-7838-4e40-b969-b3950fc96cb1.png)

### GAN(Generative Adversarial Nets)

이 뿐만 아니라 GAN을 적용하여 오버 샘플링 할 수도 있다.

GAN은 생성자와 구분자로 이루어져 있는데, 생성자는 가짜 데이터를 만들어내고, 구분자는 특정 데이터가 가짜인지 진짜인지 구분해낸다. 생성자는 구분자가 가짜로 구분해내기 힘들도록 가짜를 진짜처럼 잘 만들어내도록, 구분자는 가짜와 진짜를 더 잘 분류해내도록 번갈아가면서 학습된다. 이를 통해 생성자는 진짜와 매우 비슷한 가짜 데이터를 만들어낼 수 있게 된다. 

참고1: [https://casa-de-feel.tistory.com/15](https://casa-de-feel.tistory.com/15)

참고2: [https://www.kaggle.com/code/shahules/tackling-class-imbalance](https://www.kaggle.com/code/shahules/tackling-class-imbalance)
