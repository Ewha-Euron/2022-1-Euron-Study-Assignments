{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week16_nlp_hw.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "📌 week16 과제는 **15주차의 Word Vector, Word Embedding, ULMfit, BERT**으로 구성되어 있습니다. 트랜스포머는 다음 주에 깊게 다루기 때문에 이번 주차에는 넣지 않았습니다.\n",
        "\n",
        "📌 위키독스의 딥러닝을 이용한 자연어 처리 입문 교재 실습, 관련 블로그 등의 문서 자료로 구성되어 있는 과제입니다. \n",
        "\n",
        "📌 안내된 링크에 맞추어 **직접 코드를 따라 치면서 (필사)** 해당 nlp task 의 기본적인 라이브러리와 메서드를 숙지해보시면 좋을 것 같습니다😊 필수라고 체크한 부분은 과제에 반드시 포함시켜주시고, 선택으로 체크한 부분은 자율적으로 스터디 하시면 됩니다.\n",
        "\n",
        "📌 궁금한 사항은 깃허브 이슈나, 카톡방, 세션 발표 시작 이전 시간 등을 활용하여 자유롭게 공유해주세요!"
      ],
      "metadata": {
        "id": "BX3ac8Ag1RPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🥰 **이하 예제를 실습하시면 됩니다.**\n",
        "\n",
        "1부터 3까지는 필수 과제입니다."
      ],
      "metadata": {
        "id": "Kq8aMYKGPQR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔹 1-(1) Pre-Trained Word Vector\n",
        "\n",
        "* 아래 아티클의 **Case Study: Learning Embeddings from Scratch vs. Pretrained Word Embeddings** 부분을 실습하세요\n",
        "\n",
        "\n",
        "📌 [An Essential Guide to Pretrained Word Embeddings for NLP Practitioners](https://www.analyticsvidhya.com/blog/2020/03/pretrained-word-embeddings-nlp/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eB5XfXsWHBHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgCWVn18YIet",
        "outputId": "08e7e761-3cab-402c-d9f1-479fd41934c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD DATA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# reading csv files\n",
        "train = pd.read_csv('./sample_data/Train.csv')\n",
        "valid = pd.read_csv('./sample_data/Valid.csv')\n",
        "\n",
        "# train_test split\n",
        "x_tr, y_tr = train['text'].values, train['label'].values\n",
        "x_val, y_val = valid['text'].values, valid['label'].values"
      ],
      "metadata": {
        "id": "LY8V3ZQ7Zco3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPARING THE DATA\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "#preparing vocabulary\n",
        "tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#converting text into integer sequences\n",
        "x_tr_seq  = tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq = tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding to prepare sequences of same length\n",
        "x_tr_seq  = pad_sequences(x_tr_seq, maxlen=100)\n",
        "x_val_seq = pad_sequences(x_val_seq, maxlen=100)"
      ],
      "metadata": {
        "id": "dpCBqBpuHACO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "같은 architecture의 다른 두 개의 NLP 모델을 생성한다. 첫번째 모델은 scratch로 embedding을 학습한 것이고, 두번째 모델은 사전학습된 word embedding을 사용한 모델이다."
      ],
      "metadata": {
        "id": "785YJa5XalSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINEING THE ARCHITECTUE - LEARNING FROM SCRATCH\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.callbacks import *\n",
        "\n",
        "model=Sequential()\n",
        "\n",
        "\n",
        "#embedding layer\n",
        "size_of_vocabulary=len(tokenizer.word_index) + 1 #+1 for padding\n",
        "model.add(Embedding(size_of_vocabulary, 300, input_length=100, trainable=True)) \n",
        "\n",
        "#lstm layer\n",
        "model.add(LSTM(128,return_sequences=True,dropout=0.2))\n",
        "\n",
        "#Global Maxpooling\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "#Dense Layer\n",
        "model.add(Dense(64,activation='relu')) \n",
        "model.add(Dense(1,activation='sigmoid')) \n",
        "\n",
        "#Add loss function, metrics, optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=[\"acc\"]) \n",
        "\n",
        "#Adding callbacks\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
        "mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
        "\n",
        "#Print summary of model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqbLsiR2XWFr",
        "outputId": "55094b4d-4594-4947-e634-397d3a4f6759"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 300)          33661200  \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100, 128)          219648    \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 128)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33,889,169\n",
            "Trainable params: 33,889,169\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING THE MODEL\n",
        "history = model.fit(np.array(x_tr_seq),np.array(y_tr),batch_size=128,epochs=5,validation_data=(np.array(x_val_seq),np.array(y_val)),verbose=1,callbacks=[es,mc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOHdWgprbbSp",
        "outputId": "a9a4ea25-96c2-4280-d530-b68fc6eee443"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4133 - acc: 0.8038\n",
            "Epoch 1: val_acc improved from -inf to 0.86380, saving model to best_model.h5\n",
            "313/313 [==============================] - 309s 981ms/step - loss: 0.4133 - acc: 0.8038 - val_loss: 0.3117 - val_acc: 0.8638\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.2097 - acc: 0.9181\n",
            "Epoch 2: val_acc improved from 0.86380 to 0.86900, saving model to best_model.h5\n",
            "313/313 [==============================] - 286s 913ms/step - loss: 0.2097 - acc: 0.9181 - val_loss: 0.3269 - val_acc: 0.8690\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.1045 - acc: 0.9624\n",
            "Epoch 3: val_acc did not improve from 0.86900\n",
            "313/313 [==============================] - 250s 798ms/step - loss: 0.1045 - acc: 0.9624 - val_loss: 0.4181 - val_acc: 0.8624\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.0533 - acc: 0.9829\n",
            "Epoch 4: val_acc did not improve from 0.86900\n",
            "313/313 [==============================] - 247s 790ms/step - loss: 0.0533 - acc: 0.9829 - val_loss: 0.4865 - val_acc: 0.8542\n",
            "Epoch 4: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATING THE PERFORMACE OF THE MODEL\n",
        "#loading best model\n",
        "from keras.models import load_model\n",
        "model = load_model('best_model.h5')\n",
        "\n",
        "#evaluation \n",
        "_,val_acc = model.evaluate(x_val_seq,y_val, batch_size=128)\n",
        "print(val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gtgz0xFibeId",
        "outputId": "86b1056a-5201-4310-ffba-36184340d995"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 8s 195ms/step - loss: 0.3269 - acc: 0.8690\n",
            "0.8690000176429749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BUILD SECOND MODEL USING PRETRAINED WORD EMBEDDINGS\n",
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('../input/glove6b/glove.6B.300d.txt')\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "oeKjf0YQbhi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE AN EMBEDDING MATRIX BY ASSIGNING THE VOCABULARY WITH THE PRETRAINED WORD EMBEDDINGS\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((size_of_vocabulary, 300))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "yVbaIDt3bu4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINING THE ARCHITECTURE - RETRAINED EMBEDDINGS\n",
        "model=Sequential()\n",
        "\n",
        "#embedding layer\n",
        "model.add(Embedding(size_of_vocabulary,300,weights=[embedding_matrix],input_length=100,trainable=False)) \n",
        "\n",
        "#lstm layer\n",
        "model.add(LSTM(128,return_sequences=True,dropout=0.2))\n",
        "\n",
        "#Global Maxpooling\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "#Dense Layer\n",
        "model.add(Dense(64,activation='relu')) \n",
        "model.add(Dense(1,activation='sigmoid')) \n",
        "\n",
        "#Add loss function, metrics, optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=[\"acc\"]) \n",
        "\n",
        "#Adding callbacks\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
        "mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
        "\n",
        "#Print summary of model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "FB_6yG7fb3_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING THE MODEL\n",
        "history = model.fit(np.array(x_tr_seq),np.array(y_tr),batch_size=128,epochs=10,validation_data=(np.array(x_val_seq),np.array(y_val)),verbose=1,callbacks=[es,mc])\n"
      ],
      "metadata": {
        "id": "WTv-ervIb2HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVAULATING THE PERFORMACE OF THE MODEL\n",
        "#loading best model\n",
        "from keras.models import load_model\n",
        "model = load_model('best_model.h5')\n",
        "\n",
        "#evaluation \n",
        "_,val_acc = model.evaluate(x_val_seq,y_val, batch_size=128)\n",
        "print(val_acc)"
      ],
      "metadata": {
        "id": "oJ4X31VVcARV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔹 1-(2) Word Embeddings\n",
        "\n",
        "* Keras를 이용해 Word Embeddings를 실습해 봅니다. 아래 페이지를 필사해 주시면 됩니다.\n",
        "\n",
        "📌 [Using pre-trained word embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/)"
      ],
      "metadata": {
        "id": "5rVWh1eYPs_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# DOWNLOAD THE DATA\n",
        "data_path = keras.utils.get_file(\n",
        "    \"news20.tar.gz\",\n",
        "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
        "    untar=True,\n",
        ")"
      ],
      "metadata": {
        "id": "R0OCLDiOcKQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOOK THE DATA\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
        "dirnames = os.listdir(data_dir)\n",
        "print(\"Number of directories:\", len(dirnames))\n",
        "print(\"Directory names:\", dirnames)\n",
        "\n",
        "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
        "print(\"Number of files in comp.graphics:\", len(fnames))\n",
        "print(\"Some example filenames:\", fnames[:5])"
      ],
      "metadata": {
        "id": "X_DlDhgwcQGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = []\n",
        "labels = []\n",
        "class_names = []\n",
        "class_index = 0\n",
        "for dirname in sorted(os.listdir(data_dir)):\n",
        "    class_names.append(dirname)\n",
        "    dirpath = data_dir / dirname\n",
        "    fnames = os.listdir(dirpath)\n",
        "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
        "    for fname in fnames:\n",
        "        fpath = dirpath / fname\n",
        "        f = open(fpath, encoding=\"latin-1\")\n",
        "        content = f.read()\n",
        "        lines = content.split(\"\\n\")\n",
        "        lines = lines[10:]\n",
        "        content = \"\\n\".join(lines)\n",
        "        samples.append(content)\n",
        "        labels.append(class_index)\n",
        "    class_index += 1\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Number of samples:\", len(samples))\n",
        "\n",
        "# Shuffle the data\n",
        "seed = 1337\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(samples)\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "# Extract a training & validation split\n",
        "validation_split = 0.2\n",
        "num_validation_samples = int(validation_split * len(samples))\n",
        "train_samples = samples[:-num_validation_samples]\n",
        "val_samples = samples[-num_validation_samples:]\n",
        "train_labels = labels[:-num_validation_samples]\n",
        "val_labels = labels[-num_validation_samples:]"
      ],
      "metadata": {
        "id": "oPVPd2m_caDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
        "vectorizer.adapt(text_ds)\n",
        "\n",
        "vectorizer.get_vocabulary()[:5]\n",
        "\n",
        "output = vectorizer([[\"the cat sat on the mat\"]])\n",
        "output.numpy()[0, :6]\n",
        "\n",
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))\n",
        "\n",
        "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "[word_index[w] for w in test]"
      ],
      "metadata": {
        "id": "w7YHJ7-wcdul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD PRETRAINED WORD EMBEDDINGS\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n",
        "\n",
        "path_to_glove_file = os.path.join(\n",
        "    os.path.expanduser(\"~\"), \".keras/datasets/glove.6B.100d.txt\"\n",
        ")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "metadata": {
        "id": "arJZ9dhNcdmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "metadata": {
        "id": "PBsxFw6AcsXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BUILD THE MODEL\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "model = keras.Model(int_sequences_input, preds)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "O2Eb6vqNcwLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN THE MODEL\n",
        "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
        "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)\n",
        "\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "oCmQtS5rc1Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPORT AN END-TO-END MODEL\n",
        "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorizer(string_input)\n",
        "preds = model(x)\n",
        "end_to_end_model = keras.Model(string_input, preds)\n",
        "\n",
        "probabilities = end_to_end_model.predict(\n",
        "    [[\"this message is about computer graphics and 3D modeling\"]]\n",
        ")\n",
        "\n",
        "class_names[np.argmax(probabilities[0])]"
      ],
      "metadata": {
        "id": "naenWUVPdnJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔹 2. ELMo Embedding\n",
        "\n",
        "* 아래는 ELMo Embedding의 작동을 \b볼 수 있는 case study 코드입니다. 필사하며 ELMo Embedding을 이해해 보세요!\n",
        "\n",
        "\n",
        "📌 [Elmo Embeddings : A use case study with code](https://medium.com/@errabia.oussama/elmo-embeddings-a-use-case-study-with-code-part-1-e2e5eb7df85c)"
      ],
      "metadata": {
        "id": "s-FBc3jI2kLH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0sLOZxu06Eo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "from allennlp.modules.elmo import Elmo,batch_to_ids\n",
        "import spacy\n",
        "import seaborn as sns\n",
        "import re\n",
        "import gc\n",
        "\n",
        "# LOAD THE DATA\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "train = pd.read_csv('train.csv', nrows=10000)\n",
        "\n",
        "input_col = 'comment_text'\n",
        "target_col = 'target'\n",
        "CLEAN = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "misspell_dict = {\"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n",
        "                 \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
        "                 \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                 \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n",
        "                 \"i'd\": \"I had\", \"i'll\": \"I will\", \"i'm\": \"I am\", \"isn't\": \"is not\",\n",
        "                 \"it's\": \"it is\", \"it'll\": \"it will\", \"i've\": \"I have\", \"let's\": \"let us\",\n",
        "                 \"mightn't\": \"might not\", \"mustn't\": \"must not\", \"shan't\": \"shall not\",\n",
        "                 \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\",\n",
        "                 \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\",\n",
        "                 \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n",
        "                 \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\",\n",
        "                 \"weren't\": \"were not\", \"we've\": \"we have\", \"what'll\": \"what will\",\n",
        "                 \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "                 \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\",\n",
        "                 \"who're\": \"who are\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                 \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\",\n",
        "                 \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\",\n",
        "                 \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\": \" will\", \"tryin'\": \"trying\"}\n",
        "def _get_misspell(misspell_dict):\n",
        "    misspell_re = re.compile('(%s)' % '|'.join(misspell_dict.keys()))\n",
        "    return misspell_dict, misspell_re\n",
        "\n",
        "\n",
        "def replace_typical_misspell(text):\n",
        "    misspellings, misspellings_re = _get_misspell(misspell_dict)\n",
        "\n",
        "    def replace(match):\n",
        "        return misspellings[match.group(0)]\n",
        "\n",
        "    return misspellings_re.sub(replace, text)\n",
        "\n",
        "\n",
        "# deal with obvuous misspeled word:\n",
        "\n",
        "train[input_col] = train[input_col].apply(lambda x : str(x).lower())\n",
        "train[input_col] = train[input_col].apply(replace_typical_misspell)\n",
        "\n",
        "\n",
        "def clean_text(sentence):\n",
        "    sentence = str(sentence).lower()\n",
        "    sentence = re.sub(\"[^\\w']\",' ',sentence)\n",
        "    sentence = re.sub(' +',' ',sentence)\n",
        "    return sentence\n",
        "\n",
        "spell_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)\n",
        "words = spell_model.index2word\n",
        "w_rank = {}\n",
        "for i,word in enumerate(words):\n",
        "    w_rank[word] = i\n",
        "WORDS = w_rank\n",
        "# Use fast text as vocabulary\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "def P(word):\n",
        "    \"Probability of `word`.\"\n",
        "    # use inverse of rank as proxy\n",
        "    # returns 0 if the word isn't in the dictionary\n",
        "    return - WORDS.get(word, 0)\n",
        "def correction(word):\n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "def candidates(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or [word])\n",
        "def known(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "def edits2(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "def singlify(word):\n",
        "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])\n",
        "\n",
        "\n",
        "\n",
        "if not CLEAN:\n",
        "    class_toxic    = train[train.target>=0.5].comment_text.values.tolist()\n",
        "    class_positive = train[train.target <0.5].comment_text.values.tolist()[:1000]\n",
        "else:\n",
        "    class_toxic = list(map(clean_text,train[train.target >= 0.5].comment_text.values.tolist()))\n",
        "    class_positive = list(map(clean_text,train[train.target < 0.5].comment_text.values.tolist()[:1000]))"
      ],
      "metadata": {
        "id": "ZCcKdKNfd_V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_dict= {}\n",
        "word_index=1\n",
        "lemma_dict ={}\n",
        "word_sequences_toxic= []\n",
        "word_sequences_positive= []\n",
        "\n",
        "toxic_embed   = []\n",
        "positive_embed= []\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg',disable = ['parser','ner','tagger'])\n",
        "docs = nlp.pipe(class_toxic,n_threads = 6)\n",
        "for doc in docs:\n",
        "    word_seq= []\n",
        "    for token in doc:\n",
        "        word_seq.append(token.text)\n",
        "        #if token.text not in word_dict:\n",
        "        #    word_dict[token.text] = word_index\n",
        "        #    word_index+=1\n",
        "        #    lemma_dict[token.text] = token.lemma_\n",
        "        #    word_seq.append(token.text)\n",
        "    word_sequences_toxic.append(word_seq)\n",
        "\n",
        "del spell_model,words,w_rank\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "docs = nlp.pipe(class_positive,n_threads = 6)\n",
        "for doc in docs:\n",
        "    word_seq= []\n",
        "    for token in doc:\n",
        "        word_seq.append(correction(token.text))\n",
        "        #if token.text not in word_dict:\n",
        "        #    word_dict[token.text] = word_index\n",
        "        #    word_index+=1\n",
        "        #    lemma_dict[token.text] = token.lemma_\n",
        "        #    word_seq.append(token.text)\n",
        "    word_sequences_positive.append(word_seq)\n",
        "\n",
        "\n",
        "del nlp,docs"
      ],
      "metadata": {
        "id": "-de0ow-8eA5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "options_file = \"elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
        "weight_file = \"elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
        "\n",
        "elmo = Elmo(options_file,weight_file,1,dropout=0)\n",
        "def elmo_embe_avg(seq_text, top_to_use):\n",
        "    avg_embeds = []\n",
        "    for i in range(10):\n",
        "        if (i*100)>len(seq_text):\n",
        "            break\n",
        "        embds = elmo(batch_to_ids(seq_text[i*100:(i+1)*100]))\n",
        "        for embd, mask in zip(embds['elmo_representations'][0], embds['mask']):\n",
        "            avg_embeds.append( (torch.sum(embd, dim=0) / torch.sum(mask)).detach().numpy())\n",
        "    return avg_embeds\n",
        "\n",
        "class_toxic_embeddings = pd.DataFrame(elmo_embe_avg(word_sequences_toxic,500))\n",
        "class_toxic_embeddings['target'] = 1\n",
        "class_positive_embeddings = pd.DataFrame(elmo_embe_avg(word_sequences_positive,500))\n",
        "class_positive_embeddings['target'] =0\n",
        "data_all = pd.concat((class_toxic_embeddings,class_positive_embeddings))\n",
        "data_all.reset_index(inplace=True,drop=True)"
      ],
      "metadata": {
        "id": "586U_RkFeCMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(random_state=1991,n_iter=1500,metric='cosine',n_components=2)\n",
        "tsne.fit(data_all.drop('target',axis=1))\n",
        "\n",
        "embd_tr = tsne.fit_transform(data_all.drop('target',axis=1) )\n",
        "data_all['ts_x_axis'] = embd_tr[:,0]\n",
        "data_all['ts_y_axis'] = embd_tr[:,1]"
      ],
      "metadata": {
        "id": "Og15N5EaeDfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot('ts_x_axis','ts_y_axis',hue='target',data=data_all)\n"
      ],
      "metadata": {
        "id": "8-IeDvJ8eEv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔹 3. BERT\n",
        "\n",
        "* 아래 두 예제로 BERT의 작동을 이해해 보세요!\n",
        "\n",
        "📌 [한국어 BERT의 마스크드 언어 모델(Masked Language Model) 실습](https://wikidocs.net/152922)\n",
        "\n",
        "📌  [한국어 BERT의 다음 문장 예측(Next Sentence Prediction)](https://wikidocs.net/156774)\n",
        "\n",
        "--- \n",
        "\n",
        "참고: [버트(Bidirectional Encoder Representations from Transformers, BERT)](https://wikidocs.net/115055)"
      ],
      "metadata": {
        "id": "hbsaYOqdWMFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "bMjBPhFgYZE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForMaskedLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model = TFBertForMaskedLM.from_pretrained('klue/bert-base', from_pt=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "inputs = tokenizer('축구는 정말 재미있는 [MASK]다.', return_tensors='tf')\n",
        "print(inputs['input_ids'])"
      ],
      "metadata": {
        "id": "4oIzgYaReK0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.Tensor([[   2 4713 2259 3944 6001 2259    4  809   18    3]], shape=(1, 10), dtype=int32)"
      ],
      "metadata": {
        "id": "X-3zJcereZi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs['token_type_ids'])"
      ],
      "metadata": {
        "id": "OaaXjIRPeUx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.Tensor([[0 0 0 0 0 0 0 0 0 0]], shape=(1, 10), dtype=int32)"
      ],
      "metadata": {
        "id": "pHp0shMlYZUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.Tensor([[1 1 1 1 1 1 1 1 1 1]], shape=(1, 10), dtype=int32)\n"
      ],
      "metadata": {
        "id": "3Lh-KtGfeRiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import FillMaskPipeline\n",
        "pip = FillMaskPipeline(model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "b26WQi0ee8up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip('축구는 정말 재미있는 [MASK]다.')\n",
        "[{'score': 0.8963505625724792,\n",
        "  'sequence': '축구는 정말 재미있는 스포츠 다.',\n",
        "  'token': 4559,\n",
        "  'token_str': '스포츠'},\n",
        " {'score': 0.02595764957368374,\n",
        "  'sequence': '축구는 정말 재미있는 거 다.',\n",
        "  'token': 568,\n",
        "  'token_str': '거'},\n",
        " {'score': 0.010033931583166122,\n",
        "  'sequence': '축구는 정말 재미있는 경기 다.',\n",
        "  'token': 3682,\n",
        "  'token_str': '경기'},\n",
        " {'score': 0.007924391888082027,\n",
        "  'sequence': '축구는 정말 재미있는 축구 다.',\n",
        "  'token': 4713,\n",
        "  'token_str': '축구'},\n",
        " {'score': 0.00784421805292368,\n",
        "  'sequence': '축구는 정말 재미있는 놀이 다.',\n",
        "  'token': 5845,\n",
        "  'token_str': '놀이'}]"
      ],
      "metadata": {
        "id": "YdBBJSiVfFqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip('어벤져스는 정말 재미있는 [MASK]다.')\n",
        "[{'score': 0.8382411599159241,\n",
        "  'sequence': '어벤져스는 정말 재미있는 영화 다.',\n",
        "  'token': 3771,\n",
        "  'token_str': '영화'},\n",
        " {'score': 0.028275618329644203,\n",
        "  'sequence': '어벤져스는 정말 재미있는 거 다.',\n",
        "  'token': 568,\n",
        "  'token_str': '거'},\n",
        " {'score': 0.017189407721161842,\n",
        "  'sequence': '어벤져스는 정말 재미있는 드라마 다.',\n",
        "  'token': 4665,\n",
        "  'token_str': '드라마'},\n",
        " {'score': 0.014989694580435753,\n",
        "  'sequence': '어벤져스는 정말 재미있는 이야기 다.',\n",
        "  'token': 3758,\n",
        "  'token_str': '이야기'},\n",
        " {'score': 0.009382619522511959,\n",
        "  'sequence': '어벤져스는 정말 재미있는 장소 다.',\n",
        "  'token': 4938,\n",
        "  'token_str': '장소'}]"
      ],
      "metadata": {
        "id": "8XXLIiMvfJWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertForNextSentencePrediction\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model = TFBertForNextSentencePrediction.from_pretrained('klue/bert-base', from_pt=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "# 이어지는 두 개의 문장\n",
        "prompt = \"2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다.\"\n",
        "next_sentence = \"여행을 가보니 한국의 2002년 월드컵 축구대회의 준비는 완벽했습니다.\"\n",
        "encoding = tokenizer(prompt, next_sentence, return_tensors='tf')\n",
        "\n",
        "logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]\n",
        "\n",
        "softmax = tf.keras.layers.Softmax()\n",
        "probs = softmax(logits)\n",
        "print('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())"
      ],
      "metadata": {
        "id": "xXVws7GUfUwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 상관없는 두 개의 문장\n",
        "prompt = \"2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다.\"\n",
        "next_sentence = \"극장가서 로맨스 영화를 보고싶어요\"\n",
        "encoding = tokenizer(prompt, next_sentence, return_tensors='tf')\n",
        "\n",
        "logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]\n",
        "\n",
        "softmax = tf.keras.layers.Softmax()\n",
        "probs = softmax(logits)\n",
        "print('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())"
      ],
      "metadata": {
        "id": "mIM4lp5nfZeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔹 선택과제 \n",
        "\n",
        "\n",
        "📌 [구글 BERT의 마스크드 언어 모델(Masked Language Model) 실습](https://wikidocs.net/153992)\n",
        "\n",
        "📌 [구글 BERT의 다음 문장 예측(Next Sentence Prediction)](https://wikidocs.net/156767)"
      ],
      "metadata": {
        "id": "tkX9sIsE3z3N"
      }
    }
  ]
}