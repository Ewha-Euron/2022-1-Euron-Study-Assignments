{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week5_hw_세영.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-vPZn15zBHIv",
        "xUWWDwdiPLS9",
        "008-V5QsQG25"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "📌 week5 내용 주차에 해당되는 과제는 3주차의 Glove 모델 실습, 4주차의 NER task 실습, 5주차의 Dependency Parsing task 실습으로 구성되어 있습니다. (**참고** : 제출은 week6 branch 복습과제로!) \n",
        "\n",
        "📌 위키독스의 딥러닝을 이용한 자연어 처리 입문 교재 실습, 캐글 노트북 등의 자료로 구성되어있는 과제입니다. \n",
        "\n",
        "📌 안내된 링크에 맞추어 **직접 코드를 따라 치면서 (필사)** 해당 nlp task 의 기본적인 라이브러리와 메서드를 숙지해보시면 좋을 것 같습니다😊 필수라고 체크한 부분은 과제에 반드시 포함시켜주시고, 선택으로 체크한 부분은 자율적으로 스터디 하시면 됩니다.\n",
        "\n",
        "📌 궁금한 사항은 깃허브 이슈나, 카톡방, 세션 발표 시작 이전 시간 등을 활용하여 자유롭게 공유해주세요!"
      ],
      "metadata": {
        "id": "QhUHfXkPAORh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk colab 환경에서 실행시 필요한 코드입니다. \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "3XjTSbcxBB6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1️⃣ **Glove**\n",
        "\n"
      ],
      "metadata": {
        "id": "-vPZn15zBHIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👀 **내용 복습** \n",
        "* 스탠포드 대학에서 개발한 카운트 기반과 예측 기반을 모두 사용하는 단어 임베딩 방법론 \n",
        "* word2vec 의 단점을 보완해서 나온 모델 \n",
        "* glove model 의 **input 은 반드시 동시등장행렬 형태**여야 한다 ⭐\n",
        "\n",
        "![1](https://www.dropbox.com/s/nz0ji4yzre56ifv/word_presentation.png?raw=1) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "🤔 한국어 예제는 없는 것 같습니다. 논문에서는 k-Glove 로 소개되는 연구가 있긴 한데, 좀 더 알아봐야 할 것 같아요!\n",
        "\n",
        "➕ [논문1](https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=NPAP13255003&dbt=NPAP)\n",
        "\n",
        "\n",
        "➕[논문2](https://scienceon.kisti.re.kr/commons/util/originalView.do?cn=CFKO201832073078664&oCn=NPAP13255064&dbt=CFKO&journal=NPRO00383361&keyword=%ED%95%9C%EA%B5%AD%EC%96%B4%20%EB%8C%80%ED%99%94%20%EC%97%94%EC%A7%84%EC%97%90%EC%84%9C%EC%9D%98%20%EB%AC%B8%EC%9E%A5%EB%B6%84%EB%A5%98)"
      ],
      "metadata": {
        "id": "P11biHcUuBaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 **1-(1)** glove python\n",
        "\n",
        "* [실습 : basic code](https://wikidocs.net/22885) 👉 필수"
      ],
      "metadata": {
        "id": "asGcGy6fBM1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install glove_python_binary"
      ],
      "metadata": {
        "id": "hBJb4Vf2BFnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec1eec6-457d-4532-83ac-b5ec6e922394"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting glove_python_binary\n",
            "  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.4.1)\n",
            "Installing collected packages: glove-python-binary\n",
            "Successfully installed glove-python-binary-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from lxml import etree\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-_bCjghmgqW",
        "outputId": "b933de91-d113-4d71-effb-424d73fe9db7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 다운로드\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dUMK-gnp2VI",
        "outputId": "4fb8a519-6363-46a0-d0e3-ff2979d126e8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x7ff61a572750>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "해당 데이터는 xml 문법으로 되어 있어 자연어를 얻기 위해서는 전처리가 필요하다. 실질적으로 필요한 부분은 영어 문장으로 구성되어 있는 와 사이의 내용이다. 전처리 작업을 통해 xml 문법에 해당하는 단어를 삭제해야하고, (Laughter)나 (Applause)와 같은 배경음을 나타내는 단어도 등장하는 데 이런 단어들 또한 제거되어야 한다."
      ],
      "metadata": {
        "id": "-MSeLeSymwiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 전처리\n",
        "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "target_text = etree.parse(targetXML)\n",
        "\n",
        "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
        "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
        "sent_text = sent_tokenize(content_text)\n",
        "\n",
        "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]"
      ],
      "metadata": {
        "id": "uIRB1DB3qBIB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "corpus = Corpus()\n",
        "\n",
        "#훈련 데이터로부터 GloVe에서 사용할 동싱 등장 행렬 생성\n",
        "corpus.fit(result, window = 5)\n",
        "glove = Glove(no_components= 100, learning_rate = 0.05)\n",
        "\n",
        "# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHMuzUznpRkh",
        "outputId": "eddb97b9-3958-411b-cccd-00657a53d7f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing 20 training epochs with 4 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "glove.most_similar()는 입력 단어의 가장 유사한 단어들의 리스트를 리턴"
      ],
      "metadata": {
        "id": "WuyCw6j_nZVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(glove.most_similar(\"water\"))\n",
        "print(glove.most_similar(\"physics\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKNT8GJeqq1H",
        "outputId": "d6da7a0f-8553-4fd4-fe63-4bde92bb72ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('clean', 0.8422758803162379), ('air', 0.8292722504344053), ('fresh', 0.8257940480149598), ('food', 0.8147950610055273)]\n",
            "[('chemistry', 0.8939304397042503), ('economics', 0.87753551378986), ('mathematics', 0.8662224685448278), ('beauty', 0.8653894715358409)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 **1-(2)** pre-trained glove \n",
        "\n",
        "* **사전학습모델** : 임의의 값으로 초기화하던 모델의 가중치들을 다른 문제에 학습시킨 가중치들로 초기화하는 방법이다.사전 학습한 가중치를 활용해 학습하고자 하는 본래 문제를 하위문제라고 한다. \n",
        "\n",
        "* [실습 : 문장의 긍부정을 판단하는 감성 분류 모델 만들기](https://wikidocs.net/33793) 👉 필수\n",
        "  * [설명참고](https://omicro03.medium.com/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-16%EC%9D%BC%EC%B0%A8-pre-trained-word-embedding-bb30db424a35)\n",
        "* pre-trained data 를 가져오는데 시간이 오래걸림\n",
        "* kaggle 대회에서 주로 이 방식을 많이 사용함\n",
        "  * [참고](https://lsjsj92.tistory.com/455)"
      ],
      "metadata": {
        "id": "3ADfVM9lO9NE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-trained GLove "
      ],
      "metadata": {
        "id": "jjEdsoKjr_Up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve, urlopen\n",
        "import gzip\n",
        "import zipfile\n",
        "\n",
        "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
        "zf = zipfile.ZipFile('glove.6B.zip')\n",
        "zf.extractall() \n",
        "zf.close()"
      ],
      "metadata": {
        "id": "i4tfK0zEgoxe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 임베딩 층 사용하기 \n",
        "- 긍정적인 레이블 1\n",
        "- 부정적인 레이블 0 "
      ],
      "metadata": {
        "id": "EddEtVDrtknF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 데이터에 대한 전처리 단계"
      ],
      "metadata": {
        "id": "7zXgAscYt6nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
        "y_train = [1, 0, 0, 1, 1, 0, 1]"
      ],
      "metadata": {
        "id": "Dv_ut0CUtZIl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#케라스의 토크나이저를 사용하여 단어 집합을 만들고 그 크기를 확인\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1 # 패딩을 고려하여 +1"
      ],
      "metadata": {
        "id": "MUxj0C3VtgoB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#각 문장에 대해서 정수 인코딩을 수행\n",
        "X_encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print('정수 인코딩 결과 :',X_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qot_zLgYpJML",
        "outputId": "fc26b611-8113-4b40-8851-b4980b494e58"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 결과 : [[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#가장 길이가 긴 문장의 길이를 구하기\n",
        "max_len = max(len(l) for l in X_encoded)\n",
        "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
        "y_train = np.array(y_train)"
      ],
      "metadata": {
        "id": "0tiC7vMMpK9Y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#최대 길이로 모든 샘플에 대해서 패딩을 진행\n",
        "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
        "y_train = np.array(y_train)\n",
        "print('패딩 결과 :')\n",
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mALO9koUpdKq",
        "outputId": "9d28d20c-10ec-456e-9d05-76fb44e046f7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "패딩 결과 :\n",
            "[[ 1  2  3  4]\n",
            " [ 5  6  0  0]\n",
            " [ 7  8  0  0]\n",
            " [ 9 10  0  0]\n",
            " [11 12  0  0]\n",
            " [13  0  0  0]\n",
            " [14 15  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이진 분류 모델 설계 "
      ],
      "metadata": {
        "id": "nPnKiGnKuE8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력층에 1개의 뉴런을 배치하고 활성화 함수로는 시그모이드 함수를, 그리고 손실 함수로 binary_crossentropy를 사용합니다. 그 후 100 에포크 학습합니다."
      ],
      "metadata": {
        "id": "3Y1-VgeqphKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "embedding_dim = 4\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "metadata": {
        "id": "-Gh4yQTPuJ_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762e362b-76b1-4d45-8021-2d10697bb3a5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 1s - loss: 0.6985 - acc: 0.4286 - 815ms/epoch - 815ms/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.6972 - acc: 0.4286 - 9ms/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.6958 - acc: 0.4286 - 5ms/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.6945 - acc: 0.4286 - 4ms/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.6932 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.6918 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.6905 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.6892 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.6878 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.6865 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.6852 - acc: 0.8571 - 5ms/epoch - 5ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.6839 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.6826 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.6812 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.6799 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.6786 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.6772 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.6759 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.6745 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.6732 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.6718 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.6705 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.6691 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.6677 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.6663 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.6649 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.6635 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.6621 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.6607 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.6593 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.6578 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.6564 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.6549 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.6534 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.6520 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.6505 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.6490 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.6475 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.6459 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.6444 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.6429 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.6413 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.6398 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.6382 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.6366 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.6350 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.6334 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.6318 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.6302 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.6285 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.6269 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.6252 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.6236 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.6219 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.6202 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.6185 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.6168 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.6151 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.6134 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.6116 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.6099 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.6081 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.6063 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.6046 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.6028 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.6010 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.5992 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.5974 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.5955 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.5937 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.5919 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.5900 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.5881 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.5863 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.5844 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.5825 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.5806 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.5787 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.5768 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.5749 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.5729 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.5710 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.5690 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.5671 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.5651 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.5631 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.5612 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.5592 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.5572 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.5552 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.5532 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.5512 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.5491 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.5471 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.5451 - acc: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.5430 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.5410 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.5389 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.5369 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.5348 - acc: 1.0000 - 7ms/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff587d34810>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 과정에서 현재 각 단어들의 임베딩 벡터들의 값은 출력층의 가중치와 함께 학습됩니다."
      ],
      "metadata": {
        "id": "r4c2CmukpiyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 사전훈련된 GloVe 사용하기 "
      ],
      "metadata": {
        "id": "ZFp6WXHzuctW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "glove.6B.zip를 다운로드하고 압축을 풀면 다수의 파일이 존재하는데 여기서는 glove.6B.100d.txt 파일을 사용합니다."
      ],
      "metadata": {
        "id": "VhULqZs1ptCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve, urlopen\n",
        "import gzip\n",
        "import zipfile\n",
        "\n",
        "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
        "zf = zipfile.ZipFile('glove.6B.zip')\n",
        "zf.extractall() \n",
        "zf.close()"
      ],
      "metadata": {
        "id": "CiZRlBzipto0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "glove.6B.100d.txt에 있는 모든 임베딩 벡터들을 불러옵니다. 파이썬의 자료구조 딕셔너리(dictionary)를 사용하며, 로드한 임베딩 벡터의 개수를 확인합니다."
      ],
      "metadata": {
        "id": "AnO9IR2cpwIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dict = dict()\n",
        "\n",
        "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "\n",
        "    # 100개의 값을 가지는 array로 변환\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "\n",
        "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))\n"
      ],
      "metadata": {
        "id": "b1ktg4xggolx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e37c8cc4-de98-48b0-c54b-ad5aca316d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.013786   0.38216    0.53236    0.15261   -0.29694   -0.20558\n",
            " -0.41846   -0.58437   -0.77355   -0.87866   -0.37858   -0.18516\n",
            " -0.128     -0.20584   -0.22925   -0.42599    0.3725     0.26077\n",
            " -1.0702     0.62916   -0.091469   0.70348   -0.4973    -0.77691\n",
            "  0.66045    0.09465   -0.44893    0.018917   0.33146   -0.35022\n",
            " -0.35789    0.030313   0.22253   -0.23236   -0.19719   -0.0053125\n",
            " -0.25848    0.58081   -0.10705   -0.17845   -0.16206    0.087086\n",
            "  0.63029   -0.76649    0.51619    0.14073    1.019     -0.43136\n",
            "  0.46138   -0.43585   -0.47568    0.19226    0.36065    0.78987\n",
            "  0.088945  -2.7814    -0.15366    0.01015    1.1798     0.15168\n",
            " -0.050112   1.2626    -0.77527    0.36031    0.95761   -0.11385\n",
            "  0.28035   -0.02591    0.31246   -0.15424    0.3778    -0.13599\n",
            "  0.2946    -0.31579    0.42943    0.086969   0.019169  -0.27242\n",
            " -0.31696    0.37327    0.61997    0.13889    0.17188    0.30363\n",
            " -1.2776     0.044423  -0.52736   -0.88536   -0.19428   -0.61947\n",
            " -0.10146   -0.26301   -0.061707   0.36627   -0.95223   -0.39346\n",
            " -0.69183   -1.0426     0.28855    0.63056  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "총 40만개의 임베딩 벡터가 존재합니다. 임의의 단어 'respectable'의 임베딩 벡터값과 크기를 출력해봅니다."
      ],
      "metadata": {
        "id": "hoqILdupp736"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_dict['respectable'])\n",
        "print('벡터의 차원 수 :',len(embedding_dict['respectable']))"
      ],
      "metadata": {
        "id": "hUBqwYigp8fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "벡터값이 출력되며 벡터의 차원 수는 100입니다. 풀고자 하는 문제의 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성합니다. 이 행렬의 값은 전부 0으로 채웁니다. 이 행렬에 사전 훈련된 임베딩 값을 넣어줄 것입니다."
      ],
      "metadata": {
        "id": "JZgOXoKKqIxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "print('임베딩 행렬의 크기(shape) :',np.shape(embedding_matrix)"
      ],
      "metadata": {
        "id": "ppUajMutvXJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 집합의 모든 단어에 대해서 사전 훈련된 GloVe의 임베딩 벡터들을 맵핑한 후 'great'의 벡터값이 의도한 인덱스의 위치에 삽입되었는지 확인해보겠습니다."
      ],
      "metadata": {
        "id": "gal4s-_CqQI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in tokenizer.word_index.items():\n",
        "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
        "    vector_value = embedding_dict.get(word)\n",
        "    if vector_value is not None:\n",
        "        embedding_matrix[index] = vector_value "
      ],
      "metadata": {
        "id": "ljZQ3kefqFcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "embedding_matrix의 인덱스 2에서의 값을 확인합니다."
      ],
      "metadata": {
        "id": "NOtOSZmXqTlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eidUoceAsn3c",
        "outputId": "9796312b-ed8e-4e6b-d661-038085c369ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.013786    0.38216001  0.53236002  0.15261    -0.29694    -0.20558\n",
            " -0.41846001 -0.58437002 -0.77354997 -0.87866002 -0.37858    -0.18516\n",
            " -0.12800001 -0.20584001 -0.22925    -0.42598999  0.3725      0.26076999\n",
            " -1.07019997  0.62915999 -0.091469    0.70348001 -0.4973     -0.77691001\n",
            "  0.66044998  0.09465    -0.44893     0.018917    0.33146    -0.35021999\n",
            " -0.35789001  0.030313    0.22253001 -0.23236001 -0.19719    -0.0053125\n",
            " -0.25848001  0.58081001 -0.10705    -0.17845    -0.16205999  0.087086\n",
            "  0.63028997 -0.76648998  0.51618999  0.14072999  1.01900005 -0.43136001\n",
            "  0.46138    -0.43584999 -0.47567999  0.19226     0.36065     0.78987002\n",
            "  0.088945   -2.78139997 -0.15366     0.01015     1.17980003  0.15167999\n",
            " -0.050112    1.26259995 -0.77526999  0.36030999  0.95761001 -0.11385\n",
            "  0.28035    -0.02591     0.31246001 -0.15424     0.37779999 -0.13598999\n",
            "  0.29460001 -0.31579     0.42943001  0.086969    0.019169   -0.27241999\n",
            " -0.31696001  0.37327     0.61997002  0.13889     0.17188001  0.30362999\n",
            " -1.27760005  0.044423   -0.52736002 -0.88536    -0.19428    -0.61947\n",
            " -0.10146    -0.26301    -0.061707    0.36627001 -0.95222998 -0.39346001\n",
            " -0.69182998 -1.04260004  0.28854999  0.63055998]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 **1-(3)** fine tuning glove\n",
        "* 미세조정 : 사전 학습한 모든 가중치와 더불어 하위 문제를 위한 최소한의 가중치를 추가해 모델을 추가로 학습하는 방법이다. \n",
        "\n",
        "* fine tuning 이 필요한 경우 \n",
        "  * pretrained model 에 데이터셋에 있는 단어가 포함되지 않은 경우 \n",
        "  * 데이터 집합이 너무 작아서 전체 모델을 훈련시키기 어려운 경우 \n",
        "\n",
        "* [Mittens 라이브러리로 fine tuning](https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39) 👉 필수\n",
        "  *  GloVe 임베딩을 fine-tuning 하기 위한 파이썬 라이브러리\n",
        "  * [github](https://github.com/roamanalytics/mittens)\n",
        "\n",
        "* [한국어 소설 텍스트 데이터 미세조정 모델 학습 - GPT2](https://m.blog.naver.com/PostView.nhn?isHttpsRedirect=true&blogId=horajjan&logNo=222104684132&categoryNo=120&proxyReferer=) 👉 선택 (glove 모델 예제는 아닙니다. fine-tuning 에 초점을 두어서 참고해주시면 좋을 것 같습니다.)"
      ],
      "metadata": {
        "id": "f_wcrE5PtLMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mittens\n",
        "!pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJa7xf-zwaLC",
        "outputId": "7aeff0e8-e419-4ecb-f332-99e467070330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mittens in /usr/local/lib/python3.7/dist-packages (0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mittens) (1.21.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX0ecWn8wmww",
        "outputId": "96fc8c9c-64d6-4d23-847c-5c08eb8ba420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk.corpus import brown\n",
        "from mittens import GloVe, Mittens\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "## Loading pretrained Model \n",
        "def glove2dict(glove_filename):\n",
        "    with open(glove_filename, encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
        "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
        "                for line in reader}\n",
        "    return embed\n",
        "\n",
        "glove_path = \"glove.6B.50d.txt\" # get it from https://nlp.stanford.edu/projects/glove\n",
        "pre_glove = glove2dict(glove_path)\n",
        "\n",
        "## Data pre-processing before building co-occurence matrix\n",
        "sw = list(_stop_words.ENGLISH_STOP_WORDS)\n",
        "brown_data = brown.words()[:200000]\n",
        "brown_nonstop = [token.lower() for token in brown_data if (token.lower() not in sw)]\n",
        "oov = [token for token in brown_nonstop if token not in pre_glove.keys()]\n",
        "\n",
        "def get_rareoov(xdict, val):\n",
        "    return [k for (k,v) in Counter(xdict).items() if v<=val]\n",
        "\n",
        "## optional - use if needed \n",
        "#oov_rare = get_rareoov(oov, 1)\n",
        "#corp_vocab = list(set(oov) - set(oov_rare))\n",
        "#brown_tokens = [token for token in brown_nonstop if token not in oov_rare]\n",
        "#brown_doc = [' '.join(brown_tokens)]\n",
        "\n",
        "corp_vocab = list(set(oov))\n",
        "brown_doc = [' '.join(brown_nonstop)]\n",
        "\n",
        "## Building Co-occurence Matrix \n",
        "cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
        "X = cv.fit_transform(brown_doc)\n",
        "Xc = (X.T * X)\n",
        "Xc.setdiag(0)\n",
        "coocc_ar = Xc.toarray()\n",
        "\n",
        "## Fine-tuning Mittens Model \n",
        "mittens_model = Mittens(n=50, max_iter=1000)\n",
        "\n",
        "new_embeddings = mittens_model.fit(\n",
        "    coocc_ar,\n",
        "    vocab=corp_vocab,\n",
        "    initial_embedding_dict= pre_glove)\n"
      ],
      "metadata": {
        "id": "tW_Z7VULX_i2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c0b8c01-dc7d-43b2-9d56-094c3c1331eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/adagrad.py:139: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 1000: loss: 0.0322125107049942"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newglove = dict(zip(corp_vocab, new_embeddings))\n",
        "f = open(\"repo_glove.pkl\",\"wb\")\n",
        "pickle.dump(newglove, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "x0ALIeATX_e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* (참고) word2vec pretrained example\n",
        "\n",
        "➕ [word2vec 사전학습 모델 -한국어1](http://doc.mindscale.kr/km/unstructured/11.html)\n",
        "\n",
        "➕ [word2vec 사전학습 - 한국어2](https://monetd.github.io/python/nlp/Word-Embedding-Word2Vec-%EC%8B%A4%EC%8A%B5/#%ED%95%9C%EA%B5%AD%EC%96%B4-word2vec-%EB%A7%8C%EB%93%A4%EA%B8%B0)"
      ],
      "metadata": {
        "id": "I_-OB9Siga3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2️⃣ NER**"
      ],
      "metadata": {
        "id": "xUWWDwdiPLS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👀 **내용 복습** \n",
        "* 개체명 인식을 사용하면 코퍼스로부터 어떤 단어가 사람, 장소, 조직 등을 의미하는 단어인지를 찾을 수 있다. "
      ],
      "metadata": {
        "id": "9N0B4VknPkTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "🔹 **2-(1)** NER task by nltk library\n",
        "\n",
        "\n",
        "* nltk 에서는 개체명 인식기 (NER chunker) 를 지원하고 있다. \n",
        "* ne_chunk 는 개체명을 태깅하기 위해서 앞서 품사 태깅 pos_tag 가 수행되어야 한다. \n",
        "\n",
        "\n",
        "📌 [basic code](https://wikidocs.net/30682) 👉 필수 \n",
        "\n",
        "📌 [BIO 표현, LSTM을 활용한 NER 실습](https://wikidocs.net/24682) 👉 선택\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QWgla1BuPRqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI9anXwthcTB",
        "outputId": "c7171024-4d4f-46a7-9bd8-3ef691e29766"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "sentence = \"James is working at Disney in London\"\n",
        "# 토큰화 후 품사 태깅\n",
        "tokenized_sentence = pos_tag(word_tokenize(sentence))\n",
        "print(tokenized_sentence)"
      ],
      "metadata": {
        "id": "diaZweMyAxJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df9e21f-8b36-4021-b90b-6e82da3c40ca"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 개체명 인식\n",
        "ner_sentence = ne_chunk(tokenized_sentence)\n",
        "print(ner_sentence)"
      ],
      "metadata": {
        "id": "1k09tKha3Lgi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f24dbd-a552-4e8b-a073-48ff69f925fe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON James/NNP)\n",
            "  is/VBZ\n",
            "  working/VBG\n",
            "  at/IN\n",
            "  (ORGANIZATION Disney/NNP)\n",
            "  in/IN\n",
            "  (GPE London/NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 **2-(2)** NER task by spacy library\n",
        "\n",
        "\n",
        "* spaCy 는 자연어처리를 위한 파이썬 기반의 오픈 소스 라이브러리로 다음과 같은 기능을 제공한다. \n",
        "  * Tokenization \n",
        "  * POS tagging \n",
        "  * Lemmatization \n",
        "  * Sentence Boundary Detection (SBD)\n",
        "  * Named Entity Recognition (NER)\n",
        "  * Similarity\n",
        "  * Text Classification\n",
        "  * Rule-based Matching\n",
        "  * Training\n",
        "  * Serialization\n",
        "\n",
        "* spaCy 와 NER\n",
        "  * .ents → .label_\n",
        "\n",
        "\n",
        "📌 [basic code](https://frhyme.github.io/python-lib/nlp_spacy_1/) 👉 필수 (NER 부분만)\n",
        "\n",
        "📌 [kaggle_Custom NER using SpaCy](https://www.kaggle.com/code/amarsharma768/custom-ner-using-spacy/notebook) 👉 선택\n",
        "\n",
        "  * 훈련되지 않은 데이터 세트에 명명된 엔티티를 학습하는 방법 : 이력서 pdf 데이터 활용 \n",
        "  * manually labelled \n",
        "\n",
        "📌 [한국어 NER](https://github.com/monologg/KoBERT-NER) 👉 참고하면 좋을 자료\n",
        "\n",
        "➕ [참고](http://aispiration.com/nlp2/nlp-ner-python.html)"
      ],
      "metadata": {
        "id": "TPX-WtSvPmm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy \n",
        "## 다음처럼 spacy에서 내가 원하는 언어의 모델을 가져오고, \n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "## 다음처럼 문장을 nlp에 넘기기만 하면 끝납니다. \n",
        "doc = nlp('Apple is looking at buyin at U.K startup for $1 billion.')\n",
        "print(type(doc)) ## 타입은, Doc고, \n",
        "print(doc)## 그냥 출력하면, 원래 문장이 그대로 나오고, \n",
        "print(list(doc))## 리스트로 변형하면, tokenize한 결과가 나오고 \n",
        "print(type(doc[0]))## 리스트의 가장 앞에 있는 값은 Token이라는 타입이죠. "
      ],
      "metadata": {
        "id": "WXjRfz-qP0Xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d30c7b5-5ce3-47b7-cd3a-9dd1fad94574"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'spacy.tokens.doc.Doc'>\n",
            "Apple is looking at buyin at U.K startup for $1 billion.\n",
            "[Apple, is, looking, at, buyin, at, U.K, startup, for, $, 1, billion, .]\n",
            "<class 'spacy.tokens.token.Token'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp('Apple is looking at buying U.K. startup for $1 billion')\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY-rvqSjiCo6",
        "outputId": "3c07340b-06c6-4408-d018-11d13bd59798"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "U.K. GPE\n",
            "$1 billion MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"\"\"But Google is starting from behind. The company made a late push\n",
        "into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa\n",
        "software, which runs on its Echo and Dot devices, have clear leads in\n",
        "consumer adoption.\"\"\".replace(\"\\n\", \" \").strip())\n",
        "\n",
        "## 아래처럼 무엇이 organization이고, 무엇이 product인지, 꽤 잘 구별해주지만, \n",
        "## echo, dot 등에 대해서는 정확하지 못하다. \n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ic40CBDiF5s",
        "outputId": "b6be0dcf-5d61-48ea-fec6-3e6064f2f177"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google ORG\n",
            "Apple ORG\n",
            "Siri PRODUCT\n",
            "Amazon ORG\n",
            "Alexa ORG\n",
            "Echo PRODUCT\n",
            "Dot PRODUCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3️⃣ Dependency Parsing**"
      ],
      "metadata": {
        "id": "008-V5QsQG25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👀 **내용 복습** \n",
        "* 문장의 전체적인 구성/구조 보다는 각 개별단어 간의 '의존관계' 또는 '수식관계' 와 같은 단어간 관계를 파악하는 것이 목적인 NLP Task\n",
        "* 문장 해석의 모호성을 없애기 위해 Parsing 을 한다."
      ],
      "metadata": {
        "id": "oQfcodHQQPlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "🔹 **3-(1)** Dependency Parsing by spacy library\n",
        "\n",
        "\n",
        "* [basic](https://frhyme.github.io/python-lib/nlp_spacy_1/#navigating-parse-tree) 👉 dependecy parsing 부분만 필수\n",
        "* .dep_ 메서드\n",
        "\n"
      ],
      "metadata": {
        "id": "mJLAzZnbRNlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞서 문장을 tokenizing하고 POS에 따라서 나누었습니다. 이후에는 각각의 token들간의 의존관계를 고려하여, 관련있는 단어들을 묶을 수 있겠죠."
      ],
      "metadata": {
        "id": "9XQdooOFih11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "그냥, doc.noun_chunks를 하면, 알아서 dependency graph를 고려하여, noun phrase를 뽑아줍니다. 출력 값은 generator이고, 이를 리스트로 변환해서 모두 불러오면 되고, 각각은 token 클래스가 아니라, span class입니다(token의 복합어 느낌이죠)\n",
        "doc = nlp(\"Autonomous cars shift insur"
      ],
      "metadata": {
        "id": "fCC4WPNyijkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "\n",
        "## 특정 텍스트를 nlp에 넘기면 모두 해결되기는 하는데, \n",
        "## noun_chunks의 경우는 token 클래스도 아니고, Doc 클래스도 아니다. \n",
        "## Span이라는 클래스는 그냥 Doc와 비슷하다고 생각하면 된다, 일종의 복합어 개념.\n",
        "noun_chunks = doc.noun_chunks\n",
        "print(type(noun_chunks))\n",
        "noun_chunk = list(noun_chunks)[0]\n",
        "print(type(noun_chunk))\n",
        "token = noun_chunk[0]\n",
        "print(type(token))\n",
        "\n",
        "print(\"==\"*30)\n",
        "print(\"\"\"\n",
        "Text: The original noun chunk text.\n",
        "Root text: The original text of the word connecting the noun chunk to the rest of the parse.\n",
        "Root dep: Dependency relation connecting the root to its head.\n",
        "Root head text: The text of the root token's head.\n",
        "\"\"\".strip())\n",
        "print(\"==\"*30)\n",
        "str_format = \"{:>25}\"*4\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(str_format.format(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text))\n"
      ],
      "metadata": {
        "id": "HbQEYt76bJXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b95233e-1545-4358-dd94-c88059700568"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'generator'>\n",
            "<class 'spacy.tokens.span.Span'>\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "============================================================\n",
            "Text: The original noun chunk text.\n",
            "Root text: The original text of the word connecting the noun chunk to the rest of the parse.\n",
            "Root dep: Dependency relation connecting the root to its head.\n",
            "Root head text: The text of the root token's head.\n",
            "============================================================\n",
            "          Autonomous cars                     cars                    nsubj                    shift\n",
            "      insurance liability                liability                     dobj                    shift\n",
            "            manufacturers            manufacturers                     pobj                   toward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## navigiting parse tree\n",
        "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "for tok in doc:\n",
        "    print(tok.text)\n",
        "    children = list(tok.children)\n",
        "    print('children:', children, 'head:', tok.head if tok.head != tok else \"!this is root node\")\n",
        "    print(\"==\"*16)"
      ],
      "metadata": {
        "id": "W9QAEsrLAxHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8794f5ce-a5dc-4239-dc00-525b1ec11d5b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autonomous\n",
            "children: [] head: cars\n",
            "================================\n",
            "cars\n",
            "children: [Autonomous] head: shift\n",
            "================================\n",
            "shift\n",
            "children: [cars, liability] head: !this is root node\n",
            "================================\n",
            "insurance\n",
            "children: [] head: liability\n",
            "================================\n",
            "liability\n",
            "children: [insurance, toward] head: shift\n",
            "================================\n",
            "toward\n",
            "children: [manufacturers] head: liability\n",
            "================================\n",
            "manufacturers\n",
            "children: [] head: toward\n",
            "================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#간단한 네트워크로 표현\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "nG = nx.Graph()\n",
        "doc[2] ## root node\n",
        "\n",
        "def add_n_to_g(inputG, tok):\n",
        "    inputG.add_node(tok)\n",
        "    children = list(tok.children)\n",
        "    if children != []:\n",
        "        inputG.add_nodes_from(children)\n",
        "        for c in children:\n",
        "            inputG.add_edges_from([(tok, c, {'dependency':c.dep_})])\n",
        "            add_n_to_g(inputG, c)\n",
        "add_n_to_g(nG, doc[2])\n",
        "print(nG.nodes(data=True))\n",
        "print(\"==\"*20)\n",
        "for e in nG.edges(data=True):\n",
        "    print(f\"{e[0]}, {e[1]}, ### dependency: {e[2]['dependency']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwjkrocPiuZb",
        "outputId": "e8fd8f9a-079e-405d-9c1d-e75d7569fc13"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(shift, {}), (cars, {}), (liability, {}), (Autonomous, {}), (insurance, {}), (toward, {}), (manufacturers, {})]\n",
            "========================================\n",
            "shift, cars, ### dependency: nsubj\n",
            "shift, liability, ### dependency: dobj\n",
            "cars, Autonomous, ### dependency: amod\n",
            "liability, insurance, ### dependency: compound\n",
            "liability, toward, ### dependency: prep\n",
            "toward, manufacturers, ### dependency: pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 **3-(2)** Spacy (kaggle) \n",
        "\n",
        "* 캐글 노트북 환경에서 실습해보는 것을 권장드립니다!\n",
        "\n",
        "* [kaggle_spaCy](https://www.kaggle.com/code/nirant/hitchhiker-s-guide-to-nlp-in-spacy) 👉 필수\n",
        "  * 도날드 트럼프 트위터 트윗 내용 데이터 분석\n",
        "\n",
        "\n",
        "👀 **노트북 키포인트** \n",
        "  1. spacy.display 메서드를 사용한 NER 시각화 \n",
        "  2. Tagging 을 통한 트럼프 트윗 분석 : noun_chunks 는 dependency graph를 고려하여, noun phrase를 뽑아준다. \n",
        "  3. [spacy Match](https://yujuwon.tistory.com/entry/spaCy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-Rule-based-Matching) : 직접 문장/단어 패턴을 등록하여 parsing\n",
        "  4. Question and answering task using Dependency Parsing\n",
        "    * spacy display :  ``style = 'dep'``\n",
        "    * .dep_\n"
      ],
      "metadata": {
        "id": "XQD5oiGgRfHe"
      }
    }
  ]
}