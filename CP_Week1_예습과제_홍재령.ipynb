{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b9paTrL5yl7"
      },
      "source": [
       ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfqZp9uEbfiZ"
      },
      "source": [
        "필요한 라이브러리 불러옴"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "t8VLWWlp5t1j",
        "outputId": "e2810e3e-810b-4087-f5d5-6316d62a913d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import json  # json 파일 다루는 라이브러리\n",
        "import time\n",
        "import random\n",
        "import logging\n",
        "import easydict  ###\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm  # 프로그램 진행사항 보여주는 라이브러리\n",
        "from glob import glob \n",
        "from pathlib import Path  ###\n",
        "from natsort import natsorted \n",
        "from os.path import join as opj  ###\n",
        "from ptflops import get_model_complexity_info  ###\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold  ###\n",
        "from PIL import Image  ##\n",
        "\n",
        "import timm  ###\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, grad_scaler\n",
        "from torchvision import transforms\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAnARhvFc0GY"
      },
      "source": [
        "모델 및 학습의 하이퍼파라미터를 정의한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xrx4oHF0cyY6"
      },
      "outputs": [],
      "source": [
        "args = easydict.EasyDict(\n",
        "    {\n",
        "      'exp_num':'0',\n",
        "      'experiment':'Base',\n",
        "      'tag':'Default',\n",
        "\n",
        "      # 경로 설정\n",
        "      'data_path':'../data',\n",
        "      'fold':4,\n",
        "      'Kfold':5,\n",
        "      'model_path':'results/',\n",
        "\n",
        "      #  model 파라미터 설정\n",
        "     'encoder_name':'regnety_040',\n",
        "     'drop_path_rate':0.2,\n",
        "\n",
        "     # training 파라미터 설정\n",
        "     ## 기본 파라미터\n",
        "     'img_size':288,\n",
        "     'batch_size':16,\n",
        "     'epochs':60,\n",
        "     'optimizer':'Lamb',\n",
        "     'initial_lr':5e-6,\n",
        "     'weight_decay':1e-3,\n",
        "\n",
        "     ## augumentation\n",
        "     'aug_ver':2,\n",
        "     'flipaug_ratio':0.3,\n",
        "     'margin':50,\n",
        "     'random_margin':True,\n",
        "\n",
        "     ## Scheduler\n",
        "     'scheduler':'cycle',\n",
        "     'warm_epoch':5,\n",
        "     ### Cosine Annealing\n",
        "     'min_lr':5e-6,\n",
        "     'tmax':145,\n",
        "     ### OnecycleLR\n",
        "     'max_lr':1e-3,\n",
        "\n",
        "     ## 그외 training 파라미터\n",
        "     'patience':50,\n",
        "     'clipping':None,\n",
        "\n",
        "     # 하드웨어 설정\n",
        "     'amp':True,\n",
        "     'multi_gpu':False,\n",
        "     'logging':False,\n",
        "     'num_workers':4,\n",
        "     'seed':42\n",
        "\n",
        "        \n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4MzDSc-e_Aj"
      },
      "source": [
        "data를 불러오고 train의 성능을 높이기 위하여 augmentation을 사용하여 data를 다양하게 만든다.  \n",
        "augmentation:이미지 회전과 같은 변환을 적용하여 training data의 다양성을 증가시키는 기술"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6Ah5Cscntjz7"
      },
      "outputs": [],
      "source": [
        "# keypoint를 기준으로 이미지를 crop하기 위한 함수를 정의한다.\n",
        "# train과 test시 해당 함수가 적용된 crop 이미지가 inputs으로 들어가게 된다.\n",
        "def crop_image(imgs, point, margin=100):\n",
        "  image = np.array(Image.open(imgs).convert('RGB'))\n",
        "  point = point['data']\n",
        "  max_point = np.max(np.array(point), axis=0).astype(int) + margin\n",
        "  min_point = np.min(np.array(point), axis=0).astype(int) - margin\n",
        "  max_point = max_point[:-1]  # remove Z order(label)\n",
        "  min_point = min_point[:-1]  # remove Z order(label)\n",
        "\n",
        "  max_x, max_y = max_point\n",
        "  min_x, min_y = min_point\n",
        "  max_y += margin  # 손목까지 인지하게끔\n",
        "\n",
        "  # 데이터 포인트의 크기는 원이미지보다 작아야한다\n",
        "  max_x = max_x if max_x < 1920 else 1920\n",
        "  max_y = max_y if max_y < 1080 else 1080\n",
        "  min_x = min_x if min_x > 0 else 0\n",
        "  min_y = min_y if min_y > 0 else 0\n",
        "\n",
        "  crop_image = image[min_y:max_y, min_x:max_x]\n",
        "\n",
        "  return crop_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "sjA8CswXwhKs",
        "outputId": "5632dfd9-c72d-4e3a-fdd3-ed5f5bb12b7b"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Cannot save file into a non-existent directory: '../data'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     answers\u001b[38;5;241m.\u001b[39mappend([image_name, cat, cat_name])  \u001b[38;5;66;03m# [train 경로, training data 이미지, training data 라벨]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m answers \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(answers, columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_path\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 21\u001b[0m \u001b[43manswers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/df_train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 클래스가 1개뿐인 폴더들을 Augmentation해서 이미지 생성 후 dataframe을 재정의한다.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m## 새롭게 정의한 dataframe을 학습에 이용하면 약간의 성능 향상을 얻을 수 있었다.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/Pytorch/lib/python3.9/site-packages/pandas/core/generic.py:3563\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3552\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3554\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3555\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3556\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3560\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3561\u001b[0m )\n\u001b[0;32m-> 3563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mline_terminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mline_terminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3566\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3568\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Pytorch/lib/python3.9/site-packages/pandas/io/formats/format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1162\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1163\u001b[0m     line_terminator\u001b[38;5;241m=\u001b[39mline_terminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1179\u001b[0m )\n\u001b[0;32m-> 1180\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
            "File \u001b[0;32m~/anaconda3/envs/Pytorch/lib/python3.9/site-packages/pandas/io/formats/csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_terminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
            "File \u001b[0;32m~/anaconda3/envs/Pytorch/lib/python3.9/site-packages/pandas/io/common.py:697\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 697\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/Pytorch/lib/python3.9/site-packages/pandas/io/common.py:571\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    569\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '../data'"
          ]
        }
      ],
      "source": [
        "# dataloader에서 사용할 dataframe을 만든다\n",
        "train_path = '../data/train'\n",
        "train_folders = natsorted(glob(train_path + '/*'))\n",
        "# natsorted: 텍스트로된 숫자를 정렬해준다\n",
        "# glob('*.exe') >>> ['python.exe', 'pythonw.exe']\n",
        "\n",
        "answers = []\n",
        "for train_folder in train_folders:\n",
        "  json_path = glob(train_folder + '/*.json')[0]\n",
        "  js = json.load(open(json_path))\n",
        "  # json 파일은 Key-Value 쌍의 형태를 가진다.\n",
        "  ### ['action']\n",
        "  cat = js.get('action')[0]  # 로드한 json파일의 key값이 'action'인 value의 첫번째 값을 cat에 저장 \n",
        "  cat_name = js.get('action')[1]  # 로드한 json파일의 key값이 'action'인 value의 두번째 값을 cat_name에 저장 \n",
        "\n",
        "  images_list = glob(train_folder + '/*.png')\n",
        "  for image_name in images_list:\n",
        "    answers.append([image_name, cat, cat_name])  # [train 경로, training data 이미지, training data 라벨]\n",
        "\n",
        "answers = pd.DataFrame(answers, columns = ['train_path', 'answer', 'answer_name'])\n",
        "answers.to_csv('../data/df_train.csv', index=False)\n",
        "\n",
        "# 클래스가 1개뿐인 폴더들을 Augmentation해서 이미지 생성 후 dataframe을 재정의한다.\n",
        "## 새롭게 정의한 dataframe을 학습에 이용하면 약간의 성능 향상을 얻을 수 있었다.\n",
        "seed = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "data_path = '../data'\n",
        "df_train = pd.read_csv(opj(data_path, 'df_train.csv'))\n",
        "df_info = pd.read_csv(opj(data_path, 'hand_gesture_pose.csv'))\n",
        "df_train = df_train.merge(df_info[['pose_id', 'gesture_type', 'hand_type']],\n",
        "                          how='left', left_on='answer', right_on='pose_id')\n",
        "save_folder = 'train'\n",
        "for i in range(649, 649+5):\n",
        "  if not os.path.exists(opj(data_path, save_folder, str(i))):\n",
        "    os.makedirs(opj(data_path, save_folder, str(i)))\n",
        "\n",
        "## flip aug 가능한 label: 131, 47 (각각 하나의 sample)\n",
        "oslabel_fliplabel = [(131,156), (47,22)]  #(one sample label, flip label)\n",
        "folders = ['649', '650']  # train 648번 folder에 이은 number 생성\n",
        "for label, folder in tqdm(zip(oslabel_fliplabel, folders)):\n",
        "  idx = 0\n",
        "  os_label, f_label = label[0], label[1]\n",
        "  one_sample = df_train[df_train['answer']==os_label].reset_index(drop=True)\n",
        "  temp = df_train[df_train['answer']==f_label].reset_index(drop=True)\n",
        "  train_folders = natsorted(temp['train_path'].apply(lambda x:x[-6]).unique())\n",
        "  for train_folders in trainfolders:\n",
        "    json_path = glob(train_folder + '/*.json')[0]\n",
        "    js = json.load(open(json_path))\n",
        "    keypoints = js['annotations']  ### ['annotation']\n",
        "    images_list = natsorted(glob(train_folder + '/*.png'))\n",
        "    for _, (point, image_name) in enumerate(zip(keypoints, images_list)):\n",
        "      cropped_image = crop_image(image_name, point, margin=50)\n",
        "      flip_img = cv2.flip(cropped_image, 1)\n",
        "      save_path = opj(data_path, save_folder, folder, f'{idx}.png')\n",
        "      idx += 1 \n",
        "      cv2.imwrite(save_path, flip_img)\n",
        "      df_train.loc[len(df_train)] = [save_path] + one_sample.iloc[0][1:].values.tolist()\n",
        "\n",
        "def rotation(imgm, angle):\n",
        "  angle = int(random.uniform(-angle, angle))\n",
        "  h, w = img.shape[:2]\n",
        "  M = cv2.getRotationMatrix2D((int(w/2), int(h/2), angle, 1))\n",
        "  img = cv2.wrapAffine(img, M, (w, h))\n",
        "  return img\n",
        "\n",
        "oslabel = [92, 188, 145]\n",
        "folder = ['651', '652', '653']\n",
        "for label, folder in tqdm(zip(oslabel, folder)):\n",
        "  idx = 0\n",
        "  one_sample = df_train[df_train['answer']==label].reset_index(drop=True)\n",
        "  train_folders = natsorted(temp['train_path'].apply(lambda x:x[:-6]).unique())\n",
        "  for train_folder in train_folders:\n",
        "    json_path = glob(train_folder + '/*.json')[0]\n",
        "    js = json.load(open(json_path))\n",
        "    keypoints = js['annotations']\n",
        "    images_list = natsorted(glob(train_folder+'/*.png'))\n",
        "    for _, (point, image_name) in enumerate(zip(keypoints, images_list)):\n",
        "      cropped_image = crop_image(image_name, point, margin=50)\n",
        "      aug_img = rotation(cropped_image, 30)\n",
        "      save_path = opj(data_path, save_folder, folder, f'{idx}.png')\n",
        "      idx += 1 \n",
        "      cv2.imwrite(save_path, flip_img)\n",
        "      df_train.loc[len(df_train)] = [save_path] + one_sample.iloc[0][1:].values.tolist()\n",
        "      \n",
        "  df_train.to_csv('../data/df_train_add.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwQ227XPtjfK"
      },
      "source": [
        "data를 train할때 사용할 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxCzWe9JBzmN"
      },
      "outputs": [],
      "source": [
        "# Train dataset에 475, 543 폴더는 의도하지 않은, 나머지 손에 대해서도 Keypoint가 잡히게 된다.\n",
        "# json의 keypoint를 사용하기 위해 475, 543 폴더인 경우 해당 부분 keypoint를 제거한다.\n",
        "def remove_keypoints(folder_num, points):\n",
        "  list_ = []\n",
        "  for x,y,x in points:\n",
        "    cond1 = x<250 and y>800\n",
        "    cond2 = x>1400 and y<400\n",
        "    if not (cond1 or cond2):\n",
        "      list_.append([x, y, z])\n",
        "    return list_\n",
        "\n",
        "class Train_Dataset(Dataset):\n",
        "  def __init__(self, df, transform=None, df_flip_info=None, flipaug_ratio=0, label_encoder=None, margin=50, random_margin=True):\n",
        "    self.id = df['train_path'].values\n",
        "    self.target = df['answer'].values\n",
        "    self.transform = transform\n",
        "    self.margin = margin\n",
        "    self.random_margin = random_margin\n",
        "\n",
        "    # Flip augmentation(target class를 바꾼다)\n",
        "    if df_flip_info is not None:\n",
        "      self.use_flip = True\n",
        "      print('Use Flip Augmentation')\n",
        "      left = label_encoder.transform(df_flip_info['left'])\n",
        "      right = label_encoder.transform(df_flip_info['right'])\n",
        "      left_to_right = dict(zip(left, right))\n",
        "      right_to_left = dict(zip(right, left))\n",
        "      \n",
        "      self.flip_info = left_to_right.copy()\n",
        "      self.flip_info.update(right_to_left)\n",
        "      self.flip_possible_class = list(set(np.concatenate([left, right])))\n",
        "    self.flipaug_ratio = flipaug_ratio\n",
        "\n",
        "    print(f'Dataset size:{len(self.id)}')\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = np.array(Image.open(self.id[idx]).convert('RGB'))\n",
        "    target = self.target[idx]\n",
        "\n",
        "    # load json file \n",
        "    try:\n",
        "      image_num = int(Path(self.id[idx].stem))\n",
        "      dir = os.path.dirname(self.id[idx])\n",
        "      folder_num = os.path.basename(dir)\n",
        "      json_path = opj(dir, folder_num + '.json')\n",
        "      js = json.load(open(json_path))\n",
        "      keypoints = js['annoatations'][image_num]['data']  # 해당 이미지에 해당하는 keypoints\n",
        "    except:  # Augmentation으로 직접 새로 만든 folder는 json이 없으므로 바로 return한다(미리 손 부분이 crop된 상태)\n",
        "      image = self.tranform(Image.fromarray(image))\n",
        "      return image, np.array(target)\n",
        "    \n",
        "    if folder_num in ['475', '543']:\n",
        "      keypoints = remove_keypoints(folder_num, keypoints)\n",
        "    \n",
        "    # keypoints를 사용하여 image를 crop한다\n",
        "    max_point = np.max(np.array(keypoints), axis=0).astype(int) + self.margin\n",
        "    min_point = np.min(np.array(keypoints), axis=0).astype(int) - self.margin\n",
        "    max_point = max_point[:-1]  # remove Z order\n",
        "    min_point = min_point[:-1]  # remove Z order\n",
        "    \n",
        "    max_x, max_y = max_point\n",
        "    min_x, min_y = min_point\n",
        "    max_y += 100  # 손몬 부분까지 여유를 주기 위함\n",
        "\n",
        "    # 매 에폭마다 margin을 조금씩 다르게 주면 한 폴더 내 비슷한 이미지들의 overfiting을 방지하는 효과를 얻을 수 있다 (train에서만)\n",
        "    if self.random_margin:\n",
        "      if random.random() < 0.5:\n",
        "        max_x += self.margin\n",
        "      if random.random() < 0.5:\n",
        "        max_y += self.margin\n",
        "      if random.random() < 0.5:\n",
        "        min_x -= self.margin\n",
        "      if random.random() < 0.5:\n",
        "        min_y -= self.margin\n",
        "    else:\n",
        "      max_x += self.margin\n",
        "      max_y += self.margin\n",
        "      min_x -= self.margin\n",
        "      min_y -= self.margin\n",
        "    \n",
        "    # 데이터 포인트의 크기는 원 이미지보다 크면 안된다\n",
        "    max_x = max_x if max_x < 1920 else 1920\n",
        "    max_y = max_y if max_y < 1080 else 1080\n",
        "    min_x = min_x if min_x > 0 else 0\n",
        "    min_y = min_y if min_y > 0 else 0\n",
        "\n",
        "    image = image[min_y:max_y, min_x:max_x]\n",
        "\n",
        "    # Flip Aug\n",
        "    if (random.random() < self.flipaug_ratio) and (target in self.flip_possible_class):\n",
        "      image = np.flip(image, axis=1)  # (H,W,C)에서 width 축 flip\n",
        "      target = self.flip_info[target]\n",
        "    \n",
        "    image = self.transform(Image.fromarray(image))\n",
        "    return image, np.array(target)\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.id)\n",
        "\n",
        "def get_loader(df, batch_size, shuffle, num_workers, transform, df_flip_info=None,\n",
        "               flipaug_ratio=0, label_encoder=Ndne, margin=50, random_margin=True):\n",
        "  dataset = Train_Dataset(df, transform, df_flip_info=df_flip_info, flipaug_ratio=flipaug_ratio,\n",
        "                          label_encoder=label_encoderm, margin=margin, random_margin=random_margin)\n",
        "  data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True, drop_last=False)\n",
        "  return data_loader\n",
        "\n",
        "def get_train_augmentation(img_size, ver):\n",
        "  if ver==1:\n",
        "    # For test\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "  \n",
        "  if ver==2:\n",
        "    # For train\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomAffine(20),\n",
        "        transforms.RandomPerspective(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "  return transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7sKs1zJKPet"
      },
      "source": [
        "Network(모델) 정의:  \n",
        "pytorch image models(timm) 라이브러리를 화용하여 generalization performance에 강점을 가지는 RegNet을 base 모델로 사용하였다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSFDjWDwJvzd"
      },
      "outputs": [],
      "source": [
        "class Pose_Network(nn.Module):\n",
        "  def __init__(self, args):\n",
        "    super().__init__()\n",
        "    self.encoder = timm.create_model(args.encoder_name, pretrained=True,\n",
        "                                     drop_path_rate=args.drop_path_rate,)\n",
        "    num_head = self.encoder.head.fc.in_features\n",
        "    self.encoder.head.fc = nn.Linear(num_head, 157)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.encoder(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBfyiND0Lqmc"
      },
      "source": [
        "logging가 avgMeter를 통해 실험 기록을 log파일로 남도록 저장하였다. 추가로 실험마다 비교를 쉽게 하기 위해 neptune을 활용하였는데 코드에서는 제거하였다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_qFy_w7Hg6k"
      },
      "outputs": [],
      "source": [
        "# warmup learning rate scheduler\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "class WarmUpLR(_LRScheduler):\n",
        "    \"\"\"warmup_training learning rate scheduler\n",
        "    Args:\n",
        "        optimizer: optimzier(e.g. SGD)\n",
        "        total_iters: totoal_iters of warmup phase\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
        "        \n",
        "        self.total_iters = total_iters\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        \"\"\"we will use the first m batches, and set the learning\n",
        "        rate to base_lr * m / total_iters\n",
        "        \"\"\"\n",
        "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
        "\n",
        "# Logging\n",
        "def get_root_logger(logger_name='basicsr',\n",
        "                    log_level=logging.INFO,\n",
        "                    log_file=None):\n",
        "\n",
        "    logger = logging.getLogger(logger_name)\n",
        "    # if the logger has been initialized, just return it\n",
        "    if logger.hasHandlers():\n",
        "        return logger\n",
        "\n",
        "    format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
        "    logging.basicConfig(format=format_str, level=log_level)\n",
        "\n",
        "    if log_file is not None:\n",
        "        file_handler = logging.FileHandler(log_file, 'w')\n",
        "        file_handler.setFormatter(logging.Formatter(format_str))\n",
        "        file_handler.setLevel(log_level)\n",
        "        logger.addHandler(file_handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "class AvgMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.losses = []\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        self.losses.append(val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy9uB2K3MKPu"
      },
      "source": [
        "Trainer  \n",
        "모델의 학습(training function)과 검증(validation)을 위한 class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN9YM2vkMRGk"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "  def __init__(self, args, save_path):\n",
        "    \"\"\"\n",
        "    args: arguments\n",
        "    save_path: model 가중치 저장 경로\n",
        "    \"\"\"\n",
        "    super(Trainer, self).__init__()\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Logging\n",
        "    log_file = os.path.join(save_path, 'log.log')\n",
        "    self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n",
        "    self.logger.info(args)\n",
        "    self.logger.info(args.tag)\n",
        "\n",
        "    # load train, valid set\n",
        "    df_train = pd.read_csv(opj(args.data_path, 'df_train_add.csv'))\n",
        "    df_info = pd.read_csv(opj(args.data_path, 'hand_gesture_pose.csv'))\n",
        "\n",
        "    df_train = df_train.merge(df_info[['pose_id', 'gesture_type', 'hand_type']],\n",
        "                              how='left', left_on='answer', right_on='pose_id')\n",
        "    # 폴더별(group)으로 각 번호 부여\n",
        "    df_train['groups'] = df_train['train_path'].apply(lambda x:x.split('/')[3])\n",
        "    df_train.loc[:,:] = natsorted(df_train.values)\n",
        "    # 노이즈 이미지 제거: 596번 주먹쥐기 이미지에 손바닥을 펴는 노이즈 이미지 5장 존재, 0번 폴더의 9번 이미지도 노이즈\n",
        "    drop_idx = df_train[df_train['groups'].isin(['596'])].index.tolist()[3:8] + [9]\n",
        "    df_train = df_train.drop(drop_idx).reset_index(drop=True)\n",
        "    le = LabelEncoder()\n",
        "    df_train['answer'] = le.fit_transform(df_train['answer'])\n",
        "\n",
        "    # split fold: 오류가 심한 클래스에 대한 분석을 수행하기 위함\n",
        "    # StratifiedKFold는 클래스 비율을 고려하여 랜덤하게 데이터를 섞어 데이터를 split해준다\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(df_train, y=df_train['answer'])):\n",
        "      df_train.loc[val_idx, 'fold'] = fold\n",
        "    df_val = df_train[df_train['fold']==args.fold].reset_index(drop=True)\n",
        "    df_train = df_train[df_train['fold']!=args.fold].reset_index(drop=True)\n",
        "\n",
        "    # Augmentation\n",
        "    self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
        "    self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
        "\n",
        "    #########################################################################################\n",
        "    # filp augmentation을 위한 mapping dataframe\n",
        "    df_info = pd.read_csv('../data/hand_gesture_pose.csv')\n",
        "    df_info = df_info[df_info['hand_type'] != 'both']\n",
        "    # drop idx, 동일한 약속, gesture_type, hand_type인데 다른 클래스인 경우 존재 -> 약속 1과 2로 이름을 나누어준다.\n",
        "    df_info.loc[[105, 128], 'pose_name'] = '약속 1'\n",
        "    df_info.loc[[101, 124], 'pose_name'] = '약속 2'\n",
        "\n",
        "    # drop 41 idx, 동일한 약속, my hand, right class가 49와 54로 두 개있어 Mapping df만들 때 문제가 발생하여 미리 49번 클래스 처리\n",
        "    df_info = df_info.drop(41)\n",
        "\n",
        "    # Make a mapping dataframe\n",
        "    df_info = df_info.groupby(['pose_name', 'view_type', 'gesture_type', 'hand_type']).sum().unstack().reset_index().dropna(axis=0)\n",
        "    df_info['left'] = df_info.pose_id.left.apply(int)\n",
        "    df_info['right'] = df_info.pose_id.right.apply(int)\n",
        "    df_flip_info = df_info.drop('pose_id', axis=1).droplevel('hand_type', axis=1).reset_index(drop=True)\n",
        "    print('Mapping dataframe Length', df_flip_info.shape)   \n",
        "\n",
        "    # TrainLoader #\n",
        "    self.train_loader = get_loader(df_train, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, transform=self.train_transform,\n",
        "                                   df_flip_info=df_flip_info, flipaug_ratio=args.flipaug_ratio, label_encoder=le, margin=args.margin, random_margin=args.random_margin)\n",
        "    self.val_loader = get_loader(df_val, batch_size=args.batch_size, shuffle=False,\n",
        "                                   num_workers=args.num_workers, transform=self.test_transform)\n",
        "    \n",
        "    # Network #\n",
        "    self.model = Pose_Network(args).to(self.device)\n",
        "    macs, params = get_model_complexity_info(self.model, (3, args.img_size, args.img_size), as_strings=True,\n",
        "                                             print_per_layer_stat=False, verbose=False)\n",
        "    self.logger.info('{:<30} {:<8}'.format('Computational complexity: ', macs))\n",
        "    self.logger.info('{:<30} {:<8}'.format('Number of parameters: ', params))\n",
        "\n",
        "    # Loss #\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer & Scheduler #\n",
        "    self.optimizer = optim.Lamb(elf.model.parameters(), lr=args.initial_lr, weight_decay=args.weight_decay)\n",
        "\n",
        "    iter_per_epoch = len(self.train_loader)\n",
        "    self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch*args.warm_epoch)\n",
        "\n",
        "    if args.scheduler == 'cos':\n",
        "      tmax = args.tmax \n",
        "      self.scheduler = torch.optim.lr_scheduler.CosingAnnealingLR(self.optimizer, T_max=tmax, eta_min=args.min_lr, verbose=True)\n",
        "    elif args.scheduler == 'cycle':\n",
        "      self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
        "\n",
        "    if args.multi_gpu:\n",
        "      self.model = nn.DataParallel(self.model).to(self.device)\n",
        "    \n",
        "    # Train / validate\n",
        "    best_loss = np.inf\n",
        "    best_acc = 0\n",
        "    early_stopping = 0\n",
        "    start = time.time()\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "      self.epoch = epoch\n",
        "\n",
        "      if args.scheduler == 'cos':\n",
        "        if epoch > args.warm_epoch:\n",
        "          self.scheduler.step()\n",
        "      \n",
        "      # Training\n",
        "      train_loss, train_acc = self.training(args)\n",
        "\n",
        "      # Model weight in multiple gpu or single gpu\n",
        "      state_dict = self.odel.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
        "\n",
        "      # validation\n",
        "      val_loss, val_acc = self.validate()\n",
        "\n",
        "      # sace models\n",
        "      if val_loss < best_loss:\n",
        "        early_stopping = 0\n",
        "        best_epoch = epoch\n",
        "        best_loss = val_loss\n",
        "        best_acc = val_acc\n",
        "\n",
        "        torch.save({'epoch': epoch,\n",
        "                    'state_dict':state_dict,\n",
        "                    'optimizer':self.optimizer.state_dict(),\n",
        "                    'scheduler':self.scheduler.state_dict(),\n",
        "                    }, os.path.join(save_path, 'best_model.pth'))\n",
        "        self.logger.info(f'------------------SAVE:{best_epoch}epoch-------------------')\n",
        "      else:\n",
        "        early_stopping += 1\n",
        "\n",
        "      # early stopping\n",
        "      if early_stopping == args.patience:\n",
        "        break\n",
        "\n",
        "    self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc{best_acc:.4f}')\n",
        "    end = time.time()\n",
        "    self.logger.info(f'Total Process time:{(end-start) / 60:.3f}Minute')\n",
        "\n",
        "  # Training\n",
        "  def training(self, args):\n",
        "    self.model.train()\n",
        "    train_loss = AvgMeter()\n",
        "    train_acc = 0\n",
        "\n",
        "    scaler = grad_scaler.GradScaler()\n",
        "    for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
        "      images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
        "      targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
        "\n",
        "      if self.epoch <= args.warm_epoch:\n",
        "        self.warmup_scheduler.step()\n",
        "\n",
        "      self.model.zero_grad(set_to_none=True)\n",
        "      if args.amp:\n",
        "        with autocast():\n",
        "          preds = self.model(images)\n",
        "          loss = self.criterion(preds, targets)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        if args.clipping is not None:\n",
        "          scaler.unscale_(self.optimizer)\n",
        "          nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
        "\n",
        "        scaler.step(self.optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "      else:\n",
        "        preds = self.model(images)\n",
        "        loss = self.criterion(preds, targets)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
        "        self.optimizer.step()\n",
        "\n",
        "      if args.scheduler == 'cycle':\n",
        "        if self.epoch >  args.warm_epoch:\n",
        "          self.scheduler.step()\n",
        "\n",
        "      # metric\n",
        "      train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
        "      \n",
        "      # log\n",
        "      train_loss.update(loss.item(), n=images.size(0))\n",
        "\n",
        "    train_acc /= len(self.train_loader.dataset)\n",
        "\n",
        "    self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
        "    self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc{train_acc:.4f}')\n",
        "    return train_loss.avg, train_acc\n",
        "\n",
        "  # validation or dev\n",
        "  def validate(self):\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "      val_loss = AvgMeter()\n",
        "      val_acc = 0\n",
        "\n",
        "      for _, (images, targets) in enumerate(self.val_loader):\n",
        "        images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
        "        targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
        "\n",
        "        preds = self.model(images)\n",
        "        loss = self.criterion(preds, targets)\n",
        "\n",
        "        # metric \n",
        "        val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
        "        \n",
        "        # log\n",
        "        val_loss.update(loss.item(), n=images.size(0))\n",
        "      val_acc /= len(self.val_loader.dataset)\n",
        "\n",
        "      self.logger.info(f'Valid Loss:{val_loss.avg:.3f} | Acc{val_acc:.4f}')\n",
        "    return val_loss.avg, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKXKI6A66Gij"
      },
      "source": [
        "위이 코드와 맨 마지막 cell의 main함수가 주요 소스코드이고 main함수 전까지의 아래의 코드는 모델이 클래스를 잘못 예측하는 경우들에 대해 코드를 수정하여 모델의 정확도를 높이고자 한 것이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r4k5vHIwwVv"
      },
      "outputs": [],
      "source": [
        "# case 1: 손동작이 유사한 숫자 1과 검지 흔들기의 사진의 분류를 비교적 낮은 정확도로 수행하였다. \n",
        "## 두 클래스에 대해 계산된 검지의 x좌표 변화량들에 대해 적절한 임계치 값을 설정해 클래스를 구분짓게 하였다.\n",
        "\n",
        "# case 2: 손동작이 유사한 주먹쥐기와 주먹 내밀기의 이미지에 대한 분류의 정확도가 낮게 측정되었다.\n",
        "## 손의 움직임을 통해, keypoints 중 가장 오른쪽의 x의 변화량에 대해서 적절한 임계치를 설정해 두 클래스를 구분짓게 하였다.\n",
        "\n",
        "# 임계치를 계산하기 위해 각 case에 대한 변화량을 계산하는 함수 정의\n",
        "def check_stats(find_list, ver):\n",
        "  train_path = '../data/train'\n",
        "  train_folders = natsorted(glob(train_path + '/*'))\n",
        "  stat_list = []\n",
        "  for _, train_folder in tqdm(enumerate(train_folders)):\n",
        "    try:\n",
        "      json_path = glob(train_folder + '/*.json')[0]\n",
        "      js = json.load(open(json_path))\n",
        "      cat = js.get('action')[0]\n",
        "      keypoints = js['annotations']\n",
        "      keypoints = np.array([point['data'] for point in keypoints])  # (N-이미지개수, 21 or 42(keypoints), 3(x,y,z 좌표))\n",
        "    except:\n",
        "      pass\n",
        "    if cat in find_list:\n",
        "      # case 1의 숫자 1과 검지 흔들기 구분\n",
        "      # 검지는 이미지 내 keypoints들 중 가장 작은 y값(이미지 내 가장 높은 위치)을 갖는 point이다.\n",
        "      # 해당 point의 x값을 뽑는다.(x의 변화량을 보기 위하여)\n",
        "      if ver==1:\n",
        "        keypoints = keypoints[:,:,:2] #(N, 21 or 42, 2)\n",
        "        '''\n",
        "        point_per_img[:,0][point_per_img[:,1].argmin()]\n",
        "        : 여러 x값들에 대해 y값이 가장 큰 point값들을 나타냄\n",
        "        point_per_img[:,0]은 x좌표\n",
        "        point_per_img[:,1]은 y좌표 나타냄\n",
        "        '''\n",
        "        x_l_finger = [point_per_img[:,0][point_per_img[:,1].argmin()] for point_per_img in keypoints]  # (N, 2) Y축으로 가장 작은 포인트 두개 추출\n",
        "        stat_list.append(np.max(x_l_finger)-np.min(x_l_finger))\n",
        "\n",
        "      # case 2의 주먹쥐기와 주먹 내밀기 구분\n",
        "      # keypoints 중 가장 큰 x값(이미지 내 가장 우측 위치)을 갖는 point를 본다. \n",
        "      # case2같은 경우는 left 손목이 없기 때문에 해당 logic이 잘 작동한다\n",
        "      elif ver == 2:\n",
        "        keypoints = keypoints[:,:,0]  # x좌표 값들만 가져옴\n",
        "        x_values = [point_per_img[point_per_img.argmax()] for point_per_img in keypoints]\n",
        "        stat_list.append(np.max(x_values)-np.min(x_values))\n",
        "\n",
        "  print(stat_list)\n",
        "  return stat_list\n",
        "\n",
        "## label ##\n",
        "############ ver1 #############\n",
        "find_list0 = [0, 10, 100, 110]  # ['숫자 1', '숫자1']  my hand, your hand 좌우\n",
        "find_list1 = [42, 67, 142, 167]  # ['부정(검지 흔들기)'] my hand, your hand 좌우\n",
        "\n",
        "############ ver2 #############\n",
        "find_list2 = [146]  # ['주먹쥐기']  Your hand 우\n",
        "find_list3 = [163]  # ['경고(주먹 내밀기)'] Your hand 우\n",
        "\n",
        "find_list4 = [171]  # ['주먹쥐기']  Your hand Both\n",
        "find_list5 = [191]  # ['경고(주먹 내밀기)'] Your hand Both\n",
        "\n",
        "\n",
        "# 숫자1 & 검지 흔들기\n",
        "li0 = check_stats(find_list0,1)  #숫자1 or 숫자 1\n",
        "li1 = check_stats(find_list1,1)  #부정(검지 흔들기)\n",
        "threshold_ver1 = max(li0) + 5    # Margin 5\n",
        "print(f'\\n{threshold_ver1:.3f}보다 크면 부정(검지 흔들기) 클래스')\n",
        "\n",
        "# 주먹쥐기 vs 주먹 내밀기 Right\n",
        "li2 = check_stats(find_list2,2)   \n",
        "li3 = check_stats(find_list3,2)   \n",
        "threshold_ver2 = max(li2) + 5       # Margin 5\n",
        "print(f'\\n{threshold_ver2:.3f}보다 크면 주먹 내밀기(right) 클래스')\n",
        "\n",
        "# 주먹쥐기 vs 주먹 내밀기 Both\n",
        "li4 = check_stats(find_list4,2)   \n",
        "li4 = li4[1:]                       # 596번 폴더 변화량(218.313) Outlier -> 제외\n",
        "li5 = check_stats(find_list5,2)   \n",
        "threshold_ver2_both = max(li4) + 5  # Margin 5\n",
        "print(f'\\n{threshold_ver2_both:.3f}보다 크면 주먹 내밀기(both) 클래스')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOKmPuZa7Jxo"
      },
      "source": [
        "Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7f0i274tZyC"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "  # random seed\n",
        "  seed = args.seed\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.bechnmark = True\n",
        "\n",
        "  save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n",
        "  # create model directory\n",
        "  os.makedirs(save_path, exist_ok=True)\n",
        "  Trainer(args, save_path)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  for i in range(5):\n",
        "    args.fold = i\n",
        "    args.exp_num = str(i)\n",
        "    main(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEylqZm9u9NX"
      },
      "source": [
        "Inference with ensemble  \n",
        "- 위에서 구한 임계치를 사용하여 rule base inference를 구축한다.  \n",
        "- replace_dict이라는 변수를 통해 헷갈리는 두 클래스를 매칭한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gr3urt1vM59"
      },
      "outputs": [],
      "source": [
        "# test의 keypoints(json) 변화량을 구하기 위한 함수 정의\n",
        "def Refiner(keypoints, ver):\n",
        "  keypoints = np.array([point['data'] for point in keypoints])\n",
        "  # 숫자 1과 검지 흔들기 구분\n",
        "  if ver == 1:\n",
        "    keypoints = keypoints[:,:,:2]\n",
        "    x_l_finger = [point_per_img[:,0][point_per_img[:,1].argmin()] for point_per_img in keypoints]\n",
        "    query_value = np.max(x_l_finger) - np,min(x_l_finger)\n",
        "\n",
        "  # 주먹쥐기와 주먹 내밀기 구분\n",
        "  elif ver == 2:\n",
        "    keypoints = keypoints[:,:,0]\n",
        "    x_values = [point_per_img[point_per_img.argmax()] for point_per_img in keypoints]\n",
        "    query_value = np.max(x_values) - np.min(x_values)\n",
        "  \n",
        "  return query_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RQ5mv1AuVKm"
      },
      "source": [
        "download the pretrained weight to path of ./results/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axcLU6K_ufFf"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "test_path = f'../data/test' \n",
        "test_folders = natsorted(glob(test_path + '/*'))\n",
        "\n",
        "args = easydict.EasyDict({'encoder_name':'regnety_040',\n",
        "                        'drop_path_rate':0,\n",
        "                        })\n",
        "\n",
        "load_pretrain = True        # Use Pretrained weights\n",
        "ensemble_test = True        # Ensemble or Single\n",
        "refine = True              # Use Refiner (Rule-base)\n",
        "\n",
        "\n",
        "if load_pretrain:  # Github로부터 Pretrained Weight Load\n",
        "  model_path0 = './results/0Fold_model.pth' # fold0\n",
        "  model_path1 = './results/1Fold_model.pth' # fold1\n",
        "  model_path2 = './results/2Fold_model.pth' # fold2\n",
        "  model_path3 = './results/3Fold_model.pth' # fold3\n",
        "  model_path4 = './results/4Fold_model.pth' # fold4\n",
        "\n",
        "else:  # 위에서 학습한 모델 Weight Load\n",
        "  model_path0 = './results/000/best_model.pth' # fold0\n",
        "  model_path1 = './results/001/best_model.pth' # fold1\n",
        "  model_path2 = './results/002/best_model.pth' # fold2\n",
        "  model_path3 = './results/003/best_model.pth' # fold3\n",
        "  model_path4 = './results/004/best_model.pth' # fold4\n",
        "\n",
        "\n",
        "# 5Fold Ensemble\n",
        "if ensemble_test:\n",
        "  model0 = Pose_Network(args).to(device)\n",
        "  model0.load_state_dict(torch.load(model_path0)['state_dict'])\n",
        "  model0.eval()\n",
        "\n",
        "  model1 = Pose_Network(args).to(device)\n",
        "  model1.load_state_dict(torch.load(model_path1)['state_dict'])\n",
        "  model1.eval()\n",
        "\n",
        "  model2 = Pose_Network(args).to(device)\n",
        "  model2.load_state_dict(torch.load(model_path2)['state_dict'])\n",
        "  model2.eval()\n",
        "\n",
        "  model3 = Pose_Network(args).to(device)\n",
        "  model3.load_state_dict(torch.load(model_path3)['state_dict'])\n",
        "  model3.eval()\n",
        "\n",
        "  model4 = Pose_Network(args).to(device)\n",
        "  model4.load_state_dict(torch.load(model_path4)['state_dict'])\n",
        "  model4.eval()\n",
        "\n",
        "  model_list = [model0, model1, model2, model3, model4]\n",
        "\n",
        "else:  # Single Best Model (Using the pretrained weight)\n",
        "  model_path = './results/single_best_model.pth'\n",
        "  single_best = Pose_Network(args).to(device)\n",
        "  single_best.load_state_dict(torch.load(model_path)['state_dict'])\n",
        "  single_best.eval()\n",
        "  model_list = [single_best]\n",
        "\n",
        "\n",
        "img_size = 288\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "sub = pd.read_csv('../data/sample_submission.csv')\n",
        "df_info = pd.read_csv('../data/hand_gesture_pose.csv')\n",
        "le = LabelEncoder()\n",
        "le.fit(df_info['pose_id'])\n",
        "trans = le.transform\n",
        "\n",
        "# Class Mapping dict\n",
        "ver1_list = trans([0, 42, 10, 67, 100, 142, 110, 167])   \n",
        "ver2_list = trans([146, 163, 171, 191])\n",
        "replace_dict = {146:163, 171:191, 0:42, 10:67, 100:142, 110:167}\n",
        "replace_dict = dict([trans(x) for x in list(replace_dict.items())])   # Mapping (Origin:0~195 to 0~156)\n",
        "\n",
        "total_list = np.concatenate([ver1_list, ver2_list]).tolist()\n",
        "\n",
        "\n",
        "for i, test_folder in tqdm(enumerate(test_folders)):\n",
        "  dir = os.path.dirname(test_folder)\n",
        "  folder_num = os.path.basename(test_folder)\n",
        "  json_path = opj(dir, folder_num, folder_num+'.json')\n",
        "  js = json.load(open(json_path))\n",
        "  keypoints = js['annotations']  # 해당 이미지에 해당하는 Keypoints\n",
        "  images_list = natsorted(glob(test_folder + '/*.png'))\n",
        "  images = []\n",
        "  for _, (point, image_name) in enumerate(zip(keypoints, images_list)):\n",
        "    croped_image = crop_image(image_name, point, margin=100)\n",
        "    image = transform(croped_image)\n",
        "    images.append(image)\n",
        "\n",
        "  images = torch.stack(images).to(device)\n",
        "  ensemble = np.zeros((157,), dtype=np.float32)\n",
        "  for model in model_list:\n",
        "    preds = model(images)\n",
        "    preds = torch.softmax(preds, dim=1)\n",
        "    preds = torch.mean(preds, dim=0).detach().cpu().numpy()    # shape:(157,)\n",
        "    ensemble += preds\n",
        "  preds = ensemble / len(model_list)\n",
        "  pred_class = preds.argmax().item()\n",
        "  if refine and (pred_class in total_list):\n",
        "    idx = list(replace_dict.keys()).index(pred_class) if pred_class in replace_dict.keys() else list(replace_dict.values()).index(pred_class)\n",
        "    cand1, cand2 = list(replace_dict.items())[idx]\n",
        "\n",
        "    if pred_class in ver1_list:\n",
        "      query_value = Refiner(keypoints, ver=1)\n",
        "      answer = cand1 if query_value < threshold_ver1 else cand2\n",
        "\n",
        "    elif pred_class in ver2_list:\n",
        "      query_value = Refiner(keypoints, ver=2)\n",
        "      answer = cand1 if query_value < threshold_ver2_both else cand2\n",
        "\n",
        "    preds[answer] = 1\n",
        "    preds = np.where(preds != 1, 0, preds)  # Refiner를 통해 나온 class를 제외한 나머지의 확률값은 모두 0으로 변환\n",
        "\n",
        "  sub.iloc[i, 1:] = preds.astype(float)\n",
        "\n",
        "sub.to_csv('./results/submission_train_add_ensemble_rule.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "euron_w1_손동작인식.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
